{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"About Me","text":""},{"location":"#hello","title":"Hello!","text":"<p>My name is Cameron Presley, and welcome to my blog, The Software Mentor!</p> <p>This is where I write about software, leadership, and other lessons learned over my career. There are plenty of blogs out there already speaking about these topics, however, I've found that there's power in seeing how others approach problems.</p> <p>My hope is that you'll find a different perspective, learn a new way to think about things, and in turn level up.</p> <p>If you're interested in a perspective on a leadership problem you're running into, I'd love to hear about it? You can submit your question anonymously through this Google Form or you can drop a line at CoachingCorner@TheSoftwareMentor.com.</p>"},{"location":"courses/","title":"Courses","text":"<p>As someone who enjoys leveling up others, I have published courses on Udemy to help engineers improve by learning concepts like functional programming.</p>"},{"location":"courses/#navigating-mars-using-typescript-and-functional-programming","title":"Navigating Mars Using TypeScript and Functional Programming","text":"<p>In this course, I walk you through solving the Mars Rover kata by using functional programming concepts like immutability, functions, and composition.</p> <p>Designed for those who are new to functional programming concepts, but are familiar with TypeScript, you'll leave the course having the tools needed to start writing your own programs.</p> <p>Interested? You can find the course here on Udemy.</p>"},{"location":"courses/#learn-functional-typescript-by-building-blackjack","title":"Learn Functional TypeScript by Building Blackjack","text":"<p>In this course, I walk you through implementing your own version of Blackjack using functional proramming with TypeScript. We'll cover topics like:</p> <ul> <li>How to build better domain models using concepts like Sum and Product types</li> <li>How to build repeatable and testable business rules leveraging pure functions</li> <li>How to handle when operations can fail using the Maybe pattern</li> <li>How to manage state when it's always changing</li> </ul> <p>Interested? You can send an email to be alerted when the course goes alive (tentatively Summer 2025) </p>"},{"location":"level-up/","title":"Level-Up Resources","text":"<p>While it's required to get your hands into the work, it would be foolish to ignore the path that others have traveled before. This is a listing of books/podcasts that I've found to be helpful as a engineer and as a leader.</p>  Reading is important, because if you can read, you can learn anything about everything and everything about anything   \u2014 Tomie dePaola"},{"location":"level-up/#leadership","title":"Leadership","text":"<ul> <li>Creating Great Teams: How Self-Selection Lets People Excel</li> <li>The Effective Manager</li> <li>Extreme Ownership: How U.S.Navy SEALs Lead and Win</li> <li>First, Break All The Rules: What the World\u2019s Greatest Managers Do Differently</li> <li>The Five Dysfunctions of a Team: A Leadership Fable</li> <li>Good to Great: Why Some Companies Make the Leap and Others Don\u2019t</li> <li>The Ideal Team Player: How to Recognize and Cultivate The Three Essential Virtues</li> <li>Managing Humans: Biting and Humorous Tales of a Software Engineering Manager</li> <li>The Art of Leadership</li> <li>Radical Candor: Be a Kick-Ass Boss Without Losing Your Humanity</li> <li>Rework</li> <li>To Sell Is Human: The Surprising Truth About Moving Others</li> </ul>"},{"location":"level-up/#software-development","title":"Software Development","text":"<ul> <li>The Art of Unit Testing: with examples in C# 2nd Edition</li> <li>Starting to Unit Test: Not as Hard as You Think</li> <li>The Nature of Software Development: Keep It Simple, Make It Valuable, Build It Piece by Piece</li> <li>Code That Fits in Your Head - Heuristics for Software Engineering</li> <li>Working Effectively with Legacy Code</li> <li>Refactoring: Improving the Design of Existing Code</li> <li>Debugging: The 9 Indispensable Rules for Finding Even the Most Elusive Software and Hardware Problems</li> <li>Modern Software Engineering: Doing What Works to Build Better Software Faster</li> </ul>"},{"location":"level-up/#career","title":"Career","text":"<ul> <li>The Pragmatic Programmer: From Journeyman to Master</li> <li>Apprenticeship Patterns: Guidance for the Aspiring Software Craftsman</li> <li>The Effective Engineer: How to Leverage Your Efforts In Software Engineering to Make a Disproportionate and Meaningful Impact</li> <li>So Good They Can't Ignore You: Why Skills Trump Passion in the Quest for Work You Love</li> <li>Damn Good Advice(For People with Talent!): How To Unleash Your Creative Potential</li> <li>Deep Work- Rules for Focused Success In a Distracted World</li> <li>Hidden Potential - The Science of Achieving Great Things</li> </ul>"},{"location":"level-up/#process","title":"Process","text":"<ul> <li>Scrum: The Art of Doing Twice the Work in Half the Time</li> <li>The Goal: A Process of Ongoing Improvement</li> <li>Theory of Constraints</li> <li>The Phoenix Project: A Novel about IT, DevOps, and Helping Your Business Win</li> <li>Making Work Visible: Exposing Time Theft to Optimize Work &amp; Flow</li> <li>User Story Mapping: Discover the Whole Story, Build the Right Product</li> <li>Building a Second Brain</li> </ul>"},{"location":"level-up/#psychology","title":"Psychology","text":"<ul> <li>Drive: The Surprising Truth About What Motivates Us</li> <li>Engineering and the Mind\u2019s Eye</li> <li>A Whole New Mind: Why Right-Brainers Will Rule the Future</li> <li>Atomic Habits: An Easy &amp; Proven Way to Build Good Habits &amp; Break Bad Ones</li> <li>Tiny Habits: The Small Changes That Change Everything</li> <li>The Book of Beautiful Questions: The Powerful Questions That Will Help You Decide, Create, Connect, and Lead</li> </ul>"},{"location":"level-up/#biographies","title":"Biographies","text":"<ul> <li>Every Tool's a Hammer: Life is What You Make It by Adam Savage</li> <li>Making It So: A Memoir by Patrick Stewart</li> <li>Disrupting the Game - From the Bronx to the Top of Nintendo by Reggie Fils-Aim\u00e9</li> <li>The Storyteller - Tales of Life and Music by Dave Grohl</li> <li>Ingrained: The Making of a Craftsman </li> </ul>"},{"location":"level-up/#history","title":"History","text":"<ul> <li>The Uranium Club: Unearthing the Lost Relics of the Nazi Nuclear Program</li> <li>The Queens of Animation: The Untold Story of the Women Who Transformed the World of Disney and Made Cinematic History</li> <li>Wise Gals: The Spies Who Built the CIA and Changed the Future of Espionage</li> <li>Rise of the Rocket Girls: The Women Who Propelled Us, from Missiles to the Moon to Mars</li> <li>Alan Turing: The Enigma</li> <li>13 Minutes to the Moon - Podcast Series by BBC World Service</li> </ul>"},{"location":"presentations/","title":"Presentations","text":""},{"location":"presentations/#establishing-a-solid-foundation-an-intro-to-software-design","title":"Establishing a SOLID Foundation \u2013 An Intro to Software Design","text":"<p>Are you tired of opening classes that have hundreds or thousands of lines of code? Do you feel that you spend too much time trying to understand how something (that should be simple) in your application works? Are you frustrated with following spaghetti-like logic through your code\u2019s core?</p> <p>In this presentation, we\u2019re going to examine a set of design principles that yield easy-to-read, decoupled code referred to as SOLID. To start, we\u2019ll explore the history of SOLID and establish the problems it solves. From there, we\u2019ll dive into the different principles, defining what each principle means, how to spot violations, and how to resolve them.</p>"},{"location":"presentations/#resources","title":"Resources","text":"<ul> <li>Presentations<ul> <li>Slides (SlideShare)</li> </ul> </li> <li>Books<ul> <li>Working Effectively With Legacy Code by Michael Feathers</li> <li>Refactoring: Improving the Design of Existing Code by Martin Fowler</li> </ul> </li> <li>Articles<ul> <li>Role Interfaces by Martin Fowler</li> <li>Header Interfaces by Martin Fowler</li> </ul> </li> <li>Videos<ul> <li>SOLID Principles of Object Oriented Design by Steve Smith (Pluralsight)</li> </ul> </li> </ul>"},{"location":"presentations/#making-the-unstable-stable-an-introduction-to-testing","title":"Making the Unstable Stable \u2013 An Introduction to Testing","text":"<p>Does it always seem like bugs you\u2019ve fixed keep coming back? Does it seem like when you fix one bug, two more crop up? What if I were to tell you there\u2019s a better way?</p> <p>In this presentation, we\u2019re going to explore how to make a code base more stable by using automated testing. To start, we\u2019ll explore the business case of why you should be writing tests by looking at industry studies and personal experience. From there, we\u2019ll look at the fundamentals of testing by talking about the pros/cons of unit, integration, and UI testing. Finally, we\u2019ll look at some resources to learn how to write tests.</p>"},{"location":"presentations/#resources_1","title":"Resources","text":"<ul> <li>Presentations<ul> <li>Slides (SlideShare)</li> <li>Unit Testing Makes Me Faster by Jeremy Clark</li> </ul> </li> <li>Books<ul> <li>Starting to Unit Test: Not as Hard as You Think by Erik Dietrich</li> <li>The Art of Unit Testing: with Examples in C# by Roy Osherove</li> <li>Working Effectively With Legacy Code by Michael Feathers</li> <li>Refactoring: Improving the Design of Existing Code by Martin Fowler</li> <li>Code Complete: A Practical Handbook of Software Construction (2nd Edition) by Steve McConnell</li> </ul> </li> <li>Articles<ul> <li>On the Effectiveness of Unit Test Automation at Microsoft (pdf)</li> </ul> </li> </ul>"},{"location":"presentations/#taking-a-gamble-on-f-implementing-blackjack","title":"Taking a Gamble on F#: Implementing Blackjack","text":"<p>Have you heard a lot about about functional programming, and aren\u2019t sure how to get started? Have you tried learning the concepts, but still having a hard time applying them to a real problem? Then this talk is for you! In this presentation I\u2019ll walk you through how to model the classic card game, Blackjack, using F#.</p>"},{"location":"presentations/#resources_2","title":"Resources","text":"<ul> <li>Presentation<ul> <li>Slides (SlideShare)</li> </ul> </li> <li>Code<ul> <li>Blackjack Repo (GitHub)</li> </ul> </li> <li>Books<ul> <li>The Book of F#: Breaking Free with Managed Functional Programming by Dave Fancher</li> </ul> </li> <li>Videos<ul> <li>F# Jumpstart by Kit Eason</li> </ul> </li> </ul>"},{"location":"presentations/#how-to-have-code-reviews-developers-actually-want","title":"How To Have Code Reviews Developers Actually Want","text":"<p>This phrase can stir up a lot of emotions for people. For some, it\u2019s aggravation because they\u2019re a waste of time, for others, it\u2019s stressful because it feels like you\u2019re getting personally attacked. However, for some, it\u2019s a great learning experience that leads to the team improving. Do you want to be in the latter group? Then this talk is for you!</p> <p>In this presentation, I\u2019ll first show you the benefits of code review and the business case for why they should happen. Next, I\u2019ll show some of the most common mistakes that teams make during the review process and how to mitigate them. After talking about the bad, we\u2019ll talk about what to look for in your code review process. Finally, I\u2019ll wrap things up by showing the game plan I use for code reviews.</p>"},{"location":"presentations/#resources_3","title":"Resources","text":"<ul> <li>Presentations<ul> <li>Slides (SlideShare)</li> </ul> </li> <li>Books<ul> <li>Code Complete: A Practical Handbook of Software Construction (2nd Edition) by Steve McConnell</li> <li>Peer Reviews In Software: A Practical Guide by Karl E. Wiegers</li> </ul> </li> <li>Articles<ul> <li>11 Best Practices for Peer Code Review</li> <li>Brainstorming About Code Reviews by Geoff Mazeroff</li> <li>Thirty Percent Feedback</li> <li>Code Review is the Manager\u2019s Job</li> </ul> </li> </ul>"},{"location":"presentations/#how-functional-programming-made-me-a-better-developer","title":"How Functional Programming Made Me a Better Developer","text":"<p>With the rise in popularity recently, functional programming has become \"The Next Big Thing\". As of today, there are tons of frameworks and tools that can be used for front-end, back-end, desktop, and mobile development. With that being said, the majority of us are still using object-oriented languages for our day jobs and don\u2019t need to learn functional programming, right?</p> <p>In this talk, I\u2019ll walk you through my experiences learning functional programming over the last year, how my style of programming has changed, and how I now think about programming with regards to both functional and object-oriented paradigms.</p>"},{"location":"presentations/#resources_4","title":"Resources","text":"<ul> <li>Presentations<ul> <li>Slides (Google Slides)</li> </ul> </li> <li>Books<ul> <li>Haskell Programming from First Principles (Haskell Book) by Christopher Allen and Julie Moronuki</li> <li>Domain  Modeling Made Functional by Scott Wlaschin</li> <li>Functional-Light JavaScript by Kyle Simpson</li> </ul> </li> <li>Articles<ul> <li>Thinking Functionally (series) by Scott Wlaschin</li> <li>SOLID: The Next Step is Functional by Mark Seemann</li> </ul> </li> <li>Podcasts<ul> <li>LambdaCast</li> <li>FunctionalGeekery</li> </ul> </li> <li>Code<ul> <li>CRUDy (GitHub)</li> <li>Optionally (GitHub)</li> <li>Blackjack (GitHub)</li> </ul> </li> </ul>"},{"location":"presentations/#learning-functional-programming-through-construction-first-principles","title":"Learning Functional Programming Through Construction: First Principles","text":"<p>In the past five years, functional programming has increased dramatically in popularity which has lead to an explosion of resources in learning these concepts. But, between languages (Haskell, Elm, PureScript, F#), libraries (Ramda, fp-ts), and concepts (Monads, Monoids, Functors), it can be overwhelming in determining where to start and how to begin.</p> <p>In this talk, I\u2019m going to show you three fundamental concepts of functional programming: pure functions, immutability, and composition by not only explanation but we will walk through building these concepts into your code and applications. As we explore each concept, I\u2019ll show you the advantages of following these principles, how they will improve your development experience, and how they will set the stage for more advanced ideas.</p> <p>Intended for those who have experience with TypeScript or C#, by the end of this presentation, you will understand how pure functions lead to easier to test code, how immutability makes debugging easier, and how the power of compositions allows us to build bigger applications by combining smaller applications.</p>"},{"location":"presentations/#resources_5","title":"Resources","text":"<ul> <li>Presentation<ul> <li>TypeScript Slides(Google Slides)</li> <li>C# Slides(Google Slides)</li> </ul> </li> <li>Code Repository via GitHub </li> <li>Videos<ul> <li>Professor Frisby Introduces Composable Functional JavaScript</li> </ul> </li> <li>Articles<ul> <li>Thinking Functionally (Series) by Scott Wlaschin</li> </ul> </li> <li>Books<ul> <li>Domain  Modeling Made Functional by Scott Wlaschin</li> <li>Professor Frisby\u2019s Mostly Adequate Guide to Functional Programming by Brian Lonsdorf</li> <li>Functional Light JavaScript by Kyle Simpson</li> </ul> </li> <li>Code<ul> <li>FPThroughConstruction-Fundamentals (GitHub)</li> </ul> </li> </ul>"},{"location":"presentations/#level-up-on-functional-programming-by-rebuilding-linq","title":"Level Up on Functional Programming By Rebuilding LINQ","text":"<p>When taking your first steps into functional programming, you\u2019ll quickly run into blog posts talking about how to simplify complex loops by using the trinity of list operations: map, filter, and reduce. But what are these methods actually doing under the hood to help simplify your code?</p> <p>If you\u2019re familiar with the .NET landscape, then you already have experience with an implementation of the trinity within LINQ (Language Integrated Query).However, what if I told you that by learning how LINQ implements map (Select), filter (Where), and reduce (Aggregate), you actually know how other languages implement the same constructs?</p> <p>Intended for C# developers who have some experience with LINQ and an interest in learning functional programming, by the end of this session, you will have learned how to work with the Func type, generics, and extension methods!</p>"},{"location":"presentations/#resources_6","title":"Resources","text":"<ul> <li>Presentation<ul> <li>Slides (Google Drive)</li> </ul> </li> <li>Articles<ul> <li>JavaScript Map, Reduce, and Filter \u2013 JS Array Functions Explained with Code Examples</li> <li>Microsoft Docs on Extension Methods</li> <li>Microsoft Docs on Generics</li> <li>Microsoft Docs on Funcs</li> </ul> </li> <li>Code<ul> <li>LevelUpWithLinq (code used for the presentation on GitHub)</li> <li>LINQ.Select on Microsoft GitHub</li> <li>LINQ.Where on Microsoft GitHub</li> <li>LINQ.Aggregate on Microsoft GitHub</li> </ul> </li> </ul>"},{"location":"presentations/#taking-a-gamble-with-functional-domain-modeling","title":"Taking a Gamble With Functional Domain Modeling","text":"<p>Over the past few years, Functional Programming has entered the mainstream when it comes to libraries and frameworks for various kinds of technologies and there are plenty of tutorials and resources to learn the basics.</p> <p>With that being said, when it comes to building something non-trivial, there\u2019s a gap between what tutorials cover and real-world situations. For example, how do we model a system that makes illegal states unrepresentable? How do we handle operations that can fail? How do we combine business rules with application rules?</p> <p>In this talk, I\u2019m going to show you how to tackle these concerns as we model the game of Blackjack. By doubling down on concepts like algebraic data types, Options, and Results, you\u2019ll learn how to build applications that work in the real world.</p>"},{"location":"presentations/#resources_7","title":"Resources","text":"<ul> <li>Presentations<ul> <li>Domain Modeling Made Functional by Scott Wlaschin (YouTube)</li> <li>Slides (Google Slides)</li> </ul> </li> <li>Articles<ul> <li>Thinking Functionally (series) by Scott Wlaschin</li> <li>Functional Architecture : a definition by Mark Seemann</li> <li>Functional Architecture Is Ports and Adapters by Mark Seemann</li> <li>Algebraic Data Types Aren\u2019t Numbers on Steroids by Mark Seemann</li> </ul> </li> <li>Books<ul> <li>Domain Modeling Made Functional by Scott Wlaschin</li> </ul> </li> <li>Code<ul> <li>Source Code (on GitHub)</li> </ul> </li> </ul>"},{"location":"presentations/#having-a-whale-of-a-good-time-with-docker","title":"Having a Whale Of a Good Time With Docker","text":"<p>Docker and other containerization technologies have been around for a while now but it\u2019s not always obvious what the advantages of this new technology and why we should be using it. Over the past year, I\u2019ve found myself utilizing Docker more and more and it has completely changed the way I do work. In this presentation, we\u2019ll take a 10,000 foot view of Docker, what it is and how to use it. From there, we\u2019ll explore use standard use cases and examine how to create your own images. By the end of this session, you\u2019ll be able to explain Docker to your fellow engineers and will hopefully be inspired to start applying these principles and practices to your own work!</p>"},{"location":"presentations/#resources_8","title":"Resources","text":"<ul> <li>Presentations<ul> <li>Slides</li> </ul> </li> <li>Code<ul> <li>Source Code (on GitHub)</li> </ul> </li> </ul>"},{"location":"presentations/#the-engineers-playbook-starting-a-new-role","title":"The Engineer\u2019s Playbook: Starting a New Role","text":"<p>Starting a new role is never easy. There are a ton of ideas, new processes, people, and new technology to pick up. In addition, you may have also changed problem domains, which means you\u2019re having to learn about the problem we\u2019re trying to solve. That\u2019s a lot to take in, \u201cdrinking from the firehose\u201d, if you will. That being said, how do we control the flow? How do we optimize our learning to be effective in the short term and not need six months of ramp-up time?</p> <p>In this session, I\u2019m going to walk you through my playbook for starting a new role by exploring the four pillars of knowledge (product, people, process, and technology) and how they relate to each other. From there, we\u2019ll explore each of the pillars more in-depth, looking at what questions are urgent and important and which ones are just important. Finally, we\u2019ll wrap up on exploring timelines of what information you need to know by the end of your first week, your first sprint, first month, and first quarter.</p> <p>Intended for engineers of all experience levels, you will have an example playbook that you can use when you start a new role by the end of this session.</p>"},{"location":"presentations/#resources_9","title":"Resources","text":"<ul> <li>Slides (via Google Drive)</li> <li>Trello Board</li> <li>Question(s) of the Day</li> </ul>"},{"location":"presentations/#removing-repetition-by-building-your-own-automation-framework","title":"Removing Repetition By Building Your Own Automation Framework","text":"<p>As engineers, we don't like repetition and we want to automate as much as possible. Sometimes, this can be with our tasks, but other times, it's by improving the processes for our team or department. We could have our automation run locally on our box, but how do we share this with others? How can we build out patterns for others to follow so that they can be successful? </p> <p>In this session, I'm going to walk you through three different problems I needed to solve and my approach for building out an automation framework using TypeScript, Deno, and GitHub Actions, however, this approach is general enough that it could be done with other technology choices. </p> <p>Intended for engineers who are interested in building out their own workflows that can be shared with others, by the end of this session, you will know how to approach designing and building out your own automation framework.</p>"},{"location":"presentations/#resources_10","title":"Resources","text":"<ul> <li>Slide Deck (via Google Slides)</li> <li>GitHub Repo</li> <li>Creating A Slack Application</li> <li>Formatting Slack Posts</li> <li>GitHub Action Triggers</li> <li>Deno Docs</li> <li>PagerDuty Docs</li> <li>Building Your Own Coffee Chat Bot Series</li> <li>Building Your Own Bot For Finding Stale Docs Series</li> </ul>"},{"location":"presentations/#preventing-bugs-with-better-types-an-intro-to-type-driven-design","title":"Preventing Bugs with Better Types - An Intro to Type Driven Design","text":"<p>Have you ever shipped a logic error to production? What about writing code that allowed you to do something that violated business rules? Wouldn't it have been nice if the code itself prevented you from doing the wrong thing?</p> <p>By leveraging key concepts from Functional Programming, I'll show you how you can design your business domain such that your code can't get into a bad state (or at the very least, without jumping through some hoops).</p> <p>Intended for those with C# or TypeScript, I'll demonstrate concepts like sum types/product types for better data modeling and design patterns like Option/Result to handle when things can go wrong while we build out the game of Blackjack.</p>"},{"location":"presentations/#resources_11","title":"Resources","text":"<ul> <li>Slides (via Google Slides)</li> </ul>"},{"location":"resume/","title":"P. Cameron Presley - Resume","text":""},{"location":"resume/#table-of-contents","title":"Table of Contents","text":"<ul> <li>About Me</li> <li>Experience</li> <li>Community Involvement</li> <li>Certifications</li> <li>Education</li> </ul>"},{"location":"resume/#about-me","title":"About Me","text":"<p>I've been a software engineer for over a decade and in leadership for over five years. I pride myself on helping the team succeed while maintaining my technical aptitude, as I don't believe you can be a technical leader without the ability to work with your team.</p> <p>One of my greatest strengths is that I can explain anything to anyone, tailoring my message to be effective with my audience. With this ability, I'm a natural coach and have grown many engineers from intern-&gt;associate-&gt;mid-&gt;senior.</p> <p>Another strength is that I excel at looking at a process, understanding the goal, and then fine-tuning to remove friction. This can be how a team approaches work, the development lifecycle, or how we interact with clients. Because of this strength, I can level up a group quickly because I can amplify our strengths and make the weaknesses a non-issue.</p> <p>I enjoy learning new technology and have a track record of picking up new technologies on the fly to accomplish new tasks.</p>"},{"location":"resume/#experience","title":"Experience","text":""},{"location":"resume/#lean-techniques","title":"Lean TECHniques","text":""},{"location":"resume/#professional-software-consultant-aug-2023-may-2025","title":"Professional Software Consultant (Aug 2023 - May 2025)","text":"<p>As a Software Consultant, my engagements followed two patterns. The more common engagement is what we call the embedded \"player-coach,\" where I would be embedded into a feature delivery team to coach/mentor engineers while also delivering functionality. The other type of engagement is leadership coaching. I would work with a Team Lead or Engineering Manager to help streamline processes or provide guidance on navigating leadership topics like performance management, career progression, adopting an experimental mindset, and team process.</p> <p>Recent client accomplishments include:</p> <ul> <li>Developed and taught a custom training program for TypeScript and Amazon Web Services (AWS), focusing on building distributed systems leveraging tools like SNS, SQS, Lambda, Fargate, EventBridge, and DyanmoDB, all of which were managed by using the CDK for deployments.</li> <li>Designed, Implemented, and Deployed a custom solution solution to help combine Dynatrace alerts with Microsoft Teams to notify on-call when an issue was detected</li> <li>Created multiple PowerAutomate flows to automate support requests</li> <li>Worked with Team Leader, Product Owner, and Delivery Manager to overhaul the work breakdown process to ensure that the team knows the \"why\" behind the work and what \"done\" looks like:</li> </ul> <p>Recent company accomplishments include:</p> <ul> <li>Developed and led a mentorship program for interns, leading to a faster onboarding experience.</li> <li>Key contributor to a client account team (client was high 7 figure spend) to manage relationships and to find additional opportunities for Lean TECHniques</li> <li>Mentored and coached multiple associate and mid-level consultants to their next career progression</li> <li>Presented multiple internal trainings on topics like post-mortems and functional programming.</li> </ul>"},{"location":"resume/#rocket-central","title":"Rocket Central","text":""},{"location":"resume/#senior-team-leader-aug-2022-aug-2023","title":"Senior Team Leader (Aug 2022 - Aug 2023)","text":"<p>As part of new priorities, the team and I were moved to Rocket Central to work on a new project, the MyRocket Dashboard, a new offering to help clients navigate the home-buying journey.</p> <p>During this project, I worked with the team to design, develop, deploy, and support the API layer for the application. TypeScript and Nestjs were used to build the REST API with Node for the runtime, all deployed to AWS leveraging API Gateway and Lambda with data stored in DynamoDB. All infrastructure was managed through Terraform, which was deployed via CircleCI.</p> <p>Specific responsibilities included:</p> <ul> <li>Being the Launch Coordinate for the MyRocket Dashboard (launched to well over a million users)</li> <li>Conducted multiple performance reviews with both my team and other teams</li> <li>Promoted engineers from Associate to Mid and from Mid to Senior</li> <li>Key component of interviewing for both engineers and team leaders</li> <li>Led multiple engineering teams (3) when leaders were not available for an extended period</li> <li>Mentored other team leaders on experiments, how to have effective one-on-ones, and performance management</li> </ul>"},{"location":"resume/#rocket-mortgage","title":"Rocket Mortgage","text":""},{"location":"resume/#senior-team-leader-sept-2021-aug-2022","title":"Senior Team Leader (Sept 2021 - Aug 2022)","text":"<p>As a Senior Team Leader, I was responsible for a delivery team (8 members) in the Rocket Pro TPO space, where we helped our partners with the home loan origination process for their clients.</p> <p>Accomplishments:</p> <ul> <li>Overhauled the delivery process by cutting out not-needed meetings, changing how stories were refined and broken down, and drastically reducing lead and cycle time (going from 1-2 stories to 10 stories a sprint).</li> <li>Established a formal support rotation process for the team, which had improvements like a support playbook, a set of data quality \u00a0scripts, and domain training from the product owner to the team</li> <li>Created specialized training for the engineers to reduce bus factors (cross-training in domains and technology stacks)</li> <li>Created Team Lead Office Hours where leads could talk about issues they're running into and get advice from others (Mastermind Group)</li> <li>Overhauled the reputation of the team by improving the quality of work and the speed of shipping</li> </ul> <p>Responsibilities:</p> <ul> <li>Conducted multiple performance reviews with both my team and other teams</li> <li>Promoted engineers from Associate to Mid and from Mid to Senior</li> <li>Key component of interviewing for both engineers and team leaders</li> <li>Mentored other team leaders on experiments, how to have effective one-on-ones, and performance management</li> </ul> <p>Common technologies used included C#, ASP.NET Core, Postgres, AWS, Dynatrace, CircleCI, Angular, and TypeScript.</p>"},{"location":"resume/#sentryone","title":"SentryOne","text":""},{"location":"resume/#staff-software-engineer-feb-2021-sep-2021","title":"Staff Software Engineer (Feb 2021 - Sep 2021)","text":""},{"location":"resume/#technical-lead-developer-aug-2020-feb-2021","title":"Technical Lead Developer (Aug 2020 - Feb 2021)","text":""},{"location":"resume/#software-development-engineer-lead-jan-2019-aug-2020","title":"Software Development Engineer Lead (Jan 2019 - Aug 2020)","text":""},{"location":"resume/#senior-software-engineer-jul-2018-jan-2019","title":"Senior Software Engineer (Jul 2018 - Jan 2019)","text":"<p>As a Senior Software Engineer, I was responsible for the SentryOne Plan Explorer product. This tool allowed database admins to see the execution plan for their queries and offer suggestions for improvements. During development, I would deliver new functionality, pair with other engineers to get them unstuck and teach engineering practices (test-driven development, automated testing, and improving deployments).</p> <p>As an Engineering Lead and Staff Software Engineer, I led a software delivery team (5 members) to work on our flagship product, SentryOne. I was responsible I was responsible for both technical decisions (e.g., how to approach a problem) and \u00a0</p> <p>Accomplishments:</p> <ul> <li>Added a highly requested feature in a short period of time that landed a seven-figure deal</li> <li>Built a brand new team of recently hired interns such that they were performing as strong as teams with mid and senior engineers</li> <li>Ported Azure DevOps pipelines from GUI to YAML</li> <li>Led the team to launch SentryOne Monitor, our first SaaS solution for performance monitoring using tools like Docker, C#, .NET Core, Azure, and Terraform</li> <li>Onboarded a new team into a new tech stack with MySQL, Golang, Docker, Kubernetes, React, and AWS</li> <li>Created the desktop installer that allowed us to launch our first web product</li> </ul> <p>Responsibilities:</p> <ul> <li>Coached associate engineers to mid-level engineers</li> <li>Led technical breakdown of features and architecture decisions</li> <li>Completed performance reviews, bonuses, raises, and coaching conversations for the team</li> <li>Worked with the members of the team to define career goals and how to get them there</li> <li>Conducted interviews for engineers</li> <li>Partnered with our support team to triage and assign support requests</li> <li>Worked with Product to define a roadmap and work breakdown for the team</li> </ul>"},{"location":"resume/#pilot-flying-j","title":"Pilot Flying J","text":""},{"location":"resume/#application-architect-ii-jun-2017-jul-2018","title":"Application Architect II (Jun 2017 - Jul 2018)","text":""},{"location":"resume/#senior-software-engineer-aug-2016-jun-2017","title":"Senior Software Engineer (Aug 2016 - Jun 2017)","text":"<p>As an Architect, I was responsible for designing and architecting various solutions for all of our delivery teams. This would include monolithic applications to distributed messaging systems and microservice architectures. The main goal was to help each team to be more effective than they would be by themselves.</p> <p>Accomplishments:</p> <ul> <li>Migrated multiple Team Foundation Version Control (TFVC) repositories to Git (while maintaining history)</li> <li>Implemented and deployed an enterprise-wide solution for Continuous Integration/Continuous Deployment pipelines leveraging Azure DevOps</li> <li>Created and taught multiple delivery teams on automated testing, test-driven development, and overall good engineering practices</li> <li>Worked on an internal tool, Sparkplug, that would generate application architecture for \"well-known\" problems at Pilot.</li> <li>Build relationships with the Network and Server team to embrace a \"DevOps\" mindset where developers owned deployments and support, but the Network and Server helped us with overall system health (prelude to a platform team)</li> </ul> <p>Technologies used include C#, VB.NET, ASP.NET Core, WinForms, MS SQL Server, Docker, Git, Azure DevOps, TypeScript, JavaScript, Angular, React, and PowerShell.</p>"},{"location":"resume/#invizion-llc","title":"InVizion LLC","text":""},{"location":"resume/#programmer-analyst-iii-aug-2014-aug-2016","title":"Programmer Analyst III (Aug 2014 - Aug 2016)","text":"<p>As an Analyst III, I was responsible for the development and quality of our flagship product, the InVizion Analyzer, a tool that allows project managers to complete multiple \"what-if\" scenarios for resource management.</p>"},{"location":"resume/#accomplishments","title":"Accomplishments","text":"<ul> <li>To improve the release process, I fine-tuned our installer creation step from taking hours to minutes by automating the process using WiX.</li> <li>When a contractor could not finish a project, I designed and developed a solution that got the project back on track.</li> <li>To reduce regressions, I spearheaded writing automated tests for our codebase and training other engineers on how to test as well. In the first year, coverage improved from 10% to 55%, which helped dramatically reduce our bug count.</li> <li>Implemented a feature to have a flexible start month for the Fiscal Year, which allowed us to start selling the product to non-government entities.</li> </ul> <p>Technologies used include C#, WinForms, MS SQL Server, GitHub, Jenkins, DevExpress, and Telerik.</p>"},{"location":"resume/#daxor-corporation","title":"DAXOR Corporation","text":""},{"location":"resume/#software-engineer-sep-2012-aug-2014","title":"Software Engineer (Sep 2012 - Aug 2014)","text":"<p>As a Software Engineer, I was responsible for the flagship product, the Blood Volume Analyzer 100, a diagnostic medical device used in a nuclear lab.</p>"},{"location":"resume/#accomplishments_1","title":"Accomplishments","text":"<ul> <li>Created a Continuous Integration pipeline using Jenkins</li> <li>Created an error number code system that streamlined our support processes</li> <li>Implemented a new module, Quality Control, that allowed users to use our tool for calibration tests</li> <li>Created an Android application to facilitate communication between users and DAXOR</li> </ul> <p>Technologies used include Jenkins, C#, Windows Presentation Framework (WPF), Windows Communication Framework (WCF), MS SQL Server, Jenkins, TFS, and JIRA.</p>"},{"location":"resume/#epic-health-systems","title":"Epic Health Systems","text":""},{"location":"resume/#software-developer-oct-2011-aug-2012","title":"Software Developer (Oct 2011 - Aug 2012)","text":"<p>As a Software Developer, I supported the Data Courier and EMFI applications, tools that allow hospitals to send data from one environment to another.</p>"},{"location":"resume/#responsibilities","title":"Responsibilities","text":"<ul> <li>Fixed miscellaneous bugs</li> <li>Attended various go-live events to help troubleshoot issues</li> <li>Worked with QA to ensure that the right thing was being built</li> </ul> <p>Technologies used include MUMPS, VB6, Subversion, and ActiveX.</p>"},{"location":"resume/#community-involvement","title":"Community Involvement","text":"<ul> <li>Presented at over 70 conferences (see https://blog.thesoftwarementor.com/speaking-schedule)</li> <li>Microsoft MVP in Developer Technologies (2018-2024)</li> <li>Director of Speaker Relations for CodeStock technical conference (2018-2022)</li> </ul>"},{"location":"resume/#certifications","title":"Certifications","text":"<ul> <li>Microsoft Certified: Azure Administrator Associate</li> <li>Microsoft Certified: Azure Developer Associate</li> <li>Microsoft Certified: DevOps Engineer Expert</li> <li>LaunchDarkly Bronze Developer Certification</li> <li>LaunchDarkly Silver Developer Certification</li> <li>LaunchDarkly Gold Developer Certification</li> <li>LaunchDarkly Platinum Developer Certification</li> <li>Graph Developer Associate - Apollo GraphQL</li> </ul>"},{"location":"resume/#education","title":"Education","text":""},{"location":"resume/#maryville-college-maryville-tn","title":"Maryville College - Maryville, TN","text":"<ul> <li>Bachelor of Arts - Computer Science</li> <li>Bachelor of Arts - Mathematics</li> </ul>"},{"location":"speaking-schedule/","title":"Speaking Schedule","text":""},{"location":"speaking-schedule/#upcoming-sessions","title":"Upcoming Sessions","text":""},{"location":"speaking-schedule/#cleveland-c-user-group-cleveland-oh-mar-25-2026-more-info","title":"Cleveland C# User Group - Cleveland, OH (Mar 25, 2026) more info","text":"<ul> <li>Learning Functional Programming Through Construction: First Principles</li> </ul>"},{"location":"speaking-schedule/#roanoke-valley-net-user-group-salem-va-apr-2-2026-more-info","title":"Roanoke Valley .NET User Group - Salem, VA (Apr 2, 2026) more info","text":"<ul> <li>Preventing Bugs with Better Types - An Intro to Type Driven Design</li> </ul>"},{"location":"speaking-schedule/#codestock-knoxville-tn-apr-9-10-2026-more-info","title":"CodeStock - Knoxville, TN (Apr 9-10, 2026) more info","text":"<ul> <li>Preventing Bugs with Better Types - An Intro to Type Driven Design</li> <li>The Engineer's Playbook: Starting a New Role </li> </ul>"},{"location":"speaking-schedule/#stirtrek-colombus-oh-may-1-2026-more-info","title":"StirTrek - Colombus, OH (May 1, 2026) more info","text":"<ul> <li>Cutting Through the Hype - Building High Performing Teams</li> </ul>"},{"location":"speaking-schedule/#past-sessions","title":"Past Sessions","text":""},{"location":"speaking-schedule/#alburquerque-net-user-group-virtual-jan-28-2026-more-info","title":"Alburquerque .NET User Group - Virtual (Jan 28, 2026) more info","text":"<ul> <li>Learning Functional Programming Through Construction: First Principles</li> </ul>"},{"location":"speaking-schedule/#pittsburgh-net-user-group-virtual-oct-30-2025-more-info","title":"Pittsburgh .NET User Group - Virtual (Oct 30, 2025) more info","text":"<ul> <li>Learning Functional Programming Through Construction: First Principles </li> </ul>"},{"location":"speaking-schedule/#pittsburgh-tech-fest-pittsburgh-pa-oct-24-2025-more-info","title":"Pittsburgh Tech Fest - Pittsburgh, PA (Oct 24, 2025) more info","text":"<ul> <li>The Engineer's Playbook: Starting a New Role </li> </ul>"},{"location":"speaking-schedule/#nova-code-camp-reston-va-oct-18-2025-more-info","title":"NoVA Code Camp - Reston, VA (Oct 18, 2025) more info","text":"<ul> <li>Learning Functional Programming Through Construction: First Principles</li> </ul>"},{"location":"speaking-schedule/#atlanta-developers-conference-atlanta-ga-oct-11-2025-more-info","title":"Atlanta Developers' Conference - Atlanta, GA (Oct 11, 2025) more info","text":"<ul> <li>The Engineer's Playbook: Starting a New Role </li> </ul>"},{"location":"speaking-schedule/#cincydeliver-mason-oh-aug-1-2025-more-info","title":"CincyDeliver - Mason, OH (Aug 1, 2025) more info","text":"<ul> <li>Learning Functional Programming Through Construction: First Principles</li> </ul>"},{"location":"speaking-schedule/#san-antonio-net-user-group-virtual-june-26-2025-more-info","title":"San Antonio .NET User Group - Virtual (June 26, 2025) more info","text":"<ul> <li>Level up on Functional Programming by rebuilding LINQ</li> </ul>"},{"location":"speaking-schedule/#atlanta-javascript-user-group-atlanta-ga-june-24-2025-more-info","title":"Atlanta JavaScript User Group - Atlanta, GA (June 24, 2025) more info","text":"<ul> <li>Learning Functional Programming Through Construction: First Principles</li> </ul>"},{"location":"speaking-schedule/#atlanta-net-user-group-alpharetta-ga-june-23-2025-more-info","title":"Atlanta .NET User Group - Alpharetta, GA (June 23, 2025) more info","text":"<ul> <li>Level up on Functional Programming by rebuilding LINQ</li> </ul>"},{"location":"speaking-schedule/#indy-net-consortium-indianapolis-in-may-7-2025-more-info","title":"Indy .NET Consortium - Indianapolis, IN (May 7, 2025) more info","text":"<ul> <li>Level up on Functional Programming by rebuilding LINQ</li> </ul>"},{"location":"speaking-schedule/#capital-area-net-user-group-loudoun-va-may-6-2025-more-info","title":"Capital Area .NET User Group - Loudoun, VA (May 6, 2025) more info","text":"<ul> <li>Preventing Bugs with Better Types - An Intro to Type Driven Design</li> </ul>"},{"location":"speaking-schedule/#stirtrek-colombus-oh-may-2-2025-more-info","title":"StirTrek - Colombus, OH (May 2, 2025) more info","text":"<ul> <li>Taking a Gamble With Functional Domain Modeling</li> </ul>"},{"location":"speaking-schedule/#atlanta-net-user-group-alpharetta-ga-april-28-2025-more-info","title":"Atlanta .NET User Group - Alpharetta, GA (April 28, 2025) more info","text":"<ul> <li>Learning Functional Programming Through Construction: First Principles</li> </ul>"},{"location":"speaking-schedule/#roanoke-valley-net-user-group-salem-va-mar-6-2025-more-info","title":"Roanoke Valley .NET User Group - Salem, VA (Mar 6, 2025) more info","text":"<ul> <li>Learning Functional Programming Through Construction: First Principles</li> </ul>"},{"location":"speaking-schedule/#central-ohio-net-user-group-colombus-oh-feb-27-2025-more-info","title":"Central Ohio .NET User Group - Colombus, OH (Feb 27, 2025) more info","text":"<ul> <li>Level up on Functional Programming by rebuilding LINQ</li> </ul>"},{"location":"speaking-schedule/#cleveland-c-user-group-cleveland-oh-jan-22-2025-more-info","title":"Cleveland C# User Group - Cleveland, OH (Jan 22, 2025) more info","text":"<ul> <li>Level up on Functional Programming by rebuilding LINQ</li> </ul>"},{"location":"speaking-schedule/#tulsa-developer-association-tulsa-ok-dec-17-2024-more-info","title":"Tulsa Developer Association - Tulsa, OK (Dec 17, 2024) more info","text":"<ul> <li>Level up on Functional Programming by rebuilding LINQ</li> </ul>"},{"location":"speaking-schedule/#connecttech-atlanta-ga-nov-18-20-2024-more-info","title":"Connect.Tech - Atlanta, GA (Nov 18-20, 2024) more info","text":"<ul> <li>Removing Repetition By Building Your Own Automation Framework</li> </ul>"},{"location":"speaking-schedule/#queen-city-code-retreat-cornelius-nc-nov-9-2024-more-info","title":"Queen City Code Retreat - Cornelius, NC (Nov 9, 2024) more info","text":"<ul> <li>Come join us for a fun day of coding!</li> </ul>"},{"location":"speaking-schedule/#lean-bytes-virtual-nov-5-2024-more-info","title":"LEAN BYTES - Virtual (Nov 5, 2024) more info","text":"<ul> <li>Learning from Failure: Post-Mortems for More Resilient Teams (YouTube Recording)</li> </ul>"},{"location":"speaking-schedule/#concord-software-developers-concord-nc-sept-12-2024-more-info","title":"Concord Software Developers - Concord, NC (Sept 12, 2024) more info","text":"<ul> <li>Learning Functional Programming Through Construction: First Principles</li> </ul>"},{"location":"speaking-schedule/#springfield-dev-night-virtual-sept-4-2024-more-info","title":"Springfield Dev Night - Virtual (Sept 4, 2024) more info","text":"<ul> <li>Learning Functional Programming Through Construction: First Principles</li> </ul>"},{"location":"speaking-schedule/#carolina-code-conference-greenville-sc-aug-23-24-2024-more-info","title":"Carolina Code Conference - Greenville, SC (Aug 23-24, 2024) more info","text":"<ul> <li>Learning Functional Programming Through Construction: First Principles</li> </ul>"},{"location":"speaking-schedule/#twin-cities-net-user-group-virtual-aug-1-2024-more-info","title":"Twin Cities .NET User Group - Virtual (Aug 1, 2024) more info","text":"<ul> <li>How Functional Programming Made Me a Better Developer</li> </ul>"},{"location":"speaking-schedule/#javascript-kc-virtual-may-22-2024-more-info","title":"JavaScript KC - Virtual (May 22, 2024) more info","text":"<ul> <li>Learning Functional Programming Through Construction: First Principles</li> </ul>"},{"location":"speaking-schedule/#detroit-tech-watch-virtual-may-21-2024-more-info","title":"Detroit Tech Watch - Virtual (May 21, 2024) more info","text":"<ul> <li>Functional Domain Modeling with Blackjack</li> </ul>"},{"location":"speaking-schedule/#iowa-net-user-group-virtual-may-2-2024-more-info","title":"Iowa .NET User Group - Virtual (May 2, 2024) more info","text":"<ul> <li>Level up on Functional Programming by rebuilding LINQ</li> </ul>"},{"location":"speaking-schedule/#codemash-sandusky-oh-jan-10-13-2023-more-info","title":"CodeMash - Sandusky, OH (Jan 10-13, 2023) more info","text":"<ul> <li>Level up on Functional Programming by rebuilding LINQ</li> <li>Learning Functional Programming Through Construction: First Principles</li> <li>The Engineer's Playbook: Starting a New Role</li> </ul>"},{"location":"speaking-schedule/#granite-state-code-camp-virtual-nov-12-2022-more-info","title":"Granite State Code Camp - Virtual (Nov 12, 2022) more info","text":"<ul> <li>Learning Functional Programming Through Construction: First Principles</li> </ul>"},{"location":"speaking-schedule/#techbash-pocono-manor-pa-nov-8-11-2022-more-info","title":"TechBash - Pocono Manor, PA (Nov 8 - 11, 2022) more info","text":"<ul> <li>Learning Functional Programming Through Construction: First Principles</li> <li>Having a Whale of a Good Time with Docker</li> </ul>"},{"location":"speaking-schedule/#capital-area-net-user-group-nov-1-2022-more-info","title":"Capital Area .NET User Group (Nov 1, 2022) more info","text":"<ul> <li>Level up on Functional Programming by rebuilding LINQ</li> </ul>"},{"location":"speaking-schedule/#atlanta-developers-conference-marietta-ga-sept-17-2022-more-info","title":"Atlanta Developer's Conference, Marietta, GA (Sept 17, 2022) more info","text":"<ul> <li>How Functional Programming Made Me a Better Developer</li> </ul>"},{"location":"speaking-schedule/#glugnet-user-group-okemos-mi-sept-15-2022-more-info","title":"GLUGnet User Group - Okemos, MI (Sept 15, 2022) more info","text":"<ul> <li>How To Have Code Reviews Developers Actually Want</li> </ul>"},{"location":"speaking-schedule/#codepalousa-louisville-ky-aug-17-19-2022","title":"CodePaLOUsa - Louisville, KY (Aug 17-19, 2022)","text":"<ul> <li>Level up on Functional Programming by rebuilding LINQ</li> </ul>"},{"location":"speaking-schedule/#beer-city-code-grand-rapids-mi-aug-6-2022-more-info","title":"Beer City Code - Grand Rapids, MI (Aug 6, 2022) more info","text":"<ul> <li>How To Have Code Reviews Developers Actually Want</li> <li>Learning Functional Programming Through Construction: First Principles</li> </ul>"},{"location":"speaking-schedule/#cincydeliver-cincinnati-oh-july-29-2022-more-info","title":"CincyDeliver - Cincinnati, OH (July 29, 2022) more info","text":"<ul> <li>How To Have Code Reviews Developers Actually Want</li> </ul>"},{"location":"speaking-schedule/#scenic-city-summit-chattanooga-tn-july-22-2022-more-info","title":"Scenic City Summit - Chattanooga, TN (July 22, 2022) more info","text":"<ul> <li>The Engineer's Playbook: Starting a New Role</li> <li>Level up on Functional Programming by rebuilding LINQ</li> </ul>"},{"location":"speaking-schedule/#qc-bytes-virtual-mar-29-2022-more-info","title":"QC Bytes - Virtual (Mar 29, 2022) more info","text":"<ul> <li>The Engineer's Playbook: Starting a New Role</li> </ul>"},{"location":"speaking-schedule/#philly-code-camp-virtual-mar-4-2022-more-info","title":"Philly Code Camp - Virtual (Mar 4, 2022) more info","text":"<ul> <li>Establishing a SOLID Foundation \u2013 An Intro to Software Design</li> <li>Level up on Functional Programming by rebuilding LINQ</li> <li>How To Have Code Reviews Developers Actually Want</li> </ul>"},{"location":"speaking-schedule/#granite-state-code-camp-manchester-nh-nov-6-2021-more-info","title":"Granite State Code Camp - Manchester, NH (Nov 6, 2021) more info","text":"<ul> <li>How To Have Code Reviews Developers Actually Want</li> </ul>"},{"location":"speaking-schedule/#tech-con-detroit-mi-oct-20-2021-more-info","title":"Tech Con - Detroit, MI (Oct 20, 2021 more info","text":"<ul> <li>Level up on Functional Programming by rebuilding LINQ</li> </ul>"},{"location":"speaking-schedule/#northern-virginia-code-camp-reston-va-oct-16-2021-more-info","title":"Northern Virginia Code Camp - Reston, VA (Oct 16, 2021) more info","text":"<ul> <li>Learning Functional Programming Through Construction: First Principles</li> </ul>"},{"location":"speaking-schedule/#codepalousa-louisville-ky-aug-18-20-2021","title":"CodePaLOUsa - Louisville, KY (Aug 18-20, 2021)","text":"<ul> <li>Learning Functional Programming Through Construction: First Principles</li> <li>Taking a Gamble With Functional Domain Modeling</li> </ul>"},{"location":"speaking-schedule/#thatconference-wisconsin-dells-wi-jul-26-28-2021-more-info","title":"ThatConference - Wisconsin Dells, WI (Jul 26-28, 2021) more info","text":"<ul> <li>Learning Functional Programming Through Construction: First Principles</li> </ul>"},{"location":"speaking-schedule/#minnesota-developer-conference-minneapolis-mn-may-4-2021-more-info","title":"Minnesota Developer Conference - Minneapolis, MN (May 4, 2021) more info","text":"<ul> <li>How Functional Programming Made Me a Better Developer</li> </ul>"},{"location":"speaking-schedule/#tech-valley-net-user-group-latham-ny-apr-13-2021-more-info","title":"Tech Valley .NET User Group - Latham, NY (Apr 13, 2021) more info","text":"<ul> <li>Taking a Gamble With Functional Domain Modeling</li> </ul>"},{"location":"speaking-schedule/#nashville-women-programmers-nashville-tn-mar-8-2021-more-info","title":"Nashville Women Programmers - Nashville, TN (Mar 8, 2021) more info","text":"<ul> <li>How To Have Code Reviews Developers Actually Want</li> </ul>"},{"location":"speaking-schedule/#granite-state-code-camp-manchester-nh-nov-14-2020-more-info","title":"Granite State Code Camp - Manchester, NH (Nov 14, 2020) more info","text":"<ul> <li>How Functional Programming Made Me a Better Developer</li> </ul>"},{"location":"speaking-schedule/#scenic-city-summit-chattanooga-tn-nov-6-2020-more-info","title":"Scenic City Summit - Chattanooga, TN (Nov 6, 2020) more info","text":"<ul> <li>Learning Functional Programming Through Construction: First Principles</li> </ul>"},{"location":"speaking-schedule/#glugnet-user-group-okemos-mi-oct-15-2020-more-info","title":"GLUGnet User Group - Okemos, MI (Oct 15, 2020) more info","text":"<ul> <li>Taking a Gamble With Functional Domain Modeling</li> </ul>"},{"location":"speaking-schedule/#quicken-loans-tech-con-virtual-sept-30-oct-1-2020-more-info","title":"Quicken Loans Tech Con - Virtual (Sept 30 - Oct 1, 2020) more info","text":"<ul> <li>How To Have Code Reviews Developers Actually Want</li> </ul>"},{"location":"speaking-schedule/#st-louis-net-user-group-st-louis-mo-sept-28-2020-more-info","title":"St. Louis .NET User Group \u2013 St. Louis, MO (Sept 28, 2020) more info","text":"<ul> <li>Learning Functional Programming Through Construction: First Principles</li> </ul>"},{"location":"speaking-schedule/#northern-virginia-code-camp-reston-va-sept-26-2020-more-info","title":"Northern Virginia Code Camp - Reston, VA (Sept 26, 2020) more info","text":"<ul> <li>Establishing a SOLID Foundation \u2013 An Intro to Software Design</li> </ul>"},{"location":"speaking-schedule/#indysa-caramel-in-sept-17-2020-more-info","title":"IndySA - Caramel, IN (Sept 17, 2020) more info","text":"<ul> <li>Level up on Functional Programming by rebuilding LINQ</li> </ul>"},{"location":"speaking-schedule/#central-ohio-net-developers-group-columbus-oh-aug-27-2020-more-info","title":"Central Ohio .NET Developer's Group \u2013 Columbus, OH (Aug 27, 2020) more info","text":"<ul> <li>Learning Functional Programming Through Construction: First Principles</li> </ul>"},{"location":"speaking-schedule/#northwest-valley-net-user-group-nwvdnug-more-info","title":"Northwest Valley .NET User Group (NWVDNUG) more info","text":"<ul> <li>Level up on Functional Programming by rebuilding LINQ</li> <li>Level up on Functional Programming by rebuilding LINQ</li> </ul>"},{"location":"speaking-schedule/#northwest-valley-net-user-group-nwvdnug-more-info_1","title":"Northwest Valley .NET User Group (NWVDNUG) more info","text":"<ul> <li>Level up on Functional Programming by rebuilding LINQ</li> <li>Level up on Functional Programming by rebuilding LINQ</li> </ul>"},{"location":"speaking-schedule/#nashville-net-user-group-nashville-tn-aug-11th-2020-more-info","title":"Nashville .NET User Group \u2013 Nashville, TN (Aug 11th, 2020) more info","text":"<ul> <li>Taking a Gamble With Functional Domain Modeling</li> </ul>"},{"location":"speaking-schedule/#kansas-city-net-user-group-kansas-city-mo-july-28-2020-more-info","title":"Kansas City .NET User Group \u2013 Kansas City, MO (July 28, 2020) more info","text":"<ul> <li>Level up on Functional Programming by rebuilding LINQ</li> </ul>"},{"location":"speaking-schedule/#modern-devs-charlotte-charlotte-nc-july-14-2020-more-info","title":"Modern Devs Charlotte \u2013 Charlotte, NC (July 14, 2020) more info","text":"<ul> <li>How Functional Programming Made Me a Better Developer</li> </ul>"},{"location":"speaking-schedule/#nyc-net-developers-new-york-ny-july-9-2020-more-info","title":"NYC .NET Developers \u2013 New York, NY (July 9, 2020) more info","text":"<ul> <li>Level up on Functional Programming by rebuilding LINQ</li> </ul>"},{"location":"speaking-schedule/#queen-city-bytes-charlotte-nc-may-19-2020-more-info","title":"Queen City Bytes \u2013 Charlotte, NC (May 19, 2020) more info","text":""},{"location":"speaking-schedule/#net-dc-user-group-washington-dc-april-21-2020-more-info","title":".NET DC User Group - Washington, DC (April 21, 2020) more info","text":"<ul> <li>Establishing a SOLID Foundation \u2013 An Intro to Software Design</li> </ul>"},{"location":"speaking-schedule/#indysa-caramel-in-april-16-2020-more-info","title":"IndySA - Caramel, IN (April 16, 2020) more info","text":"<ul> <li>How To Have Code Reviews Developers Actually Want</li> </ul>"},{"location":"speaking-schedule/#dev-up-st-louis-mo-oct-14-16-2019-more-info","title":"dev up - St. Louis, MO (Oct 14-16, 2019) more info","text":"<ul> <li>Establishing a SOLID Foundation \u2013 An Intro to Software Design</li> <li>How To Have Code Reviews Developers Actually Want</li> <li>How Functional Programming Made Me a Better Developer</li> </ul>"},{"location":"speaking-schedule/#devspace-huntsville-al-oct-11-12-2019-more-info","title":"DevSpace - Huntsville, AL (Oct 11-12, 2019) more info","text":"<ul> <li>How To Have Code Reviews Developers Actually Want</li> <li>Learning Functional Programming Through Construction: First Principles</li> </ul>"},{"location":"speaking-schedule/#scenic-city-summit-chattanooga-tn-oct-4-2019-more-info","title":"Scenic City Summit - Chattanooga, TN (Oct 4, 2019) more info","text":"<ul> <li>How Functional Programming Made Me a Better Developer</li> </ul>"},{"location":"speaking-schedule/#music-city-code-nashville-tn-sep-5-7-2019-more-info","title":"Music City Code - Nashville, TN (Sep 5-7, 2019) more info","text":"<ul> <li>How Functional Programming Made Me a Better Developer</li> </ul>"},{"location":"speaking-schedule/#codepalousa-louisville-ky-aug-21-23-2019","title":"CodePaLOUsa - Louisville, KY (Aug 21-23, 2019)","text":"<ul> <li>How To Have Code Reviews Developers Actually Want</li> <li>How Functional Programming Made Me a Better Developer</li> </ul>"},{"location":"speaking-schedule/#thatconference-wisconsin-dells-wi-aug-5-8-2019-more-info","title":"ThatConference - Wisconsin Dells, WI (Aug 5-8, 2019) more info","text":"<ul> <li>How Functional Programming Made Me a Better Developer</li> </ul>"},{"location":"speaking-schedule/#kansas-city-developer-conference-kansas-city-mo-jul-18-19-2019-more-info","title":"Kansas City Developer Conference \u2013 Kansas City, MO (Jul 18-19, 2019) more info","text":"<ul> <li>How To Have Code Reviews Developers Actually Want</li> <li>Establishing a SOLID Foundation \u2013 An Intro to Software Design</li> </ul>"},{"location":"speaking-schedule/#glugnet-user-group-remote-okemos-mi-jun-20-2019-more-info","title":"GLUGnet User Group - (Remote) Okemos, MI (Jun 20, 2019) more info","text":"<ul> <li>How Functional Programming Made Me a Better Developer</li> </ul>"},{"location":"speaking-schedule/#beer-city-code-grand-rapids-mi-may-31-jun-1-2019-more-info","title":"Beer City Code - Grand Rapids, MI (May 31-Jun 1, 2019) more info","text":"<ul> <li>Establishing a SOLID Foundation \u2013 An Intro to Software Design</li> </ul>"},{"location":"speaking-schedule/#northern-virginia-code-camp-reston-va-may-18-2019-more-info","title":"Northern Virginia Code Camp - Reston, VA (May 18, 2019) more info","text":"<ul> <li>How Functional Programming Made Me a Better Developer</li> </ul>"},{"location":"speaking-schedule/#chattanooga-area-net-user-group-chattanooga-tn-apr-9-2019-more-info","title":"Chattanooga Area .NET User Group - Chattanooga, TN (Apr 9, 2019) more info","text":""},{"location":"speaking-schedule/#connectaha-omaha-ne-mar-8-2019-more-info","title":"Connectaha - Omaha, NE (Mar 8, 2019) more info","text":"<ul> <li>How Functional Programming Made Me a Better Developer</li> </ul>"},{"location":"speaking-schedule/#windsor-essex-net-user-group-nov-27-2018-more-info","title":"Windsor-Essex .NET User Group (Nov 27, 2018) more info","text":"<ul> <li>Establishing a SOLID Foundation \u2013 An Intro to Software Design</li> </ul>"},{"location":"speaking-schedule/#devspace-huntsville-al-oct-12-13-2018-more-info","title":"DevSpace - Huntsville, AL (Oct 12-13, 2018) more info","text":"<ul> <li>How To Have Code Reviews Developers Actually Want</li> <li>How Functional Programming Made Me a Better Developer</li> </ul>"},{"location":"speaking-schedule/#atlanta-code-camp-marietta-ga-sept-15-2018-more-info","title":"Atlanta Code Camp - Marietta, GA (Sept 15, 2018) more info","text":"<ul> <li>How To Have Code Reviews Developers Actually Want</li> </ul>"},{"location":"speaking-schedule/#functional-knoxville-knoxville-tn-sept-11-2018-more-info","title":"Functional Knoxville - Knoxville, TN (Sept 11, 2018) more info","text":""},{"location":"speaking-schedule/#aim-heartland-developer-conference-la-vista-ne-sept-6-7-2018-more-info","title":"AIM Heartland Developer Conference - La Vista, NE (Sept 6-7, 2018) more info","text":"<ul> <li>How To Have Code Reviews Developers Actually Want</li> </ul>"},{"location":"speaking-schedule/#chattanooga-area-net-user-group-chattanooga-tn-aug-14-2018-more-info","title":"Chattanooga Area .NET User Group - Chattanooga, TN (Aug 14, 2018) more info","text":"<ul> <li>How To Have Code Reviews Developers Actually Want</li> </ul>"},{"location":"speaking-schedule/#code-on-the-beach-atlantic-beach-fl-aug-9-12-2018-more-info","title":"Code on the Beach - Atlantic Beach, FL (Aug 9-12, 2018) more info","text":"<ul> <li>How Functional Programming Made Me a Better Developer</li> </ul>"},{"location":"speaking-schedule/#carolina-codes-greenville-sc-jul-28-2018-more-info","title":"Carolina Codes - Greenville, SC (Jul 28, 2018) more info","text":"<ul> <li>How To Have Code Reviews Developers Actually Want</li> </ul>"},{"location":"speaking-schedule/#agile-knoxville-meetup-knoxville-tn-jul-19-2018-more-info","title":"Agile Knoxville Meetup - Knoxville, TN (Jul 19, 2018) more info","text":""},{"location":"speaking-schedule/#kcdc-kansas-city-mo-aug-2-4-2017-more-info","title":"KCDC - Kansas City, MO (Aug 2-4, 2017) more info","text":"<ul> <li>Taking a Gamble on F# - Implementing Blackjack</li> <li>Making the Unstable Stable - An Introduction to Testing</li> </ul>"},{"location":"speaking-schedule/#scenic-city-summit-chattanooga-tn-jul-28-2017-more-info","title":"Scenic City Summit - Chattanooga, TN (Jul 28, 2017) more info","text":"<ul> <li>How To Have Code Reviews Developers Actually Want</li> </ul>"},{"location":"speaking-schedule/#detroitcode-detroit-mi-jul-10-12-2017-more-info","title":"Detroit.Code() - Detroit, MI (Jul 10-12, 2017) more info","text":"<ul> <li>Establishing a SOLID Foundation \u2013 An Intro to Software Design</li> <li>How Functional Programming Made Me a Better Developer</li> </ul>"},{"location":"speaking-schedule/#chattanooga-area-net-user-group-jun-20-2017-more-info","title":"Chattanooga Area .NET User Group (Jun 20, 2017) more info","text":"<ul> <li>Making the Unstable Stable - An Introduction to Testing</li> </ul>"},{"location":"speaking-schedule/#codepalousa-louisville-ky-jun-7-9-2017","title":"CodePaLOUsa - Louisville, KY (Jun 7-9, 2017)","text":"<ul> <li>Making the Unstable Stable - An Introduction to Testing</li> <li>How to Have Code Reviews Developers Actually Want</li> <li>Establishing a SOLID Foundation \u2013 An Intro to Software Design</li> </ul>"},{"location":"speaking-schedule/#music-city-code-nashville-tn-jun-1-3-2017-more-info","title":"Music City Code - Nashville, TN (Jun 1-3, 2017) more info","text":"<ul> <li>How to Have Code Reviews Developers Actually Want</li> <li>How Functional Programming Made Me a Better Developer</li> </ul>"},{"location":"speaking-schedule/#codestock-knoxville-tn-may-5-6-2017-more-info","title":"CodeStock - Knoxville, TN (May 5-6, 2017) more info","text":"<ul> <li>How to Have Code Reviews Developers Actually Want</li> <li>How Functional Programming Made Me a Better Developer</li> </ul>"},{"location":"speaking-schedule/#indycode-indianapolis-in-mar-29-31-2017-more-info","title":"Indy.Code() - Indianapolis, IN (Mar 29-31, 2017) more info","text":"<ul> <li>How Functional Programming Made Me a Better Developer</li> <li>Taking a Gamble on F#: Implementing Blackjack</li> </ul>"},{"location":"speaking-schedule/#pellissippi-state-technical-community-college-knoxville-tn-feb-27-2017-more-info","title":"Pellissippi State Technical Community College - Knoxville, TN (Feb 27, 2017 more info","text":"<ul> <li>How to Have Code Reviews Developers Actually Want</li> </ul>"},{"location":"speaking-schedule/#detroit-dev-day-troy-mi-nov-12-2016-more-info","title":"Detroit Dev Day - Troy, MI (Nov 12, 2016) more info","text":"<ul> <li>Establishing a SOLID Foundation \u2013 An Intro to Software Design</li> </ul>"},{"location":"speaking-schedule/#devspace-huntsville-al-oct-14-15-2016-more-info","title":"DevSpace - Huntsville, AL (Oct 14-15, 2016) more info","text":"<ul> <li>Taking a Gamble with F#: Implementing Blackjack</li> <li>Making the Unstable Stable - An Introduction to Testing</li> <li>Establishing a SOLID Foundation \u2013 An Intro to Software Design</li> </ul>"},{"location":"speaking-schedule/#music-city-code-nashville-tn-aug-18-20-2016-more-info","title":"Music City Code - Nashville, TN (Aug 18-20, 2016) more info","text":"<ul> <li>Establishing a SOLID Foundation \u2013 An Intro to Software Design</li> </ul>"},{"location":"speaking-schedule/#thatconference-wisconsin-dells-wi-aug-8-10-2016-more-info","title":"ThatConference - Wisconsin Dells, WI (Aug 8-10, 2016) more info","text":"<ul> <li>Taking a Gamble on F#: Implementing Blackjack</li> </ul>"},{"location":"speaking-schedule/#codestock-knoxville-tn-july-15-16-2016-more-info","title":"CodeStock - Knoxville, TN (July 15-16, 2016 more info","text":"<ul> <li>Establishing a SOLID Foundation \u2013 An Intro to Software Design</li> <li>Making the Unstable Stable - An Introduction to Testing</li> </ul>"},{"location":"speaking-schedule/#devspace-huntsville-al-oct-9-10-2015-more-info","title":"DevSpace - Huntsville, AL (Oct 9-10, 2015) more info","text":"<ul> <li>Establishing a SOLID Foundation \u2013 An Intro to Software Design</li> <li>Making the Unstable Stable - An Introduction to Testing</li> </ul>"},{"location":"story/","title":"Why The Software Mentor?","text":"<p>Everyone has a reason behind what they do. Whether it's financial, altruism, or something in between, there's a motivation behind it. You can think about this as their story.</p> <p>Humans naturally gravitate to stories, so this is the story behind The Software Mentor and why I do what I do. </p>"},{"location":"story/#beginning-computer-engineer-or-math-teacher","title":"Beginning - Computer Engineer or Math Teacher?","text":"<p>As the story goes, I came home from kindergarten one day and told my mom that I wanted to be a computer engineer. I think it was because there was someone at school who talked about their job and what they did, but not sure. Whatever happened, it was enough for it to stick out to 5 year old Cameron and I decided that's what I wanted to be.</p> <p>As I got older, I was lucky that my parents were able to get a computer for the family. Unlike the normal trope, I didn't learn how to program at a young age. I mostly spent my time playing video games and working on my typing skills. Over time, I became the one responsible for fixing/repairing the computer when I invariably broke it. I credit my troubleshooting and teaching skills to explaining to my non-technical family, what caused the problem and why.</p> <p>Once I got to my senior year in high school, I had a big change on what I wanted to go to college for. Inspired by my teachers, I wanted to become a high school math teacher which means that Maryville College was where I needed to go.</p> <p>Maryville College is well known for producing top-notch educators and my favorite teachers came from Maryville. Plus, all the kids that I liked hanging out with were going to Maryville, so that made the most sense.</p> <p>At Maryville, I had to take some intro to computer science courses and absolutely fell in love with it. I found programming to be natural and was able to pick up the concepts quickly. So much so, that I was a lab assistant and tutor for the intro courses for my time at Maryville. I was intrigued about how to help students learn programming, I even did my undergraduate thesis on a new programming language that was centered around better error messages and cleaner syntax.</p>"},{"location":"story/#first-job","title":"First Job","text":"<p>After graduating, I got my first job in the healthcare space working on Electronic Medical Record (EMR) software. There was some culture shock (moving away from Tennessee, working with a team where English was a minority) and a ton of skilling up (learning new languages, source control, application development).</p> <p>I struggled mightily at this job. I remember working on one bug where I had to escape some data before it was transmitted and I couldn't figure out a way to solve the problem without breaking existing functionality. To make matters worse, we had long ship times, so if I shipped broken code, it would impact other teams for weeks until we could ship the next version.</p> <p>My one-on-ones with my manager were stressful and I didn't feel comfortable that I could do the job. To compensate for my poor performance, I started working 60-70 hour weeks, trying to get more stuff done and prove that I could hack the job.</p> <p>While putting in these hours, I started second guessing myself, asking \"Am I cut out to be software developer? Did I waste my time getting this degree\"?</p> <p>This was a dark time in my career and I didn't have a mentor or anyone I could talk to or get perspective.</p>"},{"location":"story/#the-turning-point","title":"The Turning Point","text":"<p>One of the perks of the job is that I got $50 to spend on whatever book(s) that I wanted. Looking through some recommendations, I saw a book called The Clean Coder by Robert C. Martin that had some decent reviews and decided to pick up a copy.</p> <p>This book fundamentally changed my view on the work I was doing, the company I was at, and expectations.</p> <p>Long story short, I wasn't the problem, this place was the problem. With leadership agreeing to commitments without informing the team, little to no mentoring, and the unwritten rule for people to be working 60 hour weeks, this place was not healthy for me.</p> <p>Though I don't agree with Robert Martin with his personal views, this book was the kick in the pants that I needed to change jobs and find a place where I could be successful.</p>"},{"location":"story/#finding-a-place-to-flourish","title":"Finding a Place To Flourish","text":"<p>Due to the antiquated technology this company used, I had little success of finding other companies that used that tech. This in turn, made it hard to find new opportunities. Looking at openings back in Tennessee, I found a post for a healthcare/investment company that was looking for a entry level software engineer.</p> <p>Even though I didn't know the technology stack at all, I hoped that my experience in healthcare would help me land the job.</p> <p>My interview was pleasant. For the technical portion, I was lucky that my interviewer focused on pairing/TDD and didn't have a problem with mentoring on the fly. Even though I had no experience with C#, we were able to TDD a simple problem (FizzBuzz, I believe) and made decent progress on a second problem.</p> <p>Even though the benefits were not great, those two years proved to be a transformative part of my career. I learned so much from my mentor on how to be a professional, how to think about software, how to reason about problems, how to troubleshoot and debug issues, what good code looks like, </p>"},{"location":"story/#pouring-back-into-the-community","title":"Pouring Back Into the Community","text":"<p>I was lucky to have found such an excellent mentor to help me level up and to build up my confidence. However, not everyone gets that opportunity. </p> <p>I want to be a mentor for those who are new to the industry or don't know if they have what it takes to be successful. The community has given me so much, that I want to pour back in, for the next generation.</p> <p>I've been blogging and presenting since 2015 and I've never made money doing either of those things. However, it's allowed me to share my ideas with those I wouldn't normally come across and hopefully helps pull them up.</p>"},{"location":"articles/2024/12/27/year-in-review-2024/","title":"Year in Review: 2024","text":"<p>As the year comes to a close, I like to take this time to reflect on the past year: what got done, how do I feel like things are progressing, and are there any changes that I should be working on for the next year.</p> <p>With that in mind, I generally break my goals into three sections: what progress did I make for my career? what progress did I make for the community, and finally, what progress did I make for myself?</p>"},{"location":"articles/2024/12/27/year-in-review-2024/#professional","title":"Professional","text":"<p>This year, I focused more on learning about consulting and what a successful engagement looks like. As such, this year was a good year as I've had experience working with both larger companies (Fortune 500) and smaller companies. All in all, I find consulting to be a good application of my skills and is a natural inclination for my process improvement skills.</p> <ul> <li>Completed engagement with a Fortune 500 client to teach TypeScript and AWS (members were coming from COBOL)</li> <li>Mentored two interns and helped get them hired full-time</li> <li>Designed a system for custom form creation for an education company (which led to follow-up development work)</li> <li>Started coaching engagement for an AgTech company focusing on delivery and process improvements</li> <li>Designed a system for database health checks for a Database Administrator as a Service company</li> </ul> <p>To help Lean TECHniques with their Microsoft partnership, I spent time over the summer working on obtaining various Microsoft certifications (I ran out of time before getting the Microsoft Certified: Azure Solutions Architect Expert certificate, but that's on my docket for 2025)</p> <ul> <li>Earned the Microsoft Certified: Azure Administrator Associate certification</li> <li>Earned the Microsoft Certified: Azure Developer Associate certification</li> <li>Earned the Microsoft Certified: DevOps Engineer Expert certification</li> </ul> <p>Last, but not least, I spent some time learning some (new to me tech) like GraphQL and basic Java.</p> <ul> <li>Earned the Graph Developer - Associate certification from Apollo GraphQL</li> <li>Was included as an acknowledgement in Pragmatic Unit Testing in Java with JUnit: 3rd Edition by Jeff Langr for my feedback and review before publishing.</li> </ul>"},{"location":"articles/2024/12/27/year-in-review-2024/#community","title":"Community","text":"<p>On the community side, 2024 was a year of firsts. For example, I hosted and organized Queen City Code Camp, a conference where attendees learned the fundamentals of Test Driven Development (TDD) by solving the Mars Rover kata. This was the first time since 2019 that I organized a conference (and absolutely the first time since I moved to North Carolina), so lots of good lessons learned here.</p> <p>In another first, I created my first training video, Navigating Mars Using Functional Programming in TypeScript, a Udemy course where I walk you through how to solve the Mars Rover kata using functional programming concepts.</p> <p>Lastly, I created two new talks this year: How to Make Your Own Automation Framework where I show you how to build your own framework using Deno, TypeScript, and GitHub Action; and How to Build More Resilient Teams where I show you how to use post-mortem and experiments to build high performing teams. In general, I shoot to create one new talk a year and I over-delivered on that goal this year (with 2025 already looking to be in good shape).</p> <p>Outside of new talks, I authored 23 different blogs posts with my most common topics being Leadership and Functional Programming and presented at 11 different events.</p>"},{"location":"articles/2024/12/27/year-in-review-2024/#personal","title":"Personal","text":"<ul> <li>Read 32 books in various genres (biographical, investing, leadership, history, consulting, process, science fiction, and general fiction)</li> <li>Learned the basics of knitting, made scarves and baby blankets using both seed and garter stitches</li> <li>Learned the basics of working with chisels and planes (including sharpening) for woodworking</li> <li> <p>Built a catio for our new cat, Trix. </p> </li> <li> <p>Learned how to make two different kinds of coffee drinks (Americano and Latte)</p> </li> <li>Built my first computer from components (major shout-out to Isaac for helping me figure out the components)</li> <li>Picked up over 30 bags of trash around the neighborhood</li> <li>Volunteered with Cub Scout Pack 58</li> </ul>"},{"location":"articles/2024/12/27/year-in-review-2024/#what-does-2025-have-in-store","title":"What Does 2025 Have in Store?","text":""},{"location":"articles/2024/12/27/year-in-review-2024/#obtain-three-client-engagements","title":"Obtain Three Client Engagements","text":"<p>I've spent plenty of time at step 1 (collect information), but it's now time to start putting that information into use (for better or for worse) and get my own clients. If I'm going to be successful, then I need to apply what I learn, make mistakes, mess up, learn, and iterate. As such, I'm going to expand my connections here in Charlotte, honing in on my ideal client. From there, I can expand and see what makes sense.</p>"},{"location":"articles/2024/12/27/year-in-review-2024/#author-two-training-courses","title":"Author Two Training Courses","text":"<p>I learned a lot about authoring a training course by going through the Udemy process. Though it has a lower barrier to entry to get started (and they handle a good amount of the marketing), the fact that the pricing can fluctuate so much is a bit bonkers. For example, I have the course listed at $99.99, with a coupon that can bring the price down to $49.99. However, Udemy can then cut the price down to $9.99. On one hand, it increases sales, however, the amount of money made (both for Udemy and myself) is not enough to to cover the cost.</p> <p>As such, I'll be spending time finding a new platform for 2025 where I can host my training such that the pricing is a bit more in my control. (Udemy, if you could allow course authors to set a floor on their pricing, I'd be so much happier with the service!)</p>"},{"location":"articles/2024/12/27/year-in-review-2024/#build-a-hope-chest","title":"Build a Hope Chest","text":"<p>I come from a long line of makers. My dad worked in construction and manufacturing, various uncles worked as welders, fabricators, and construction, and my grandfather was a carpenter (and moonshiner, but that's a tale for a different day). I've always been fascinated about how things work and how to make them better, hence why I started my journey into woodworking during the pandemic (idle time and I had the space for it).</p> <p>I spend my work hours building things that either won't be seen by the general public or I can't talk about the project at all, so it can be difficult to explain what I do. Plus, you can't really touch software or applications, you can use them.</p> <p>Enter woodworking. I can build something with my own two hands and give it to a friend, keep it for myself, or as a lesson for me going forward. I've gotten some experience building small boxes using power tools last year, but I haven't built anything of a certain size and definitely not with hand tools.</p> <p>My last major goal for 2025 is to build a hope chest as this is a solid piece of fine furniture and it can be something I can pass down to my children when it comes time.</p> <p>Building this is would be tougher than anything I've built up to this point, however, it should be in the realm of my knowledge. The challenge with this build will be using strictly hand tools to build it out. It'll take me longer and it probably won't be as nice as if I had used machines, but at the end of the day, I can say that I made it.</p>"},{"location":"articles/2023/10/23/scaling-effectiveness-with-docs---finding-stale-docs/","title":"Scaling Effectiveness with Docs - Finding Stale Docs","text":"<p>In a previous post, I argued that to help your team be effective, you need to have up-to-date docs, and to have this happen, you need some way of flagging stale documentation.</p> <p>In this series, I show you how you can automate this process by creating a seed script, a check script, and then automating the check script. In today's post, let's develop the check script.</p>"},{"location":"articles/2023/10/23/scaling-effectiveness-with-docs---finding-stale-docs/#breaking-down-the-check-script","title":"Breaking Down the Check Script","text":"<p>At a high level, our script will need to perform the following steps:</p> <ol> <li>Specify the location to search.</li> <li>Find all the markdown files in directory.</li> <li>Get the \"Last Reviewed\" line of text.</li> <li>Check if the date is more than 90 days in the past.</li> <li>If so, print the file to the screen.</li> </ol>"},{"location":"articles/2023/10/23/scaling-effectiveness-with-docs---finding-stale-docs/#specifying-location","title":"Specifying Location","text":"<p>Our script is going to search over our repository, however, I don't want our script to be responsible for cloning and cleaning up those files. Since the long term plan is for our script to run through GitHub Actions, we can have the pipeline be responsible for cloning the repo.</p> <p>This means that our script will have to be told where to search and since it can't take in manual input, we're going to use an environment variable to tell the script where to search.</p> <p>First, let's create a <code>.env</code> file that will store the path of the repository:</p> .env<pre><code>REPO_DIRECTORY=\"ABSOLUTE PATH GOES HERE\"\n</code></pre> <p>From there, we can start working on our script to have it use this environment variable.</p> index.ts<pre><code>import { load } from \"https://deno.land/std@0.195.0/dotenv/mod.ts\";\n\nawait load({ export: true }); // this loads the env file into our environment\n\nconst directory = Deno.env.get(\"REPO_DIRECTORY\");\n\nif (!directory) {\n  console.log(\"Couldn't retrieve the REPO_DIRECTORY value from environment.\");\n  Deno.exit();\n}\nconsole.log(directory);\n</code></pre> <p>If we were to run our Deno script with the following command <code>deno run --allow-read --allow-env ./index.ts</code>, we should see the environment variable getting logged.</p>"},{"location":"articles/2023/10/23/scaling-effectiveness-with-docs---finding-stale-docs/#finding-all-the-markdown-files","title":"Finding all the Markdown Files","text":"<p>Now that we have a directory, we need a way to get all the markdown files from that location.</p> <p>Doing some digging, I didn't find a built-in library for doing this, but building our own isn't too terrible.</p> <p>By using <code>Deno.readDir/Sync</code>, we can get all the entries in the specified directory.</p> <p>From here, we can then recurse into the other folders and get their markdown files as well.</p> <p>Let's create a new file, <code>utility.ts</code> and add a new function, <code>getMarkdownFilesFromDirectory</code></p> utility.ts<pre><code>export function getMarkdownFilesFromDirectory(directory: string): string[] {\n  // let's get all the files from the directory\n  const allEntries: Deno.DirEntry[] = Array.from(Deno.readDirSync(directory));\n\n  // Get all the markdown files in the current directory\n  const markdownFiles = allEntries.filter(\n    (x) =&gt; x.isFile &amp;&amp; x.name.endsWith(\".md\")\n  );\n  // Find all the folders in the directory\n  const folders = allEntries.filter(\n    (x) =&gt; x.isDirectory &amp;&amp; !x.name.startsWith(\".\")\n  );\n  // Recurse into each folder and get their markdown files\n  const subFiles = folders.flatMap((x) =&gt;\n    getMarkdownFilesFromDirectory(`${directory}/${x.name}`)\n  );\n  // Return the markdown files in the current directory and the markdown files in the children directories\n  return markdownFiles.map((x) =&gt; `${directory}/${x.name}`).concat(subFiles);\n}\n</code></pre> <p>With this function in place, we can update our <code>index.ts</code> script to be the following:</p> index.ts<pre><code>import { load } from \"https://deno.land/std@0.195.0/dotenv/mod.ts\";\nimport { getMarkdownFilesFromDirectory } from \"./utility.ts\";\n\nconst directory = Deno.env.get(\"REPO_DIRECTORY\");\n\nif (!directory) {\n  console.log(\"Couldn't retrieve the REPO_DIRECTORY value from environment.\");\n  Deno.exit();\n}\n\nconst files = getMarkdownFilesFromDirectory(directory);\nconsole.log(files);\n</code></pre> <p>Running the script with <code>deno run --allow-read --allow-env ./index.ts</code>, should get a list of all the markdown files being printed to the screen.</p>"},{"location":"articles/2023/10/23/scaling-effectiveness-with-docs---finding-stale-docs/#getting-the-last-reviewed-text","title":"Getting the Last Reviewed Text","text":"<p>Now that we have each file, we need a way to get their last line of text.</p> <p>Using Deno.readTextFile/Sync, we can get the file contents. From there, we can convert them to lines and then find the latest occurrence of Last Reviewed</p> <p>Let's add a new function, <code>getLastReviewedLine</code> to the <code>utility.ts</code> file.</p> utility.ts<pre><code>export function getLastReviewedLine(fullPath: string): string {\n  // Get the contents of the file, removing extra whitespace and blank lines\n  const fileContent = Deno.readTextFileSync(fullPath).trim();\n\n  // Convert block of text to a array of strings\n  const lines = fileContent.split(\"\\n\");\n\n  // Find the last line that starts with Last Reviewed On\n  const lastReviewed = lines.findLast((x) =&gt; x.startsWith(\"Last Reviewed On\"));\n\n  // If we found it, return the line, otherwise, return an empty string\n  return lastReviewed ?? \"\";\n}\n</code></pre> <p>Let's try this function out by modifying our <code>index.ts</code> file to display files that don't have a Last Reviewed On line.</p> index.ts<pre><code>import { load } from \"https://deno.land/std@0.195.0/dotenv/mod.ts\";\nimport {\n  getMarkdownFilesFromDirectory,\n  getLastReviewedLine,\n} from \"./utility.ts\";\n\nconst directory = Deno.env.get(\"REPO_DIRECTORY\");\n\nif (!directory) {\n  console.log(\"Couldn't retrieve the REPO_DIRECTORY value from environment.\");\n  Deno.exit();\n}\n\nconst files = getMarkdownFilesFromDirectory(directory);\nfiles\n  .filter((x) =&gt; getLastReviewedLine(x) !== \"\")\n  .forEach((s) =&gt; console.log(s)); // print them to the screen\n</code></pre>"},{"location":"articles/2023/10/23/scaling-effectiveness-with-docs---finding-stale-docs/#determining-if-a-page-is-stale","title":"Determining If A Page Is Stale","text":"<p>At this point, we can get the \"Last Reviewed On\" line from a file, but we've got some more business rules to implement.</p> <ul> <li>If there's a Last Reviewed On line, but there's no date, then the files needs to be reviewed</li> <li>If there's a Last Reviewed On line, but the date is invalid, then the file needs to be reviewed</li> <li>If there's a Last Reviewed On line, and the date is more than 90 days old, then the file needs to be reviewed.</li> <li>Otherwise, the file doesn't need review.</li> </ul> <p>We know from our filter logic that we're only going to be looking at lines that start with \"Last Reviewed On\", so now we need to extract the date.</p> <p>Since we assume our format is Last Reviewed On, we can use substring to get the rest of the line. We're also going to assume that the date will be in YYYY/MM/DD format.</p> utility.ts<pre><code>export function doesFileNeedReview(line: string): boolean {\n  if (!line.startsWith(\"Last Reviewed On: \")) {\n    return true;\n  }\n  const date = line.replace(\"Last Reviewed On: \", \"\").trim();\n  const parsedDate = new Date(Date.parse(date));\n  if (!parsedDate) {\n    return true;\n  }\n\n  // We could something like DayJS, but trying to keep libraries to a minimum, we can do the following\n  const cutOffDate = new Date(new Date().setDate(new Date().getDate() - 90));\n\n  return parsedDate &lt; cutOffDate;\n}\n</code></pre> <p>Let's update our <code>index.ts</code> file to use the new function.</p> index.ts<pre><code>import { load } from \"https://deno.land/std@0.195.0/dotenv/mod.ts\";\nimport {\n  getMarkdownFilesFromDirectory,\n  getLastReviewedLine,\n} from \"./utility.ts\";\n\nconst directory = Deno.env.get(\"REPO_DIRECTORY\");\n\nif (!directory) {\n  console.log(\"Couldn't retrieve the REPO_DIRECTORY value from environment.\");\n  Deno.exit();\n}\n\ngetMarkdownFilesFromDirectory(directory)\n  .filter((x) =&gt; getLastReviewedLine(x) !== \"\")\n  .filter((x) =&gt; doesFileNeedReview(x))\n  .forEach((s) =&gt; console.log(s)); // print them to the screen\n</code></pre> <p>And just like that, we're able to print stale docs to the screen. At this point, you could create a scheduled batch job and start using this script.</p> <p>However, if you wanted to share this with others (and have this run not on your box), then stay tuned for the final post in this series where we put this into a GitHub Action and post a message to Slack!</p>"},{"location":"articles/2023/10/23/scaling-effectiveness-with-docs---finding-stale-docs/#other-posts-in-the-series","title":"Other Posts In The Series","text":"<ul> <li>Having Coffee with Deno - Inspiration</li> <li>Having Coffee with Deno - Dynamic Names</li> <li>Having Coffee with Deno - Sharing the News</li> <li>Having Coffee with Deno - Automating All the Things</li> </ul>"},{"location":"articles/2023/08/08/scaling-effectiveness-with-docs---seeding-dates/","title":"Scaling Effectiveness with Docs - Seeding Dates","text":"<p>In a previous article, I argued that to help your team be effective, you need to have up-to-date docs, and to have this happen, you need some way of flagging stale documentation.</p> <p>This process lends itself to being easily automated, so in this series of posts, we'll build out the necessary scripts to check for docs that haven't been reviewed in the last 90 days.</p> <p>All code used in this post can be found on my GitHub.</p>"},{"location":"articles/2023/08/08/scaling-effectiveness-with-docs---seeding-dates/#approach","title":"Approach","text":"<p>To make this happen, we'll need to create the following:</p> <ol> <li>A seed script that will add a Last Reviewed Date to all of our pages.</li> <li>A check script that will check files for the Last Reviewed Date, returning which ones are either missing a date or are older than 90 days.</li> <li>Create a scheduled job using GitHub Actions to run our check script and post a message to our Slack channel.</li> </ol> <p>For this post, we'll be creating the seed script.</p>"},{"location":"articles/2023/08/08/scaling-effectiveness-with-docs---seeding-dates/#breaking-down-the-seed-script","title":"Breaking Down the Seed Script","text":"<p>For this script to work, we need to be able to do two things:</p> <ol> <li>Determine the last commit date for a file.</li> <li>Add text to the end of the file.</li> <li>Getting a list of files in a directory.</li> </ol> <p>To determine the last commit date for a file, we can leverage <code>git</code> and its <code>log</code> command (more on this in a moment). Since we're mainly doing file manipulation, we could use Deno here, but it makes much more sense to me to use something like <code>bash</code> or <code>PowerShell</code>.</p>"},{"location":"articles/2023/08/08/scaling-effectiveness-with-docs---seeding-dates/#determining-the-last-commit-date-for-a-file","title":"Determining the Last Commit Date For a File","text":"<p>To make this automation work, we need to have a date for the Last Reviewed On footer. You don't want to set all the files to the same date because all the files will come up for review in one big batch.</p> <p>So, you're going to want to stagger the dates. You can do this by generating random dates, but honestly, getting the last commit date should be \"good\" enough.</p> <p>To do this, we can take advantage of git's <code>log</code> command with the <code>--pretty</code> flag.</p> <p>We can test this out by using the following script.</p> <pre><code>file=YourFileHere.md\ncommitDate=$(git log -n 1 --pretty=format:%aI -- $file)\n# formatting date to YYYY/MM/DD\nformattedDate=$(date -d \"$commitDate\" \"+%Y/%m/%d\")\necho $formattedDate\n</code></pre> <p>Assuming the file has been checked into Git, we should get the date back in a YYYY/MM/DD format. Success!</p>"},{"location":"articles/2023/08/08/scaling-effectiveness-with-docs---seeding-dates/#adding-text-to-end-of-file","title":"Adding Text to End of File","text":"<p>Now that we have a way to get the date, we need to add some text to the end of the file. Since we're working in markdown, we can use <code>---</code> to denote a footer and then place our text.</p> <p>Since we're going to be appending multiple lines of text, we can use the cat command with here-docs.</p> <pre><code>file=YourFileHere.md\n# Note the blank lines, this is to make sure that the footer is separated from the text in the file\n# Note: The closing EOF has to be on its own line with no whitespace or other characters in front of it.\ncat &lt;&lt; EOF &gt;&gt; $file\n\n\n---\nLast Reviewed On: 2023/08/12\nEOF\n</code></pre> <p>After running this script, we'll see that the file has appended blank lines and our new footer.</p>"},{"location":"articles/2023/08/08/scaling-effectiveness-with-docs---seeding-dates/#combining-into-a-new-script","title":"Combining Into a New Script","text":"<p>Now that we have both of these steps figured out, we can combine them into a single script like the following:</p> <pre><code>file=YourFileHere.md\ncommitDate=$(git log -n 1 --pretty=format:%aI -- $file)\n# formatting date to YYYY/MM/DD\nformattedDate=$(date -d \"$commitDate\" \"+%Y/%m/%d\")\n# Note the blank lines, this is to make sure that the footer is separated from the text in the file\n# Note: The closing EOF has to be on its own line with no whitespace or other characters in front of it.\ncat &lt;&lt; EOF &gt;&gt; $file\n\n\n---\nLast Reviewed On: $formattedDate\nEOF\n</code></pre> <p>Nice! Given a file, we can figure out its last commit date and append it to the file. Let's make this more powerful by not having to hardcode a file name.</p>"},{"location":"articles/2023/08/08/scaling-effectiveness-with-docs---seeding-dates/#finding-files-in-a-directory","title":"Finding Files In a Directory","text":"<p>At this point, we can update a file, but the file is hardcoded. But we're going to have a lot of docs to review, and we don't want to do this manually, so let's figure out how we can get all the markdown files in a directory.</p> <p>For this exercise, we can use the <code>find</code> command. In our case, we need to find all the files with a <code>.md</code> extension, no matter what directory they're in.</p> <pre><code>directory=DirectoryPathGoesHere\nfind $directory -name \"*.md\" -type f\n</code></pre> <p>We're going to need to process each of these files, so some type of iteration would be helpful. Doing some digging, Bash supports a for loop, so let's use that.</p> <pre><code>directory=DirectoryPathGoesHere\nfor file in `find $directory -name \"*.md\" -type f`; do\n  echo \"printing $file\"\ndone\n</code></pre> <p>If everything works, we should see each markdown file name being printed.</p>"},{"location":"articles/2023/08/08/scaling-effectiveness-with-docs---seeding-dates/#when-a-plan-comes-together","title":"When a Plan Comes Together","text":"<p>We've got all the pieces, so let's bring this together:</p> <pre><code>directory=DirectoryPathGoesHere\nfor file in `find $directory -name \"*.md\" -type f`; do\n  commitDate=$(git log -n 1 --pretty=format:%aI -- $file)\n  # formatting date to YYYY/MM/DD\n  formattedDate=$(date -d \"$commitDate\" \"+%Y/%m/%d\")\n  # Note the blank lines, this is to make sure that the footer is separated from the text in the file\n  # Note: The closing EOF has to be on its own line with no whitespace or other characters in front of it.\n  cat &lt;&lt; EOF &gt;&gt; $file\n\n\n---\nLast Reviewed On: $formattedDate\nEOF\ndone\n</code></pre>"},{"location":"articles/2023/08/08/scaling-effectiveness-with-docs---seeding-dates/#bells-and-whistles","title":"Bells and Whistles","text":"<p>This script works and we could ship this, however, it's a bit rough.</p> <p>For example, the script assumes that it's in the same directory as your git repository. It also assumes that your repository is up-to-date and that it's safe to make changes on the current branch.</p> <p>Let's make our script a bit more durable by making the following tweaks:</p> <ol> <li>Clone the repo to a new temp directory.</li> <li>Create a new branch for making changes.</li> <li>Commit changes and publish the branch.</li> </ol>"},{"location":"articles/2023/08/08/scaling-effectiveness-with-docs---seeding-dates/#getting-the-latest-version-of-the-repo","title":"Getting the latest version of the repo","text":"<p>For this step, let's add logic for creating a new temp directory and adding a call to <code>git clone</code>.</p> <pre><code># see https://unix.stackexchange.com/questions/30091/fix-or-alternative-for-mktemp-in-os-x#answer-84980\n# for why tmpDir is being created this way\ndocRepo=\"RepoUrlGoesHere\"\ntmpDir=$(mktemp -d 2&gt;/dev/null || mktemp -d -t 'docSeed')\ncd $tmpDir\necho \"Cloning from $docRepo\"\n# Note the . here, this allows us to clone to the temp folder and not to a new folder of repo name\ngit clone \"$docRepo\" . &amp;&gt; /dev/null\n</code></pre>"},{"location":"articles/2023/08/08/scaling-effectiveness-with-docs---seeding-dates/#making-a-new-branch-and-pushing-changes","title":"Making a new branch and pushing changes","text":"<p>Now that we've got the repo, we can add the steps for switching branches, committing changes, and publishing the branch.</p> <pre><code># ... code to clone repository\ngit switch -c 'adding-seed-dates'\n# ... code to make file changes\ngit add --all\ngit commit -m \"Adding seed dates\"\ngit push -u origin adding-seed-dates\n</code></pre>"},{"location":"articles/2023/08/08/scaling-effectiveness-with-docs---seeding-dates/#final-script","title":"Final Script","text":"<p>Let's take a look at our final script:</p> <pre><code>#!/bin/bash\ndocRepo=\"RepoUrlGoesHere\"\ntmpDir=$(mktemp -d 2&gt;/dev/null || mktemp -d -t 'docSeed')\ncd $tmpDir\n\necho \"Cloning from $docRepo\"\ngit clone \"$docRepo\" . &amp;&gt; /dev/null\n\ngit switch -c 'adding-dates-to-files'\n\nfor file in `(find . -name \"*.md\" -type f)`; do\n  echo \"updating $file\"\n  commitDate=\"$(git log -n 1 --pretty=format:%aI -- $file)\"\n  formattedDate=$(date -d $commitDate \"+%Y/%m/%d\")\n  cat &lt;&lt; EOT &gt;&gt; $file\n\n\n---\nLast Reviewed On: $formattedDate\nEOT\ndone\ngit add --all\ngit commit -m \"Adding initial dates\"\ngit push -u origin adding-dates-to-files\n</code></pre>"},{"location":"articles/2023/08/08/scaling-effectiveness-with-docs---seeding-dates/#wrapping-up","title":"Wrapping Up","text":"<p>In this post, we wrote a bash script to clone our docs and add a new footer to every page with the file's last commit date. In the next post, we'll build the script that checks for stale files.</p>"},{"location":"articles/2023/05/21/five-tips-to-improve-your-coaching-conversations/","title":"Five Tips to Improve Your Coaching Conversations","text":"<p>As a leader, you're responsible for coaching and growing your team, helping them be successful. To do this, you need to set the tone and example of the behaviors you want the team to have.</p> <p>No matter how good of a team you have or how good of a leader you are, you will have to have a conversation about performance. Whether it's delivery, professional skills, or technology skills, you will have a moment where you need someone to change their behaviors.</p> <p>Having these types of conversations can be scary, no matter how much experience in leadership you might have. However, when done correctly, these moments can greatly impact the other person, helping them grow tremendously.</p> <p>On the other hand, poor coaching will do the absolute opposite. The other person can become confused or angry. They could even shut down and disengage altogether, making coaching them that much harder.</p> <p>So what does good coaching look like? I can't guarantee that these steps will solve all your woes; however, I do guarantee that following these tips will increase the odds of the other person listening and at least consider your feedback. Remember, you want to do this coaching because some behavior has caught your attention, and you want to correct it. If the other person doesn't listen or want to engage, then you literally can't make this happen.</p>"},{"location":"articles/2023/05/21/five-tips-to-improve-your-coaching-conversations/#step-1-be-timely","title":"Step 1: Be Timely","text":"<p>The sooner you can have this conversation, the more effective it will be. Remember, the point of feedback is to let the other person know how they're doing and correct if needed. They can't do this if the behavior happened three weeks ago because there's no correlation at that point.</p> <p>Imagine if you had a test suite that only told you about failing tests a week after the build started. There's no way you could make the right decisions, so why would we think that's the case for behavior?</p> <p>If I've noticed a pattern and feel that it's time to coach, I will get that feedback to them that week, if not the next day.</p>"},{"location":"articles/2023/05/21/five-tips-to-improve-your-coaching-conversations/#step-2-be-specific","title":"Step 2: Be Specific","text":"<p>When giving this feedback, the behavior may be obvious to you but not even a thought for the other person. Because we can't control what the other person is thinking, we need to set the context for the feedback so they know what you're talking about.</p> <p>Let's take my son, for example. He's particular about his food, so when he says that \"dinner was awesome,\" that makes me feel great as I'm happy he enjoyed dinner. But I have no clue what he actually liked or why he thought that. Was it the food? The way it was served? The fact that we had a picnic? No idea, so I'd respond with, \"What made it stand out to you?\". When he mentions that he liked the pizza, I go \"Ah! He enjoyed the food, nice!\"</p> <p>Providing this specific context is crucial for the other person because it lets them kow what caught your attention and drastically reduces the confusion in the conversation. For those who like more concrete details, sharing links to chats, emails, or other artifacts with the behavior can be helpful because you can use it as the foundation for the conversation.</p>"},{"location":"articles/2023/05/21/five-tips-to-improve-your-coaching-conversations/#step-3-explain-why","title":"Step 3: Explain Why","text":"<p>There's a reason that you're having this conversation. There's something that's important to you, and, in your opinion, it wasn't important to the other person. We've got to explain why it's important and why you're commenting on it.</p> <p>I never want to remove someone's autonomy as I like to set the direction and let the team blaze a path, with me guiding to make sure we don't get lost in the wilderness. However, for someone to have autonomy, they need to understand the goals and the reasoning behind it.</p> <p>If they don't have this knowledge, then it's that much harder for them to make the right decisions. Ensuring they know the why is a leader's responsibility.</p>"},{"location":"articles/2023/05/21/five-tips-to-improve-your-coaching-conversations/#step-4-seek-to-understand","title":"Step 4: Seek To Understand","text":"<p>You're working with a team of professionals. A professional makes the right decisions based on their knowledge and experience. If the person is making mistakes, we need to understand why they made the choice that they did.</p> <p>For example, let's say I'm coaching someone who's consistently missing meetings. I'm frustrated that they're unresponsive and that they don't care. The issue here is that it's okay for me to feel frustrated, but I can't make the judgment that they don't care. I don't know that, and it causes more problems than it solves. I won't vent my feelings to the other person because even though it'd make me feel better, it doesn't help the situation.</p> <p>A better approach would be to understand why they're missing meetings. Is it something outside of work? Could it be that they don't know why they need to attend? What if they didn't receive an invite? In any of the above cases, there was a solid reason why they didn't attend, and I wouldn't have known that if I had not opened the conversation.</p> <p>Don't assume malice or apathy when something happens. We are humans first, which means we're going to make mistakes.</p>"},{"location":"articles/2023/05/21/five-tips-to-improve-your-coaching-conversations/#step-5-working-together","title":"Step 5: Working Together","text":"<p>The entire point of coaching is to help the person improve, and we also don't want to take away their autonomy. To make this happen, we need to work with the other person to come up with ideas that can help improve the situation. It's not any one person's responsibility, but it's your responsibility to brainstorm with them and help guide them down the correct path.</p> <p>The key here is to have an open mind and really consider all ideas. One of my favorite leadership books, First, Break All The Rules, talks about how great leaders work with their people to have their strengths shine and to make their weaknesses a non-issue.</p> <p>In the missing meeting example, I found out that the issue was that they didn't know why they needed to attend the meeting, so they didn't attend, in order to focus on their development work. Working together, I changed invites to include the reason for attending and encouraged them to chat with me so we could figure it out if they didn't know why they needed to be there.</p>"},{"location":"articles/2023/05/21/five-tips-to-improve-your-coaching-conversations/#case-study-bringing-it-together","title":"Case Study - Bringing It Together","text":"<p>In this example, let's explore where we would need to do some coaching.</p> <p>While reviewing a pull request from Bruce, you see a comment from Alvin, a member of your team, where they were particularly critical of the work. Reading through the pull request, you see Alvin has left more harsh comments about Bruce's work.  </p> <p>Talking with Bruce, they mention that they don't work well with Alvin as it seems like he's always critical of Bruce.</p> <p>Based on this scenario, we know that Alvin has left some harsh words for Bruce, which makes them less likely to work together. If Alvin keeps this behavior up with other people, this will impact others wanting to work with him, reducing his effectiveness.</p> <p>After collecting your thoughts, you reach out to Alvin to see if he's got a few minutes to chat about Bruce's pull request.</p> <p>\"Hey Alvin, I noticed you left some pretty harsh comments that in Bruce's pull request. For example, saying that 'this code is convoluted, rewrite it'. Even if that was the case, it's not clear why you think that. I'm more concerned with how the messaging came across because we work with others to accomplish our tasks, and that communication style can make people not want to work with us.</p> <p>I don't believe you intend to alienate others, so can you walk me through your thought process here and why you thought this was the right approach?\"</p> <p>In this example, we've already hit four out of the five tips. Our feedback was timely and specific to the problem. We included why it caught our attention and started with an open-ended question for the conversation about the behavior.</p> <p>In the follow-up conversation, Alvin mentions that he was having a rough day, particularly outside of work, and that he wasn't entirely focused on his tone. Given that this is the first time Alvin has done this, we want to focus on fixing the issue before it becomes a pattern.</p> <p>\"I understand that it can be hard to focus on your tone when you're having a rough time, however, we can't speak to others this way. I don't want this to become a pattern, so what are some things that we could do instead when we're not in the right mental head space for code reviews?\"</p> <p>At this point, we've acknowledged what was said and reaffirmed expectations. Using another open-ended question, we can start brainstorming things that we could do to help improve Alvin's tone. Since we're opening the conversation, Alvin is also giving feedback on what might work for him and what wouldn't work.</p>"},{"location":"articles/2023/05/21/five-tips-to-improve-your-coaching-conversations/#wrapping-up","title":"Wrapping Up","text":"<p>Giving critical feedback to someone is not the easiest thing to do, however, it can have the most impact for them. To help frame the conversation, our coaching should:</p> <ul> <li>Be Timely </li> <li>Be Specific</li> <li>Explain the Why</li> <li>Seeking to Understand</li> <li>Be Collaborative</li> </ul>"},{"location":"articles/2023/05/14/building-relationships-through-one-on-ones/","title":"Building Relationships Through One-on-Ones","text":"<p>As a leader, one of your goals is to build a strong, high-performing team. To do this, you'll need to establish and develop relationships within the team and with each other. One approach I've found helpful to build and sustain these relations is through one-on-ones. </p> <p>When most people think of one-on-ones, they typically think of some scheduled time, every so often, where they talk about work concerns or whatever is on the leader's mind. Some might find one-on-ones a waste of time and skip them.</p> <p>Let's face it, we've all had bad one-on-ones where the conversation was forced and stiff. Or, it felt like the other person wasn't listening or cared about what was being talked about. If enough one-on-ones go down this route, it's no surprise that people don't want to have these conversations.</p> <p>So, how do we improve the situation? In this post, I'm going to show you three things you can do to improve the one-on-ones you're having with the team by making sure that they're being heard, relationships are being built, and, just maybe, you might even get to know them better.</p>"},{"location":"articles/2023/05/14/building-relationships-through-one-on-ones/#its-not-a-status-update","title":"It's Not a Status Update","text":"<p>A common mistake is treating one-on-ones as status updates for work or projects. Even though it might be tempting to get an update (especially on important projects), remember this time is for the other person to talk with you about what's on their mind. They can't do this if you're asking for updates on work. If you're leveraging stand-ups or a task board, then you should be able to get updates from there. If this isn't sufficient, ask for updates outside this conversation. The one-on-one is where you give the other person your full attention.</p> <p>Despite your best intent, this can be a tough habit to break. To help get out of this mindset, start time-boxing the updates to be a set period of time (e.g., ten minutes) with the goal of making this period shorter in subsequent one-on-ones.</p> <p>Another technique I use is resetting, where I remind my teammate of the purpose and goal of the conversation. Doing this, it's a gentle nudge in the correct direction and reinforces that this time is for them, not for whatever is on my mind.</p>"},{"location":"articles/2023/05/14/building-relationships-through-one-on-ones/#dont-get-distracted","title":"Don't Get Distracted","text":"<p>In the world of working remotely, it's easy to get distracted by chat notifications or emails. You're on the call, and you see the notification at the bottom of your screen, and before you realize it, you've read it and already thinking about a response. This may be great for you getting things done; however, for the other person, it's clear that you stopped paying attention. So what's important? Is it the notification or the other person?</p> Credit to Matt Quinn via Unsplash <p>To help reinforce the focus, I put myself in Do Not Disturb mode, which will hide notifications from me until the meeting ends. If the issue is truly important, then someone would give me a call, in which case, I can excuse myself from the one-on-one to see what's up. In this rare case, I will reschedule our conversation as it's essential that we meet. </p>"},{"location":"articles/2023/05/14/building-relationships-through-one-on-ones/#allow-them-to-set-the-agenda","title":"Allow Them to Set the Agenda","text":"<p>Another mistake I see leaders make is that they'll come to the one-on-one with a list of topics they want to discuss for the day. This can be particularly true if you're coaching this person and you want to provide concrete feedback. However, remember, the goal is to build and strengthen the relationship, and you can't do that if you're always setting the agenda for the conversations. </p> <p>I find it amazing that you can learn quite a bit about the other person based on what they bring up. For example, if they're speaking about a conflict with another person, that lets me know that they're aware of relationships and how they're being perceived. On the other hand, if they're talking about a project and the concerns they have, then they're thinking outside of a task and are thinking at a higher level.</p> <p>In one case, I was in a one-on-one with an engineer where they brought up their concern about supporting an API as they didn't know much about it. My first impression was that they didn't know how to read the code, but digging in, it turned out that they didn't see how the API fit into the bigger picture of the system. Learning this was a great fact check because I thought it was a technical issue, but in reality, it was a system question, which changed my perspective on them.</p>"},{"location":"articles/2023/05/14/building-relationships-through-one-on-ones/#bonus-seeding-a-conversation","title":"(Bonus) - Seeding a Conversation","text":"<p>I mentioned that the other person should own the agenda, but sometimes, they may not have much on their mind or a topic that sticks out to them. For these cases, I come prepared with a list of questions to help start the conversation.</p> Credit to Markus Spiske via Unsplash <p>In Warren Berger's The Book of Beautiful Questions, he talks about the power of open-ended questions and how they can help people be more open. So instead of asking, \"How's the project coming along?\", which can be answered in a binary fashion, we could instead ask, \"What's one thing about the project that stands out to you?\", which could give us a richer response and allow you to learn more.</p> <p>For those looking for questions, here's an excerpt of questions I've used to help seed a conversation:</p> <ul> <li>What's one thing about the current project that turned out to be harder than you thought?</li> <li>What was your biggest win last week, and why does it stand out?</li> <li>What's something that happened in the past week that you want to learn more about?</li> <li>I know you've been working more with  this past week, talk to me about that experience <li>What's something that you're looking forward to?</li>"},{"location":"articles/2023/05/14/building-relationships-through-one-on-ones/#wrapping-up","title":"Wrapping Up","text":"<p>To build great teams, you will need to foster relationships with the team and the individual members. Having one-on-ones can help with build these relationships, but only if you treat them like so. By allowing your teammate to set the agenda, giving them your full attention, and cutting out project updates, you can improve the quality of your conversations and get to know them better.</p>"},{"location":"articles/2023/05/07/building-relationships-with-intent/","title":"Building Relationships with Intent","text":"<p>As a leader, one of your superpowers is how you can connect your team with someone else. For example, if your team is struggling to work with an API and you know someone who's made recent changes or is a Subject Matter Expert, you can get your team unblocked and moving faster.</p> <p>I've worked with leaders like this, and it's amazing how fast and helpful it is to get unstuck quickly. Don't get me wrong, sometimes it's the right strategy to \"burn the time\" to learn, but it's helpful to know someone who can get you unstuck if you need it.</p> <p>With one leader, it seemed like they always knew a guy, no matter the topic, and I was amazed at how they did it. So like a lifelong learner, I asked, and they told me networking.</p>  Credit to ProductSchool via Unsplash <p>Ugh</p> <p>If you're like me, you hear networking and think about dozens (if not hundreds) of people milling around, introducing themselves, and sharing business cards. Don't get me wrong, that approach can work for some people, but to me, that sounds exhausting. Instead of a hummingbird, going from flower to flower, I'm more like a bear. I just want to sit down and eat my jar of honey.</p> <p>So what am I supposed to do? How am I supposed to network if I don't like large groups?</p> <p>Given time, you can work on becoming more comfortable in large groups. However, why fight your natural tendencies and what you're good at?</p> <p>For me, it's small groups and one-on-one conversations. As such, that's my approach to networking. Though it takes a bit longer, I find that I build stronger relationships with those people, and in turn, can be just as successful. I like to think about one-on-ones as lazy river conversations as I never know where it will take us.</p> Credit to Kiara Kulikova via Unsplash"},{"location":"articles/2023/05/07/building-relationships-with-intent/#one-cup-of-coffee","title":"One Cup of Coffee","text":"<p>At a previous company, we used Slack and, as such, had an integration called Random Coffee that would pair the members of a channel up to get together for the week. Such a simple idea, but so powerful when you now have a built-in excuse to chat with someone.</p>  Credit to Priscilla Du Preez via Unsplash <p>After a couple of weeks of getting to know people, I started learning what others did, what interests them, and who to go to about specific issues. Combine that with asking, \"How did you know that?\" I found that I could quickly fill gaps in my knowledge.</p> <p>But something else happened. Once I knew the person, I didn't see them as a name in the chat anymore, I saw them as their selves. I'd find myself saying, \"Oh, it's Chris, and Chris is cool, so I'll help him out,\" instead of thinking, \"Ugh, another thing to do.\" In a way, these conversations humanized those I worked with, and I found myself caring more about them.</p>"},{"location":"articles/2023/05/07/building-relationships-with-intent/#caring-by-knowing","title":"Caring By Knowing","text":"<p>To me, this is the most important thing about relationship building and networking. Deep down, I want to care about those I work with because I want them to be successful. I can't help them be successful if I don't know them both as a colleague and as a person.</p> <p>Having one-on-ones is how I know my people and how I continue building care. It might be as simple as knowing what types of things they like to work on or what they did over the weekend. However, having these conversations helps both of us open up, and I get to know them so much better. Once I know them, I can guide and direct them better, looking for opportunities I wouldn't have thought of before.</p>"},{"location":"articles/2023/05/07/building-relationships-with-intent/#how-do-i-start","title":"How Do I Start?","text":"<p>If you want to start this for your own company, you don't have to have Slack to make this happen. The important thing is getting buy-in from others and explaining the why behind the exercise.</p> <p>Once you have buy-in, you can start low-tech by using an Excel sheet and randomizing the list of names. This isn't the most robust solution, but it's a start and you can iterate as you figure out the timing, the frequency, and the other steps that setting this up would look like.</p> <p>Once you've got something in motion, you can always work on automating the process later. Don't let a perfect solution stop you from starting with a good solution.</p> <p>What I've found successful is having either a weekly or fortnightly scheduled message in our main channel that assigns the groups. From there, participants are encouraged to share something they've learned about their counterparts during their conversation. To help make sure that people meet, scheduling a set time during the week for all the groups can be helpful as it removes another barrier (e.g., if you know that you'll have coffee at 10:30 am on Tuesdays, you learn to expect it).</p> <p>If you have a group that is just starting, it might be helpful to provide some starting questions to help jump-start the conversation. A good list of questions can be found in one of my gists.</p>"},{"location":"articles/2023/05/07/building-relationships-with-intent/#wrapping-up","title":"Wrapping Up","text":"<p>To be a successful leader, you must cultivate and grow relationships with those you work with. Not only does it help your team be successful, but it allows you to have a richer experience with your work and helps solidify that we're all working together.</p>"},{"location":"articles/2023/06/08/camerons-coaching-corner---mentoring-an-intern/","title":"Cameron's Coaching Corner - Mentoring an Intern","text":"<p>Welcome to Cameron's Coaching Corner, where we answer questions from readers about leadership, career, and software engineering.</p> <p>In this week's post, we look at how <code>test123</code> can improve the mentoring experience for their new intern.</p> <p>I recently had an intern join my time and I\u2019m going to be his mentor. I\u2019ve had interns in the past, but this one doesn\u2019t understand any fundamentals and struggles with everything.</p> <p>My question to you is this, how can I help him? He doesn\u2019t know HTML/CSS/JS, so I\u2019m trying to teach him those, but it\u2019s taking away a lot of time. I suggested for him to watch some videos and then we can sync twice a day to go over the topics and discuss them further.</p> <p>My issue: I don\u2019t want to just say \u201cgo watch videos.\u201d Bc, that\u2019s not the best way to learn - I want him to dive into the code and try things and break, that\u2019s how I learned at least.</p> <p>How do you think I should handle this? I wanna be a good mentor and I want him to learn and grow. I don\u2019t wanna fail the kid bc I don\u2019t know the proper way to mentor.</p> <p>First, thank you for caring about your intern and wanting to do right by them. It might not seem like it right now, but you're playing a critical role in helping them succeed.</p> <p>I think you're wise not to put them in front of videos for learning everything and using that as coaching, a la Matrix style.</p> <p></p> I know Kung Fu Web Development <p>The main issue with this approach is that your intern isn't just leveling up on knowledge, but they're learning a new skill, and this can't be picked up by just watching videos. They need to do the work through practice, receiving feedback, and iterating.</p> <p>For example, I started woodworking a few years back and watched tons of videos on using certain tools and techniques. But until I got into the shop and started building, all I had was theory, no application. What leveled up my skills was putting the time in and building shop projects and critiquing what I did well and not so well.</p> <p>Learning software development is similar to learning woodworking because there are a lot of tools and techniques out there, but we can't learn it all at once, we need to simplify. In my case, I learned how to build a box. It's the foundation for most woodworking, and the skills to make one apply to almost any other project.</p> <p>Let's move from woodworking back to software development. I think teaching someone HTML, CSS, and JavaScript all at once is going to lead to them feeling overwhelmed, especially since CSS and JavaScript have a ton of tooling and options available. So, let's narrow the scope.</p>"},{"location":"articles/2023/06/08/camerons-coaching-corner---mentoring-an-intern/#what-to-focus-on","title":"What to Focus On?","text":"<p>For me, an internship is successful if they have a solid grasp of the fundamentals of engineering and they shipped meaningful work to production (ideally multiple times). In addition, I want to ensure their knowledge is transferrable (e.g., teaching them some internal library is cool, but since other companies aren't using it, their knowledge gained here is minimal). With that in mind, here's the approach I'd take.</p>"},{"location":"articles/2023/06/08/camerons-coaching-corner---mentoring-an-intern/#source-control","title":"Source Control","text":"<p>Without having a basic understanding of source control, it's hard to establish a foundation to build other skills on. For example, how would they undo their changes, share their code easily with others, or review others' work? We don't need them to become a Git guru. However, they need to be able to do the following.</p> <ul> <li>Clone</li> <li>Create branch</li> <li>Commit changes</li> <li>Push changes</li> <li>Create a pull request</li> <li>Merge a pull request</li> <li>Pull changes</li> </ul> <p>If they can complete the above steps, they know enough to complete development work. The cool thing here is that you don't have to spend much time on this, as the projects they'll be working on will build this habit into them.</p> <p>I don't care how they do this (whether through the command line or a GUI); the important thing is that they know how to do it. I have a cheat sheet I give them so that if they're using the command line, they can get going quickly.</p>"},{"location":"articles/2023/06/08/camerons-coaching-corner---mentoring-an-intern/#pick-a-technology","title":"Pick a Technology","text":"<p>Instead of teaching them everything at once, let's have them focus on one piece of technology. But which one do we start with? It depends on what work you're currently doing. Remember, we want this person to ship something to production, so if your work is mostly JavaScript, then it might not make sense to teach this person HTML immediately. On the other hand, if you're doing more design work, then focusing on HTML and then CSS would make more sense.</p>"},{"location":"articles/2023/06/08/camerons-coaching-corner---mentoring-an-intern/#how-to-begin","title":"How to Begin","text":"<p>When it comes to mentoring, I've found that having a mix of pairing and then independent practice to be effective for skill development. The pairing helps set the tone and provides the tools they need, whereas the independent time allows them to practice and get feedback from you, helping them iterate on the process.</p>"},{"location":"articles/2023/06/08/camerons-coaching-corner---mentoring-an-intern/#creating-a-repository","title":"Creating a Repository","text":"<p>If you can have them learn/experiment in your main repository, then awesome, feel free to skip this step.</p> <p>If not, I would spend the first session helping them get their first repository stood up and walk them through switching branches, adding a file (like a README), creating a pull request, and merging that in.</p>"},{"location":"articles/2023/06/08/camerons-coaching-corner---mentoring-an-intern/#pair-on-something-small","title":"Pair On Something Small","text":"<p>To start, I would make sure that we created a basic scaffolding so that they could run their project and see their changes. For example, if I were teaching HTML, we'd work to create a file that printed Hello World and get that to show up in the browser. To simplify, I would have a simple <code>.html</code> file that they could open up.</p> <p>Once the scaffolding was in place, I would pair with them to teach some fundamental concepts and then give them an assignment to code independently. For example, if they're learning HTML, we'd work together to build a version of the Google home page as it's relatively simple and teaches them about labels, input controls, and buttons.</p>"},{"location":"articles/2023/06/08/camerons-coaching-corner---mentoring-an-intern/#let-them-work","title":"Let Them Work","text":"<p>Once we've spent some time with the basics, we can reinforce that skill by having them work solo on an assignment. It's this independent time where they start learning those skills, get stuck, ask questions, but overall, make progress. For you, this is also where you can see what they're getting quickly and where they're struggling.</p> <p>For example, if we just built out the google.com page, I would give them an assignment to build a form page, as it uses the same concepts but forces them to think about how to structure it and apply those principles. The work you give should be accomplishable within a couple of hours, as anything that takes multiple days will kill any momentum they would gain.</p>"},{"location":"articles/2023/06/08/camerons-coaching-corner---mentoring-an-intern/#give-feedback","title":"Give Feedback","text":"<p>When the work is ready, they need to create a pull request and have you (and ideally your team) review their changes. It can be a nerve wracking experience for them as this may be their first time having their work critiqued as a professional, so keep in mind the tone and the verbiage used.</p> <p>One thing that can help is to walk them through how you do a code review, what you're looking for, and how you deliver feedback as this gives them context of what's expected of them.</p>"},{"location":"articles/2023/06/08/camerons-coaching-corner---mentoring-an-intern/#how-to-grow","title":"How To Grow","text":"<p>Once they've completed an assignment or two, see how they can plug into your current work. If they've been learning about HTML and you have a story for creating a new page, pull them in and pair on it. For example, you could pair on making the basic page and connections to the CSS and JavaScript, but then they could work independently on building the page out while you work on the other aspects.</p> <p>By dividing the work up, you're allowing them autonomy in how the work gets done, it helps you out, and gives them experience of working on a project, in the smaller scale.</p> <p>Do you have a question about leadership, career, or software engineering? Would you like a different perspective on these topics? Drop a line at CoachingCorner@TheSoftwareMentor.com or you can remain anonymous by filling out this form.</p>"},{"location":"articles/2023/06/22/camerons-coaching-corner-volume-2/","title":"Cameron's Coaching Corner Volume 2","text":"<p>Welcome to Cameron's Coaching Corner, where we answer questions from readers about leadership, career, and software engineering.</p> <p>In this week's post, we look at how Chase can balance writing the perfect code and shipping something.</p> <p>My question: As a young developer, I notice that sometimes I get paralyzed by options. I want to write the perfect piece of code. This helps me in writing good code but usually at the cost of efficiency. Especially when I am faced with multiple good options. Sometimes I want to KNOW I\u2019m gonna write the right thing before I\u2019m writing it when I my be better off with some trial and error</p> <ol> <li>Are these common problems that you see people face?</li> <li>What rules of thumb or other pieces of advice do you have to avoid writing nothing instead of something as a result of seeking the ideal?</li> <li>How important is planning vs trial and error (\"failing fast\" as they say) to good software development flow?</li> </ol> <p>First, absolutely yes, this is a common occurrence among developers; it's a sign of your style ane current experience, as many developers ask themselves this question. I've worked with developers who were meticulous to a fault, and their code was clean; however, they never shipped because it was never \"ready\" in their eyes. Therefore, little was gained. </p> <p>On the other hand, I've worked with developers who worked at breakneck speed, burning down backlogs and delivering tons of functionality. However, the code would be brittle, and maintaining the work after the fact would be challenging due to the mountain of technical debt.</p> <p>This topic resonates with me as I was that meticulous developer when I started my career. My early experience was with healthcare companies, where the focus was to get the solution right, even if it look longer. This focus made sense to me, so that I got accustomed to. However, this all changed when I joined a start-up where the focus was to get v1 out and start getting customers.</p> <p>Upon joining, I met a developer that was quick to implement a solution but would be hard to maintain. However, they were delivering 5-7x more value than I was. To learn how to work at this pace (and honestly, get my rear in gear), I paired up with them. I consider this a pivotal point in my career because they taught me how they could knock out work quickly, and in return, I taught them how to make tweaks to their design that led to more maintainable and testable code. Like iron sharpening iron, we both became better for it.</p> <p>Let's break down the next two parts - how do we get started, and what's the right mix of planning vs trying it out?</p> <p>When I'm approaching a problem, I do spend some time planning what I need to do. It might not be long (maybe 10 minutes or less), but I do write down the needed steps. Remember, software is the automation of steps, so if you don't know the steps, how could you possibly automate them?</p> <p>But what if you don't know how to solve the problem? At this point, I'd prototype and start experimenting. When it comes to R&amp;D, the industry focuses a ton on the \"D\" (Development) but can forget that Research is a normal part of the job.</p> <p>I start with a hypothesis and build the simplest thing that disproves or proves it. When possible, I eliminate or drastically reduce the amount of software to write. For example, I've used Powerpoint to build mock UIs and screen flows because it was something easy to share, and I didn't have to burn a lot of time creating custom CSS and an application over it.</p> <p>With my hypothesis and plan of what to build, I'm going to take shortcuts and write some of the worst throwaway code imaginable because it's not going to production; it's to get validation of my approach. If my experiment works, then hooray! I can start refactoring and cleaning to get to something better than before. If it fails, then no problem. I can scrap the whole thing and try a different hypothesis.</p> <p>Another way of thinking about this approach is that we're taking the largest risk/unknown and are building the simplest thing to learn or to mitigate the risk. As we rinse and repeat, we will find a spot where there aren't any major unknowns, and we can continue. Fun fact, this approach is known as Spiral Driven Development based from the Spiral Risk Model.</p> <p>At the end of the day, we are professionals, we are paid to solve problems, and if shipping software solves the problem, we should do that. At the same time, we need to do right by our client and provide a solution that not only works but is reasonably stable as well. This back-and-forth between quality and speed is not only natural, but healthy.</p> <p>Do you have a question about leadership, career, or software engineering? Would you like a different perspective on these topics? Drop a line at CoachingCorner@TheSoftwareMentor.com or you can fill out this form. </p>"},{"location":"articles/2023/07/06/coaching-corner-volume-3/","title":"Coaching Corner Volume 3","text":"<p>Welcome to Cameron's Coaching Corner, where we answer questions from readers about leadership, career, and software engineering.</p> <p>In this week's post, we look at how Alan can help their engineer figure out what they want to be when they grow up.</p> <p>Hey Cameron!</p> <p>I have a front-end engineer who's sharp, but they're not sure what their career growth looks like. I get the sense that they're interested in other roles outside of software development. How do you navigate this and help them grow?</p> <p>Oh man, I love working with these types of engineers, as you have no clue what they'll end up liking. I call these types Eevees because, just like the Pokemon, they need to be exposed to a bit of everything to truly figure out what they want to do.</p> <p></p> Eevee Evolutions from Bulbagarden"},{"location":"articles/2023/07/06/coaching-corner-volume-3/#what-do-they-like-and-dislike","title":"What Do They Like and Dislike?","text":"<p>To begin, you need to figure out what they like. You mention front-end development, but what about that type of work sparks joy for them?</p> <p>For example, if they enjoy working with customers to come up with clean designs, they might be interested in improving their User Experience skills and leveling up there.</p> <p>On the other hand, if they enjoy building reusable components and clean code, they might enjoy learning about a different part of the stack (like API design) and see how their knowledge can be applied.</p> <p>The main thing to note is that they might be interested in different roles (User Experience, Product Owner) or different parts of the technology stack (Front-end, Back-end, or DevOps), so I'd recommend keeping an open mind on possibilities.</p> <p>Just as important as their likes, you need to know their dislikes. For example, if they enjoy seeing the visuals of their work, then they may not like learning about databases or DevOps, as those parts of the stack aren't client-facing by default.</p> <p>What you're trying to do is thread a needle for opportunities that include some of their interests and stays away from their dislikes.</p>"},{"location":"articles/2023/07/06/coaching-corner-volume-3/#youve-got-a-plan-what-next","title":"You've Got a Plan, What Next?","text":"<p>Excellent, you know where a possible interest may lie, but how do you get started?</p> <p>If it's something you're an expert in or a role you're currently doing, have them shadow you and teach them what you're doing and the why. I've had success with this approach when engineers express interest in leadership. Once they have some experience, give them tasks to run solo with.</p> <p>If it's something you're not an expert in, you need to find that person where you're at and get a mentorship started. It can be an introductory conversation followed by a job shadow, but it should evolve into the person taking on tasks themselves.</p> <p>At this point, you want to see if the role could be a good fit. In the ideal world, your engineer would immediately know that it's not a good fit, so you can pivot to something else, but it will take time to figure that out.</p> <p>During this time, you need to understand that they will be learning new skills, and their focus won't be on their current responsibilities. To help, I'd recommend setting about 20% of their time for learning/trying out the new role as this continues giving you the engineering effort needed but gives the other person a real opportunity to try it out.</p>"},{"location":"articles/2023/07/06/coaching-corner-volume-3/#putting-up-guardrails","title":"Putting up Guardrails","text":"<p>Nice, we've got a mentor identified and ground rules established, so let's wrap up by defining some guard rails.</p> <p>Like anything new, your engineer is going to mess up and fail. That's perfectly normal, and it should be embraced as learning. As such, you shouldn't give them mission critical tasks to take on at the beginning.</p> <p>When I've done these experiments in the past, I've defined failure criteria, where if any of these things happen, the experiment immediately stops. This approach is similar to a futurespective, where we hypothesize scenarios that could cause us to fail and then plan responses for when that happens.</p> <p>In this scenario, that could be things like:</p> <ul> <li>The engineer doesn't feel comfortable doing the new tasks</li> <li>A critical project is in jeopardy, and the engineer needs to focus on the project</li> <li>There's a struggle with balancing the current responsibilities and the new tasks</li> <li>Then mentor doesn't feel comfortable working with the engineer</li> </ul>"},{"location":"articles/2023/07/06/coaching-corner-volume-3/#keep-trying","title":"Keep Trying!","text":"<p>Unless you have someone who is genuinely amenable to everything, there will be some failures and setbacks, and that's okay! Instead of thinking about it like Oh man, I messed up, think more along the lines of Hey, that didn't work because of X; let's try something else where X isn't involved.</p> <p>For Harry Potter fans out there, it reminds me of the scene where Harry gets his first wand at Ollivander's Wand Shop:</p> <p> Harry tried. And tried. He had no idea what Mr Ollivander was waiting for. The pile of tried wands was mounting higher and higher on the spindly chair, but the more wands Mr Ollivander pulled from the shelves, the happier he seemed to become. </p> <p> \"Tricky customer, eh? Not to worry, we'll find the perfect match here somewhere\" </p> <p>If you are upbeat about the process, this will make the other person feel at ease, increasing the odds of success!</p> <p>Do you have a question about leadership, career, or software engineering? Would you like a different perspective on these topics? Drop a line at CoachingCorner@TheSoftwareMentor.com or you can fill out this form.</p>"},{"location":"articles/2023/08/03/coaching-corner-volume-4/","title":"Coaching Corner Volume 4","text":"<p>Welcome to Cameron's Coaching Corner, where we answer questions from readers about leadership, career, and software engineering.</p> <p>In this post, we'll look at a question posed by Bastien in the Engineering Manager's Slack Group on how to praise your team.</p> <p>Context: New to Engineering Manager, managing 5 people and working in a 5 person team. My managees are not 100% on my team.</p> <p>Details: OK, so I've quickly learnt how to spot mistakes and follow up improvements to both teams (one I manage and one I work on). I'm confident taking actions and communicating on all of that. But there is the other side -&gt; congratulation and following up on behavior/action.</p> <p>Example: The current team has low velocity. They recently finished the specs and review. It didn't happen for months (always late on that), but it's their \"normal\" velocity. I congratulated them, but I'm wondering if I should have since they \"just did their job\".</p> <p>How do you congratulate your coworkers? Specifically</p> <ul> <li>Do you? Why or Why Not?</li> <li>How?</li> <li>On trivial/exceptional stuff?</li> <li>When and Where?</li> </ul> <p>This is a great question, as there's a fine line between not giving enough praise (e.g., \"Does my manager even care/notice?\") and giving it so much that it losses its effectiveness (e.g., \"My manager's praise doesn't mean much to me\").</p>"},{"location":"articles/2023/08/03/coaching-corner-volume-4/#should-you-congratulate-your-colleagues","title":"Should You Congratulate Your Colleagues?","text":"<p>Absolutely. Full stop. You should be congratulating your team for their accomplishments. It doesn't cost you anything to do so, and it can help build relationships with your team members as it's clear that you're noticing their efforts.</p> <p>I don't recommend congratulating them on every single thing as that's an expectation of the role. However, if it's a behavior I've been encouraging, I'll absolutely praise them.</p> <p>For example, I had a teammate that was chronically late to meetings, so much so that it put our project behind because they weren't communicating effectively. After having a conversation with them, they started being more communicative asynchronously, which helped us reduce the need for the meeting as they were able to lead the project.</p> <p>It's crucial that I praise this behavior as it's clear they took the coaching to heart, and I want them to know that I'm seeing the improvements and that I'm appreciative of their efforts.</p> <p>Outside of improvements, I'll also praise or send congrats for specific achievements and milestones. Some examples include:</p> <ul> <li>Completing their first pull request</li> <li>Shipping their first story</li> <li>Shipping a feature</li> <li>Handling a production support request</li> <li>Resolving a production outage</li> <li>Working outside of their comfort zone (e.g., an engineer doing DevOps work for the first time)</li> <li>Getting over a slog of a story/feature</li> <li>Celebrating a birthday or work anniversary</li> </ul> <p>In most of these cases, you could argue that it's expected behavior for the members of your team. However, you still want to recognize and appreciate them for knocking it out of the park. I've found it to be a great way to keep the team's momentum high.</p>"},{"location":"articles/2023/08/03/coaching-corner-volume-4/#how-do-you-praise-people","title":"How Do You Praise People?","text":"<p>When it comes to praising people, a common adage is to \"Praise publicly, criticize privately\", and overall, it's good advice. However, you need to know your audience and those that you want to praise. I've had team mates that would not want to be praised publicly as it would put the spotlight on them, something they don't want.</p> <p>In my initial one-on-ones, I'll ask the members of the team how they want to receive their praise and criticisms. Even though it's a bit more work for me, it helps me tailor my approach to each person so that it's more effective. I've had teammates that preferred to have their praise emailed to them or written down, whereas others are happy to have it verbally spoken to them.</p> <p>The only time that I'll put a spotlight for praise is when the team does something terrific. For example, did the team ship a highly requested feature? Even though I might send private messages to the individuals, I'll shout out the team in our public channel for others to see, as it's important to highlight the effort spent to get it done.</p>"},{"location":"articles/2023/08/03/coaching-corner-volume-4/#how-do-you-praise-when-things-arent-going-well","title":"How Do You Praise When Things Aren't Going Well?","text":"<p>All this being said, not everything is going to be rainbows, lollipops, and sunshine. There will be times where the team is struggling on a feature or getting through a slog of an item.</p> <p>My approach is to keep the team focused so we can get through it, but to also have a retrospective on what happened and what we could do to prevent another slog from happening again.</p> <p>In these cases, I'll congratulate the team for getting through it and showing tenacity, as it's not easy to get through difficult work.</p> <p>However, I don't celebrate that it was tough or that we did an awesome job as at the end of the day, something's not working in our process and we need to figure that out.</p> <p>Do you have a question about leadership, career, or software engineering? Would you like a different perspective on these topics? Drop a line at CoachingCorner@TheSoftwareMentor.com or you can fill out this form.</p>"},{"location":"articles/2024/06/03/coaching-corner-volume-5/","title":"Coaching Corner Volume 5","text":"<p>Welcome to Cameron's Coaching Corner, where we answer questions from readers about leadership, career, and software engineering.</p> <p>In this post, we'll look at a question from TheRefsAlwaysWin about how to get a new engineer on their team to open up and get comfortable asking for help.</p> <p>I've got a newish member to the team and they're still early in their career. They've got a good head on their shoulders, however, they tend to go down rabbit holes when problem solving and they don't speak up or ask for help.</p> <p>They've asked me a few times about \"how did I know ....\", and a lot of the times, it's experience (I've been doing this for a few decades now).</p> <p>How do I help them open up and ask more questions to the team and group?</p> <p>Any time that you have a new member join the team, there's going to be some acclimation going on (the \"storming\" portion of the Tuckman Stages of Group Development Model). If the team has been together for a while, there could be some personality conflicts at play. </p> <p>Pretend that you're a student who just transferred to a new school. The kids in the class have been together since kindergarten and you're the new person. It can be intimidating navigating the group dynamics since you don't have context.</p> <p>For the new member to your team, the same thing could be at play, especially since they're a newer engineer, there might be some confidence issues at play.</p>"},{"location":"articles/2024/06/03/coaching-corner-volume-5/#establish-psychological-safety","title":"Establish Psychological Safety","text":"<p>It might be cliche, but the first thing I'd do here is make sure that they feel comfortable even asking questions. Depending on the environment they've been in, they could have experienced being belittled, bullied, or otherwise put down for asking questions. In one of my first jobs, I remember my questions coming up during a performance review.</p> <p>If a person doesn't feel emotionally safe at work, then it's highly unlikely for them to be successful because they will be worried/stressed about how they come off, on top of trying to learn the new codebase, problem domain, teammates, etc...</p> <p>You can demonstrate this by practicing what you preach. Ask questions in your chat channels, in meetings, where ever it makes sense. Let the person know that you want them asking questions as it will help them learn faster than struggling in the corner somewhere.</p>"},{"location":"articles/2024/06/03/coaching-corner-volume-5/#encouraging-curiosity-and-questions","title":"Encouraging Curiosity and Questions","text":"<p>When I onboard a new engineer, I tell them that they should be asking questions and making all the mistakes that they can as this is the perfect time to learn.</p> <p>New people have the gift of perspective, so decisions that made sense to the team at the time may not make sense to the newcomer, hence their questions can actually impact change.</p> <p>I like to tell new team members that if they're not asking at least 3 questions a day, I'm not challenging them enough. No matter how much technical experience you have, there are questions to be asked about our product, the users, the problem that we're trying to solve, or team history that can (or should) be asked.</p> <p>Another technique I'll use is to make sure they have a mentor that is not their boss help coach them along. By removing the boss/subordinate dynamic, this leads to the mentee feeling more comfortable asking \"dumb\" questions because they're not bothering their boss.</p> <p>During the mentoring period, I'll check in with the mentor to see if there's anything I need to encourage or to dig into with the mentee, but overall, the results of the mentee/mentor will speak for themselves and I'll let the mentor own that relationship.</p>"},{"location":"articles/2024/06/03/coaching-corner-volume-5/#learning-how-they-think","title":"Learning How They Think","text":"<p>On the subject of rabbit holes, this is a super common occurrence for engineers. We get an idea, then we want to try it out to see how it works. What gets us into trouble is that it's three hours later and we're still futzing with a solution.</p> <p>When I work on a \"new to me\" task, I give myself a time limit (generally 30 minutes) to make actionable progress. With all the available libraries, documentation, and resources that are out there, if I've not made any progress in that time, that implies one of two things:</p> <ul> <li>I'm thinking about the problem wrong (shoving a square peg into a round hole)</li> <li>I don't understand a core concept, which in turn, is impacting my decisions and troubleshooting.</li> </ul> <p>In either case, it's a good idea for me to bounce ideas off of someone else to help me figure out where I've got something wrong. </p> <p>Generally, speaking to the duck will help me catch the simple issues, but for more complex topics, I want a conversation with someone to help me flush out the idea/design.</p> <p>When I work with engineers, I take a more Socratic approach and ask them what have they tried and more importantly why did they think that would work.</p> <p>The last part is key here, by understanding why they thought it would work, this sheds light on how they think which allows me to see their assumptions and where the gaps of understanding are. </p> <p>For example, if an engineer is trying to fix an issue in the CI/CD pipeline where one of the tests is failing, I might ask if they've tried running it locally. </p> <p>If not, then we can dig into why they didn't try that first. This could expose flaws in knowledge (didn't know we could run them locally or how to run them locally) or in troubleshooting (I was looking at the logs, but never though to run them locally).</p> <p>Regardless, of the answer, this gives me context on where I need to focus my coaching to help get them unstuck and back on track for their task.</p> <p>Do you have a question about leadership, career, or software engineering? Would you like a different perspective on these topics? Drop a line at CoachingCorner@TheSoftwareMentor.com or you can fill out this form.</p>"},{"location":"articles/2025/07/15/creating-a-node-tool-with-typescript-and-jest-from-scratch/","title":"Creating a Node Tool With TypeScript and Jest From Scratch","text":"<p>In a previous post, I showed how to create a Node project from scratch. This is a great basis to start building other types of projects, so in this post, I'm going to show you how to build a tool that you can install via npm.</p> <p>One of my favorite tools to use is gitignore, useful for generating a stock <code>.gitignore</code> file for all sorts of projects.</p> <p>Let's get started!</p> <p>Note: Do you prefer learning via video? You can find a version of this article on YouTube</p> <p>At a high level, we'll be doing the following</p> <ol> <li>Setting the Foundation</li> <li>Update our project to compiler TypeScript to JavaScript</li> <li>Update our testing workflow</li> <li>Set up our executable</li> </ol>"},{"location":"articles/2025/07/15/creating-a-node-tool-with-typescript-and-jest-from-scratch/#step-0-dependencies","title":"Step 0 - Dependencies","text":"<p>For this project, the only tool you'll need is the Long Term Support (LTS) version of Node (as of this post, that's v22, but these instructions should hold regardless). If you're working with different Node applications, then I highly recommend using a tool to help you juggle the different versions of Node you might need (Your options are nvm if you're on Mac/Linux or Node Version Manager for Windows if you're on Windows).</p>"},{"location":"articles/2025/07/15/creating-a-node-tool-with-typescript-and-jest-from-scratch/#step-1-setting-the-foundation","title":"Step 1 - Setting The Foundation","text":"<p>The vast majority of work we need was created as part of build a basic node project, so make sure to complete the steps there first before proceeding!</p>"},{"location":"articles/2025/07/15/creating-a-node-tool-with-typescript-and-jest-from-scratch/#step-2-compiling-typescript-to-javascript","title":"Step 2 - Compiling TypeScript to JavaScript","text":"<p>In order for another application to use our library, we need to make sure we're shipping JavaScript, not TypeScript. To make that happen, we're going to be using the TypeScript compiler, <code>tsc</code> to help us out.</p>"},{"location":"articles/2025/07/15/creating-a-node-tool-with-typescript-and-jest-from-scratch/#telling-typescript-where-to-find-files","title":"Telling TypeScript Where To Find Files","text":"<p>When we first setup TypeScript, we created a basic <code>tsconfig.json</code> file and kept the defaults. However, in order to publish a library, we need to set two more <code>compilerOptions</code> in the file.</p> <p>First, we need to set the <code>outDir</code> property so that our compiled code all goes into a single directory. If we don't do this, our JavaScript files be next to our TypeScript files and that creates a mess.</p> <p>Second, we need to set the <code>rootDir</code> property so that the TypeScript compiler knows where to search for our code.</p> <p>Let's go ahead and make those changes in the <code>tsconfig.json</code> file.</p> <pre><code>// existing code\n\"compilerOptions\": {\n  // existing code\n  \"outDir\": \"dist\", // This tells the TypeScript compiler where to put the compiled code at\n  \"rootDir\": \"src\", // This tells the TypeScript compiler where to search for TypeScript code to compile\n}\n// existing code\n</code></pre>"},{"location":"articles/2025/07/15/creating-a-node-tool-with-typescript-and-jest-from-scratch/#adding-a-build-script","title":"Adding a build script","text":"<p>Now that we've told TypeScript which files to compile and where to put those files, we can update our <code>package.json</code> file with a build script that will invoke the TypeScript compiler, <code>tsc</code> when ran:</p> <pre><code>{\n  // existing code\n  \"script\":{\n    \"build\": \"tsc\"\n    // existing scripts\n  }\n  // existing code\n}\n</code></pre> <p>With this script in place, we can run <code>npm run build</code> from the command line and we'll see that a <code>dist</code> folder was created with some JavaScript files inside.</p>"},{"location":"articles/2025/07/15/creating-a-node-tool-with-typescript-and-jest-from-scratch/#improving-the-build-with-rimraf","title":"Improving the Build with Rimraf","text":"<p>Now that we have compilation happening, we need a way to make sure our <code>dist</code> folder is cleaned out before a new compilation as this helps make sure that we don't have old files hanging around. This will also simulate our Continuous Integration pipeline when we start working on that.</p> <p>We could update our build script with something like <code>rm -rf dist</code>, but that's a Linux command, which most likely won't work on Windows. So let's add a new library, rimraf, that handles removing files in an OS agnostic way.</p> <pre><code>npm install --save-dev rimraf\n</code></pre> <p>After installing, we can update our build script to be the following.</p> <pre><code>{\n  \"build\": \"rimraf dist &amp;&amp; tsc\"\n}\n</code></pre> <p>Now, when we run <code>npm run build</code>, we'll see that the <code>dist</code> folder is removed and then recreated.</p>"},{"location":"articles/2025/07/15/creating-a-node-tool-with-typescript-and-jest-from-scratch/#step-3-updating-jest-to-run-the-right-files","title":"Step 3 - Updating Jest to Run the Right Files","text":"<p>At this point, we have our TypeScript being compiled into JavaScript, so we're close to being ready to publish our tool. As a sanity check, let's go ahead and run our test suite from step 1</p> <pre><code>npm run test\n</code></pre> <p></p> <p>Uh oh, it looks like <code>jest</code> is not only running the tests in the <code>src</code> folder, but also in the <code>dist</code> folder. We'll need to tweak our <code>jest.config.js</code> file to ignore the dist folder.</p> <pre><code>// existing code\nmodule.exports = {\n  //existing code\n},\ntestPathIgnorePatterns: ['dist/'] // this will ignore any matches in the dist folder\n</code></pre> <p>If we run our test command again, we'll only see the index.spec.ts file in the listing.</p> <p></p>"},{"location":"articles/2025/07/15/creating-a-node-tool-with-typescript-and-jest-from-scratch/#step-4-setting-up-the-executable","title":"Step 4 - Setting up the executable","text":""},{"location":"articles/2025/07/15/creating-a-node-tool-with-typescript-and-jest-from-scratch/#adding-bin-to-packagejson","title":"Adding Bin to Package.json","text":"<p>Now, it's time to tell <code>npm</code> which file to execute as part of the tool. </p> <p>First, we need to update our <code>package.json</code> file to include a new property bin.</p> <pre><code>{\n  // existing code\n  \"main\": \"index.js\",\n  \"bin\": {\n    \"nameOfExecutable\": \"dist/index.js\"\n  }\n}\n</code></pre> <p>So if we wanted the name of our tool to be <code>greet</code>, then we would update the \"nameOfExecutable\" to be <code>greet</code>.</p>"},{"location":"articles/2025/07/15/creating-a-node-tool-with-typescript-and-jest-from-scratch/#updating-index-with-shebang","title":"Updating Index With Shebang","text":"<p>If we tried to run our tool now, we'd see that nothing would happen, but the file would be opened in a text editor.</p> <p>It turns out, that if we don't add a [shebang](https://en.wikipedia.org/wiki/Shebang_(Unix)) to the file, then Node doesn't know that it should execute this file. </p> <p>So let's update our <code>index.ts</code> file</p> <pre><code>#!/usr/bin/env node\n// rest of the file\n</code></pre>"},{"location":"articles/2025/07/15/creating-a-node-tool-with-typescript-and-jest-from-scratch/#step-5-building-the-tool","title":"Step 5 - Building the Tool","text":"<p>With this final step, we're in a good place to build our tool and try it out.</p> <p>To create the package, we need to run two commands:</p> <pre><code>npm run build # This creates a clean folder with our code to deploy\nnpm pack # This creates a tarball that has our code, which we can execute via npx\n</code></pre>"},{"location":"articles/2025/07/15/creating-a-node-tool-with-typescript-and-jest-from-scratch/#next-steps","title":"Next Steps","text":"<p>With this final step done, we have a great foundation for building out our tool! For example, you could build your own version of [gitignore], maybe a command line interface that wraps around a favorite tool of yours, the sky's the limit!</p>"},{"location":"articles/2025/07/10/creating-a-node-project-with-typescript-and-jest-from-scratch/","title":"Creating a Node Project With TypeScript and Jest From Scratch","text":"<p>When teaching engineers about Node, I like to start with the bare bone basics and build from there. Though there is value in using tools to auto-scaffold a project (Vite for React applications, for example), there's also value in understanding how everything hangs together.</p> <p>In this post, I'm going to show you how to create a Node project from scratch that has TypeScript configured, Jest configured, and a basic test suite in place.</p> <p>Let's get started!</p> <p>Note: Do you prefer learning via video? You can find this article on YouTube</p>"},{"location":"articles/2025/07/10/creating-a-node-project-with-typescript-and-jest-from-scratch/#step-0-dependencies","title":"Step 0 - Dependencies","text":"<p>For this project, the only tool you'll need is the Long Term Support (LTS) version of Node (as of this post, that's v22, but these instructions should hold regardless). If you're working with different Node applications, then I highly recommend using a tool to help you juggle the different versions of Node you might need (Your options are nvm if you're on Mac/Linux or Node Version Manager for Windows if you're on Windows).</p>"},{"location":"articles/2025/07/10/creating-a-node-project-with-typescript-and-jest-from-scratch/#step-1-creating-the-directory-layout","title":"Step 1 - Creating the Directory Layout","text":"<p>A standard project will have a layout like the following:</p> <pre><code>projectName\n    \\__src # source code for project is here\n        \\__ index.ts\n        \\__ index.spec.ts\n    \\__ package.json\n    \\__ package.lock.json\n    \\__ README.md\n</code></pre> <p>So let's go ahead and create that. You can do this manually or by running the following in your favorite terminal.</p> <pre><code>mkdir &lt;projectName&gt; &amp;&amp; cd projectName\nmkdir src\n</code></pre>"},{"location":"articles/2025/07/10/creating-a-node-project-with-typescript-and-jest-from-scratch/#step-2-setting-up-node","title":"Step 2 - Setting up Node","text":"<p>With the structure in the right place, let's go ahead and create a Node application. The hallmark sign of a Node app is the <code>package.json</code> file as this has three main piece of info.</p> <ol> <li>What's the name of the application and who created it</li> <li>What scripts can I execute?</li> <li>What tools does it need to run?</li> </ol> <p>You can always manually create a <code>package.json</code> file, however, you can generate a standard one by using <code>npm init --yes</code>. </p> <p>Tip: By specifying the <code>--yes</code> flag, this will generate a file with default settings that you can tweak as needed.</p>"},{"location":"articles/2025/07/10/creating-a-node-project-with-typescript-and-jest-from-scratch/#step-3-setting-up-typescript","title":"Step 3 - Setting up TypeScript","text":"<p>At this point, we have an application, but there's no code or functionality. Given that we're going to be using TypeScript, we'll need to install some libraries.</p>"},{"location":"articles/2025/07/10/creating-a-node-project-with-typescript-and-jest-from-scratch/#installing-libraries","title":"Installing Libraries","text":"<p>In the project folder, we're going to install both <code>typescript</code> and a way to execute it, <code>ts-node</code>.</p> <pre><code>npm install --save-dev typescript ts-node\n</code></pre> <p>Note: With Node v24, you can execute TypeScript natively, but there are some limitations. For me, I still like using ts-node for running the application locally.</p>"},{"location":"articles/2025/07/10/creating-a-node-project-with-typescript-and-jest-from-scratch/#setting-up-typescript","title":"Setting up TypeScript","text":"<p>Once the libraries have been installed, we need to create a <code>tsconfig.json</code> file. This essentially tells TypeScript how to compile our TypeScript to JavaScript and how much error checking we want during our compilation.</p> <p>You can always create this file manually, but luckily, <code>tsc</code> (the TypeScript compiler) can generate this for you.</p> <p>In the project folder, we can run the following</p> <pre><code>npx tsc --init\n</code></pre>"},{"location":"articles/2025/07/10/creating-a-node-project-with-typescript-and-jest-from-scratch/#writing-our-first-typescript-file","title":"Writing Our First TypeScript File","text":"<p>At this point, we have TypeScript configured, but we still don't have any code. This is when I'll write a simple <code>index.ts</code> file that leverages TypeScript and then try to run it with <code>ts-node</code>.</p> <p>In the <code>src</code> folder, let's create an <code>index.ts</code> file write the following code. </p> <pre><code>export function add(a:number, b:number): number {\n  return a+b;\n}\n\nconsole.log(\"add(2,2) =\", add(2,2));\n</code></pre> <p>This uses TypeScript features (notice the type annotations), which if we try to run this with <code>node</code>, we'll get an error.</p>"},{"location":"articles/2025/07/10/creating-a-node-project-with-typescript-and-jest-from-scratch/#using-ts-node-to-run-file","title":"Using ts-node To Run File","text":"<p>Let's make sure everything is working by using <code>ts-node</code>.</p> <p>Back in the terminal, run the following:</p> <pre><code>npx ts-node src/index.ts\n</code></pre> <p>If everything works correctly, you should see the following in the terminal window.</p> <pre><code>add(2,2) = 4\n</code></pre>"},{"location":"articles/2025/07/10/creating-a-node-project-with-typescript-and-jest-from-scratch/#adding-our-first-npm-script","title":"Adding Our First NPM Script","text":"<p>We're able to run our file, but as you could imagine, it's going to get annoying to always type out <code>npx ts-node src/index.ts</code>. Also, this kills discoverability as you have to document this somewhere (like a README.md) or it'll become a thing that someone \"just needs to know\".</p> <p>Let's improve this by adding a script to our <code>package.json</code> file.</p> <p>Back in setting up node, I mentioned that one of the cool things about <code>package.json</code> is that you can define custom scripts.</p> <p>A common script to have defined is <code>start</code>, so let's update our <code>package.json</code> with that script.</p> <pre><code>{\n  // some code here\n  \"scripts: {\n    \"start\": \"ts-node src/index.ts\"\n    // other scripts here\n  },\n}\n</code></pre> <p>With this change, let's head back to our terminal and try it out.</p> <pre><code>npm run start\n</code></pre> <p>If everything was setup correctly, we should see the same output as before.</p>"},{"location":"articles/2025/07/10/creating-a-node-project-with-typescript-and-jest-from-scratch/#step-4-setting-up-jest","title":"Step 4 - Setting up Jest","text":"<p>At this junction, we can execute TypeScript and made life easier by defining a <code>start</code> script. The next step is that we need to set up our testing framework.</p> <p>While there are quite a few options out there, a common one is jest so that's what we'll be using in this article.</p> <p>Since jest is a JavaScript testing library and our code is in TypeScript, we'll need a way to translate our TypeScript to JavaScript. The jest docs mention a few ways of doing this (using a tool like Babel for example). However, I've found using <code>ts-jest</code> to be an easier setup and still get the same outcomes.</p>"},{"location":"articles/2025/07/10/creating-a-node-project-with-typescript-and-jest-from-scratch/#installing-libraries_1","title":"Installing Libraries","text":"<p>With our tools selected, let's go ahead and install them.</p> <pre><code>npm install --save-dev jest ts-jest @types/jest\n</code></pre> <p>Note: You might have seen that we're also installing \"@types/jest\". This does't provide any functionality, however, it does gives us the types that jest uses. Because of that, our code editor can understand @types/jest and give us auto-complete and Intellisense when writing our tests</p>"},{"location":"articles/2025/07/10/creating-a-node-project-with-typescript-and-jest-from-scratch/#configuring-jest","title":"Configuring Jest","text":"<p>So we have the tools installed, but need to configure them. Generally, you'll need a <code>jest.config.js</code> file which you can hand-write.</p> <p>Or we can have ts-jest generate that for us :)</p> <pre><code>npx ts-jest config:init\n</code></pre> <p>If this step works, you should have a <code>jest.config.js</code> file in the project directory.</p> <p>Let's write our first test!</p>"},{"location":"articles/2025/07/10/creating-a-node-project-with-typescript-and-jest-from-scratch/#writing-our-first-test","title":"Writing Our First Test","text":"<p>Jest finds tests based on file names. So as long as your test file ends with either <code>.spec.ts</code>, <code>.spec.js</code>, <code>.test.ts</code>, or <code>.test.js</code>, jest will pick it up.</p> <p>So let's create a new file in the <code>src</code> folder called <code>index.spec.ts</code> and add the following:</p> <pre><code>import { add } from \"./index\"\n// describe is container for one or more tests\ndescribe(\"index\", () =&gt; {\n  // it - denotes this is a test\n  it(\"does add work\", () =&gt; {\n    const result = add(2,2);\n\n    expect(result).toBe(4);\n  })\n})\n</code></pre> <p>With our test written, we can run it in the terminal with the following:</p> <pre><code>npx jest\n</code></pre>"},{"location":"articles/2025/07/10/creating-a-node-project-with-typescript-and-jest-from-scratch/#adding-a-test-npm-script","title":"Adding a Test NPM Script","text":"<p>Just like when we added a custom <code>start</code> script to our <code>package.json</code> file, we can do a similar thing here with our tests. </p> <p>In <code>package.json</code>, update the <code>scripts</code> section to look like:</p> <pre><code>{\n  // other code\n  \"script\": {\n    \"start\": \"ts-node src/index.ts\",\n    \"test\": \"jest\"\n  },\n  // other code\n}\n</code></pre> <p>With this change, we can run our test suite by using <code>npm run test</code> in the terminal.</p> <p>Congrats, just like that, you have a working Node application with TypeScript and Jest for testing!</p>"},{"location":"articles/2025/07/10/creating-a-node-project-with-typescript-and-jest-from-scratch/#next-steps","title":"Next Steps","text":"<p>With the scaffolding in place, you're in a solid spot to start growing things out. For example, you could...</p> <ul> <li>Start setting up a Continuous Integration pipeline</li> <li>Fine-tune your <code>tsconfig.json</code> to enable more settings (like turning off the ability to use any)</li> <li>Fine-tune your <code>jest.config.js</code> (like having your mocks auto-rest)</li> <li>Start writing application code!</li> </ul>"},{"location":"articles/2024/04/25/troubleshooting-a-dynamodb-connection-issue/","title":"Troubleshooting a DynamoDB Connection Issue","text":"<p>Most of my blog posts cover process improvements, leadership advice, and new (to me) technologies. In this post, I wanted to shift a bit and cover some of the fun troubleshooting problems that I run into from time to time.</p> <p>Enjoy!</p>"},{"location":"articles/2024/04/25/troubleshooting-a-dynamodb-connection-issue/#the-setup-how-did-we-get-here","title":"The Setup - How Did We Get Here?","text":"<p>At a high level, the team had a need to process messages coming from a message queue, parse the data, and then insert into a DynamoDB table. At a high level, here's what the architecture looked like:</p> <pre><code>graph LR\nQueue[Message Queue] --&gt; Lambda[Lambda]\nLambda --&gt; Process[Process?]\nProcess --&gt; |Failed| DLQ[Dead Letter Queue]\nProcess --&gt; |Success| DB[DynamoDB Table]</code></pre> <p>The business workflow is that a batch job was running overnight that would send messages to various queues (including this one). The team knew that we would receive about 100K messages, but had plenty of time to process them as this data was not needed for real-time.</p>"},{"location":"articles/2024/04/25/troubleshooting-a-dynamodb-connection-issue/#what-went-wrong","title":"What Went Wrong?","text":"<p>For the first night, everything worked as intended. However, for the second night, the team saw that only some of the messages made it to their DynamoDB table. A non-trivial number of them errored out with a message of <code>Error: connect EMFIL &lt;IP ADDRESS&gt;</code>.</p> <p>I don't know about you, but I had never seen <code>EMFIL</code> as an error before and the logs weren't very helpful on what was going on.</p> <p>Doing some digging, we found this GitHub Issue where someone has ran into a similar problem.</p> <p>Digging through the comment chain, we found this comment, stating that you could run into this problem if you were exhausting the connection pool to DynamoDB.</p> <p>Ah, now that's an idea! Even though I hadn't seen that error before, I know that if an application isn't cleaning up their connections properly, then the server can't accept new ones and that would fail the application. With almost 100K messages coming through and the large amount of failures, I could absolutely see how that might be the issue.</p>"},{"location":"articles/2024/04/25/troubleshooting-a-dynamodb-connection-issue/#inspecting-the-code","title":"Inspecting the Code","text":"<p>With this in mind, I started to take a look at the lambda in question and found the following:</p> <pre><code>export const handler = (event) =&gt; {\n  // logic to parse event\n\n  const dbClient = DynamoDbDocumentClient.from(new DynamoDBClient());\n\n  // logic to insert event\n}\n</code></pre> <p>Aha! This code implies that for every execution of the lambda, it would attempt to create a new connection.</p>  But Cameron, hold up. Yes, it will create the connection every time the lambda executes, but once the lambda is done, the connection will get cleaned up, so will it really try to spin up 100K connections?  <p>You're right, when the lambda goes out of scope, the connection will get cleaned up.</p> <p>But don't forget, it'll take the target server (DynamoDB) some time to tidy up. The problem is that since we were slamming 100K messages in rapid succession, DynamoDB didn't have enough time to clean up the connection before another connection was requested. And that was the problem.</p>"},{"location":"articles/2024/04/25/troubleshooting-a-dynamodb-connection-issue/#resolution","title":"Resolution","text":"<p>Now that we have an idea on what the problem could be, time to fix it. In this case, the change is straightforward (though the reasoning might not be.)</p> <p>So instead of having this</p> <pre><code>export const handler = (event) =&gt; {\n  // logic to parse event\n\n  const dbClient = DynamoDbDocumentClient.from(new DynamoDBClient());\n\n  // logic to insert event\n}\n</code></pre> <p>We moved the client creation to be outside of the <code>handler</code> block altogether.</p> <pre><code>const dbClient = DynamoDbDocumentClient.from(new DynamoDBClient());\n\nexport const handler = (event) =&gt; {\n  // logic to parse event\n  // logic to insert event\n}\n</code></pre>  Wait, wait. How does this solve the problem? You're still going to be executing this code for every message, so won't you have the same issue?  <p>Now that's a great question! Something that the team learned is that when a Lambda gets spun-up, there's a context that's created that hosts the external dependencies. When a lambda execution finishes, the context is maintained by AWS for a certain amount of time to be reused in case the lambda is invoked again. This saves on the init/start-up times.</p> <p>Because of the shared context, this allows us to essentially pool the connections and drastically reduce the amount of connections needed.</p> <p>This same advice is given in the best practices documentation for lambdas.</p>"},{"location":"articles/2024/04/25/troubleshooting-a-dynamodb-connection-issue/#lessons-learned","title":"Lessons Learned","text":"<p>After making the code change and redeploying, we were able to confirm that everything was working again with no issues.</p> <p>Even though the problem was new to us, this was a great opportunity to learn more about how Lambdas work under the hood, understand more about execution context, and a bit of dive into troubleshooting unknown errors for the team.</p>"},{"location":"articles/2024/04/30/running-effective-experiments-with-the-team/","title":"Running Effective Experiments With the Team","text":"<p>As a leader, you're always on the look out for new tools and approaches to help the team be more effective.</p> <p>But what happens when you have an idea? How do you introduce it to the team and get buy-in? How do you encourage others to propose ideas as well (remember, you're job isn't to have all the ideas, but to encourage and choose the best ones).</p> <p>Let's say that the idea works, what happens next? What if it failed, what do you do next? How do you share your lessons with others?</p> <p>In this post, I'll walk you through my approach for running experiments with the team and how to answer these questions. Like any other advice, I've found success using this process, but you might find that you'll need to tweak or adjust for your team.</p>"},{"location":"articles/2024/04/30/running-effective-experiments-with-the-team/#working-in-the-open","title":"Working In the Open","text":"<p>When it comes to the team, I'm a big proponent of working in the open. Not only does this reduce the amount of questions from my leader about what we're doing, it also empowers others to chime in when they see something off or the team going down the wrong path.</p> <p>With this philosophy in mind, I document our experiments in the team wiki. Now, I know that we should favor people over processes, however, I have found immense value in taking the 10 minutes to document as this helps get everyone on the same page and when we look at these experiments later, we have the context behind the experiment.</p> <p>To me, this no different than a scientist writing down their experiments for later reference.</p>"},{"location":"articles/2024/04/30/running-effective-experiments-with-the-team/#defining-an-experiment","title":"Defining an Experiment","text":"<p>As you might have guessed, I'm a big fan of using the scientific method for engineering work and especially so when it comes to experiments. As such, I capture the following info:</p>  Seriously, if we're not taking notes, what kind of scientists are we?   Photo by Louis Reed on Unsplash <ul> <li>Context - Why are we doing this? What inspired the experiment or what problem are we trying to solve?</li> <li>Hypothesis - What change are we proposing and what outcome are we looking for?</li> <li>Implementation - How are we going to run this experiment?</li> <li>Duration - How long are we going to run this experiment for?</li> <li>(Optional) Immediate Failure Criteria - Is there anything that could happen during this experiment that would cause to immediately stop?</li> </ul> <p>For those looking for a template, you can find a markdown version in my Leadership Toolkit on GitHub</p>"},{"location":"articles/2024/04/30/running-effective-experiments-with-the-team/#scheduling-the-retrospective","title":"Scheduling the Retrospective","text":"<p>With the experiment documented, I send a meeting request the day after the experiment is scheduled to end. The goal of this meeting is to reflect on the experiment and to decide whether we should adopt the changes or to stop.</p>"},{"location":"articles/2024/04/30/running-effective-experiments-with-the-team/#leading-the-team","title":"Leading the Team","text":"<p>After sending this meeting, my job is to help the team implement the experiment and coach/encourage as needed. Since it's a process change, it might take a bit for the team to adjust, so showing some patience and understanding is critical here.</p> <p>While we are going the through the experiment, I'm going to note any changes that I'm noticing. For example, if we're running an experiment to have asynchronous stand-ups, I'm going to take notes on how I'm feeling about the team getting updates and how they're communicating with each other.</p> <p>Depending on what comes up in our one-on-ones, I might even use this as a starter question.</p>"},{"location":"articles/2024/04/30/running-effective-experiments-with-the-team/#retrospective","title":"Retrospective","text":"<p>Once the experiment has ran its course, it's time to reflect on the experiment and decide as a team on whether to adopt the changes or reject them.</p> <p>To prepare, I'd recommend getting the right people in the room and setting the context.</p> <p>During the retro, the team should be doing the majority of the talking. Your role is to seed the conversation and make sure everyone gets their opinions out. I like to capture these notes on a board so that the team has clear visibility on what worked and didn't work.</p> <p>Once the notes have been added to the board, it's time for the team to decide to adopt the changes or not. During this step, I remind the team that this process isn't set in stone and if we want to tweak it in a future experiment, that's normal and encouraged.</p> <p>At this point, I update the experiment write-up that we did earlier with the team decision and the logic behind the decision. This provides an easy way of sharing our lessons with others.</p>"},{"location":"articles/2024/04/30/running-effective-experiments-with-the-team/#sharing-outcomes-with-others","title":"Sharing Outcomes With Others","text":"<p>One cool thing about leading teams is that no two teams are the same. Between the personalities, skills, company culture, and motivations, what works for one team won't work for another team (and the other way around).</p> <p>Because of this, it's critical to share your results with your leader and your peers. This way, they could see what we did, what worked, what didn't work, and hopefully get inspired to run their own experiments with teams.</p> <p>If the team paid a learning tax for an experiment, why wouldn't we share those results with others so that they can learn from our experiences? They might be able to make suggestions to turn a failure into a success or to ask questions about how we dealt with an issue.</p> <p>The group being successful is your success, do don't hoard knowledge, share it with others!</p> <p>With the write-up completed, you can start simply by sending a link to the group. A better approach would be to have a standing agenda item for your team lead meeting where leaders can talk about experiments that have been ran recently and their outcomes.</p>"},{"location":"articles/2024/04/30/running-effective-experiments-with-the-team/#common-mistakes","title":"Common Mistakes","text":"<p>When I've worked with leaders to introduce experiments, it can be a lot to take in because this is a different way of thinking. This is especially true if leaders are not in a psychologically safe environment or if they have prior experiences that weren't successful.</p> <p>I can't guarantee that you'll run experiments flawlessly, however, if you avoid these common mistakes, your odds of success will be higher.</p>"},{"location":"articles/2024/04/30/running-effective-experiments-with-the-team/#not-time-boxing-the-experiment","title":"Not Time Boxing the Experiment","text":"<p>One of the key features of the experiment is that it's only going to run for a set period of time, so that if you find that it's not working, you've not permanently impacted the team.</p> <p>If you have an experiment that's going to run into perpetuity, that's not an experiment anymore, that's a process change and that shouldn't go through this workflow because experiments can be abandoned, but process changes typically can't.</p>"},{"location":"articles/2024/04/30/running-effective-experiments-with-the-team/#treating-experiments-as-foregone-conclusions","title":"Treating Experiments as Foregone Conclusions","text":"<p>At some point, you're going to get a directive from your leader that you don't agree with, but you need to commit to the idea anyway. You know the idea isn't going to go over with the team, so you think framing it as an experiment can soften the blow.</p> <p>DON'T DO THIS!</p> <p></p>  Really, don't do this!  <p>Experiments are just that, experiments. They are not a vehicle for you to make unpopular changes. If you use experiments for slipping in these types of changes, then the team will learn that experiments is code for \"not great idea\" and they'll stop using the process.</p> <p>Remember, experiments are ideas that you and the team come up with to make things better, not directives from the top coming down.</p> <p>Now, you could use an experiment to figure out a way to carry out the directive. A good leader tells you where we have to go, but not necessarily how to get there. The experiment could be to figure out how to get there, but not what the destination should be.</p>"},{"location":"articles/2024/04/30/running-effective-experiments-with-the-team/#running-multiple-experiments","title":"Running Multiple Experiments","text":"<p>When getting a new team or after identifying multiple areas that a team could improve in, it's going to be tempting to want to implement multiple changes at once.</p> <p>Resist the urge.</p> <p>Remember, an experiment, by definition, is a process change. So the more experiments you run, the more process changes happening, which in turn puts more stress on the team to remember all the changes.</p> <p>In addition to all the process changes, you might find that one experiment futzes with another experiment and you may not get clear results.</p> <p>Let's say that we had two experiments going on at the same, asynchronous stand-ups and spending Tuesday afternoons in independent learning. During your one-on-ones, you get some feedback that it's a bit odd to not know what other team members are working on.</p> <p>What's driving that? Is it the async stand-ups? Or is it the dedicated learning time? Could it be both? You can't be sure.</p> <p>Another way to think about this is to think about debugging a program. If something's not working, do you change 5 things at once? No, you're going to change one thing, re-run, and see what happens.</p> <p>Same thing for experiments.</p>  But Cameron! This team is a hot mess and could stand to improve in so many areas, what should I do then?  <p>Instead of running all the experiments, instead, the team should decide which experiment would have the biggest payoff and then pursue that one. Remember, you're not playing the short game, but you're in for the long haul, so you'll have the time to make those changes.</p>"},{"location":"articles/2014/01/07/beginner-basics-establishing-a-solid-foundation--the-dependency-inversion-principle/","title":"Beginner Basics: Establishing a SOLID Foundation \u2013 The Dependency Inversion Principle","text":"<p>Welcome to the final installment of Establishing a SOLID Foundation series. In this post, we\u2019ll be exploring the fifth part of SOLID, the Dependency Inversion Principle.</p>"},{"location":"articles/2014/01/07/beginner-basics-establishing-a-solid-foundation--the-dependency-inversion-principle/#what-is-the-dependency-inversion-principle","title":"What is the Dependency Inversion Principle?","text":"<p>When working with object-oriented languages, we take large problems and break them down into smaller pieces. These smaller pieces in turn are broken down into even smaller, more manageable pieces to work on. As part of the breaking down process, we inherently have to introduce dependencies between the larger pieces and the smaller pieces.</p> <p>How we weave these dependencies together is the difference between easily changing behavior and spending the next week pulling your hair out.</p> <p>When working with classes, dependencies are usually introduced by constructing them in the class that they\u2019re used in. For example, let\u2019s say that we\u2019ve been asked to write an utility that emulates an calculator but it also keeps a transaction log for record keeping purposes.</p> <pre><code>class Logger\n  def log (content)\n    File.open(\"C:\\\\temp\\\\results.txt\", 'a') {|f| f.write(content)}\n  end\nend\n\nclass Calculator\n  def initialize\n      @logger = Logger.new()\n  end\n  def add (a, b)\n      log(a, b, \"+\")\n      return a + b\n  end\n  def sub (a, b)\n      log(a, b, \"-\")\n      return a - b\n  end\n  def mult (a, b)\n      log(a,b,\"*\")\n      return a * b\n  end\n  def div (a, b)\n      log(a,b,\"/\")\n      return a.to_f / b\n  end\n\n  def log(a, b, sym)\n      text = a.to_s + \" \" + sym + \" \" + b.to_s + \" = \"\n      if sym == \"+\"\n            text += (a + b).to_s\n      elsif sym == \"-\"\n            text += (a-b).to_s\n      elsif sym == \"*\"\n            text += (a*b).to_s\n      else\n            text += (a.to_f/b).to_s\n      end\n      text += \"\\n\"\n      @logger.log(text)\n  end\nend\n\n# Usage\ncalc = Calculator.new()\nputs calc.add(4,3)\nputs calc.sub(2,1)\nputs calc.mult(100,2)\nputs calc.div(5,2)\n</code></pre> <p>So far so good, we have two classes (Logger and Calculator) that is responsible for logging and the calculations. Even though this is a small amount of code (~50 lines for both classes), there are three dependencies in the code. The first dependency that I notice is that Calculator depends on Logger. We can see this by looking at the initialize method for Calculator (as hinted above):</p> <pre><code>class Calculator\n  def initialize\n    @logger = Logger.new(\"C:\\\\temp\\\\results.txt\")\n  end\nend\n</code></pre> <p>The second dependency is a bit trickier to find, but in the Logger class\u2019 log method, we use a hard coded file path.</p> <pre><code>class Logger\n  def log (content)\n      File.open(\"C:\\\\temp\\\\results.txt\", 'a') {|f| f.write(content)}\n  end\nend\n</code></pre> <p>The third dependency is probably the hardest to find, but in the the Logger class\u2019 log method, we are also depending on the file system for the machine by using Ruby\u2019s File class. But wait a minute, I hear you say, why is the File class considered a dependency, that\u2019s a part of the Ruby language? I agree with you, but it\u2019s still a dependency in the code and something that we should keep in mind.</p> <p>From these three dependencies, we\u2019re going to focus on resolving the first two. We could make resolve the dependency on the file system, but it would take so much effort for so little gain.</p>"},{"location":"articles/2014/01/07/beginner-basics-establishing-a-solid-foundation--the-dependency-inversion-principle/#why-dont-we-resolve-the-file-dependency-issue","title":"Why don\u2019t we resolve the file dependency issue?","text":"<p>One thing that I keep in mind when identifying which dependencies to invert is to focus on inverting dependencies outside of the framework. In this case, we\u2019re going to ignore the file system dependency because I, the programmer, depend on Ruby to behave correctly. If Ruby stops working, then a broken file system is the least of my worries. Therefore, it\u2019s not worth the effort to resolve.</p>"},{"location":"articles/2014/01/07/beginner-basics-establishing-a-solid-foundation--the-dependency-inversion-principle/#making-the-calculator-and-logger-more-flexible","title":"Making the Calculator and Logger more flexible","text":"<p>In order to resolve these DIP violations, we need to expose ways to drop in these dependencies. There are two ways of doing this. We can either:</p> <ul> <li>Expose the dependency via the constructor</li> <li>Expose the dependency via a public property</li> </ul> <p>Using the Calculator example, we can expose the logger dependency via the constructor and we can expose the filePath dependency by exposing it as a property.</p> <p>For the Calculator class, I\u2019m going to first change the initialize method so that it takes a logger instead of constructing it\u2019s own.</p> <pre><code>class Calculator\n  def initialize(logger)\n    @logger = logger\n  end\nend\n</code></pre> <p>Next, I will construct the logger in the usage and pass the logger as part of the constructor.</p> <pre><code># Usage\nlogger = Logger.new()\ncalc = Calculator.new(logger)\n</code></pre> <p>A quick run of our program tells us that everything is still working correctly.</p> <p>Now that we\u2019ve finished up exposing the logger dependency, it\u2019s time to expose the file path. First, I\u2019m going to add a public property on the Logger class call filePath and use that property in the log method</p> <pre><code>class Logger\n  def log (content)\n      File.open(filePath, 'a') {|f| f.write(content)}\n  end\n  attr_accessor :filePath\nend\n</code></pre> <p>Now that we\u2019ve introduced a seam for the file path dependency, we use that seam in the program and assign the property```ruby</p> <pre><code># Usage\nlogger = Logger.new()\nlogger.filePath = \"C:\\\\temp\\\\results.txt\"\ncalc = Calculator.new(logger)\n</code></pre>"},{"location":"articles/2014/01/07/beginner-basics-establishing-a-solid-foundation--the-dependency-inversion-principle/#multiple-ways-of-solving-the-issue-which-one-is-best","title":"Multiple ways of solving the issue, which one is best?","text":"<p>When using the constructor method, it\u2019s very clear to see what dependencies the class has, just look at the constructor. However, adding a new dependency to the constructor may cause other code to fail because the signature of the constructor has changed. This in turn can lead to cascading changes where multiple places of code need to be updated to pass in the dependency.</p> <p>On the other hand, using the property method, the change is less invasive because the property can be set independently of the when the object was constructed. However, it\u2019s harder to see the dependencies for the class because now the properties are containing the dependencies. Also, it\u2019s very easy to forget to set a property before using the object.</p> <p>Both of these methods are valid, but when I\u2019m working with DIP, I prefer to expose the dependencies via the constructor because if my class starts to gain more and more dependencies, then it\u2019s a sign that my class is doing too much and violating the Single Responsibility Principle (SRP). You can say that DIP is the canary in the coal mine for SRP.</p>"},{"location":"articles/2014/01/07/beginner-basics-establishing-a-solid-foundation--the-dependency-inversion-principle/#tldr","title":"TL;DR","text":"<p>In summary, the Dependency Inversion Principle (DIP) tells us that we should we should have the outside world pass in our dependencies. If we don\u2019t follow this rule, then we will not. In order to resolve violations, we need to determine what dependencies we have and modify our constructor to accept those dependencies. If our constructor becomes too large, then our class might be violating the Single Responsibility Principle. By following the DIP, we expose the dependencies our classes require and allows for greater decoupling. As always, don\u2019t forget to refactor as you go along.</p>"},{"location":"articles/2014/01/07/beginner-basics-establishing-a-solid-foundation--the-dependency-inversion-principle/#establishing-a-solid-foundation-series","title":"Establishing a SOLID Foundation Series","text":"<ul> <li>Introduction</li> <li>The Single Responsibility Principle (SRP)</li> <li>The Open/Closed Principle (OCP)</li> <li>The Liskov Substitution Principle (LSP)</li> <li>The Interface Segregation Principle (ISP)</li> <li>The Dependency Inversion Principle (DIP)</li> </ul>"},{"location":"articles/2013/12/12/beginner-basics-establishing-a-solid-foundation--the-interface-segregation-principle/","title":"Beginner Basics: Establishing a SOLID Foundation \u2013 The Interface Segregation Principle","text":"<p>Welcome to the fourth installment of Establishing a SOLID Foundation series. In this post, we\u2019ll be exploring the fourth part of SOLID, the Interface Segregation Principle and how by following this principle, you will write more robust code.</p>"},{"location":"articles/2013/12/12/beginner-basics-establishing-a-solid-foundation--the-interface-segregation-principle/#what-is-the-interface-segregation-principle","title":"What is the Interface Segregation Principle?","text":"<p>The Interface Segregation Principle (ISP) tell us that clients should not be forced to use an interface that defines methods that it does not use. But what does this mean? Let\u2019s say that we have the following ContactManager class and a ContactFinder class.</p> <pre><code>public class ContactManager\n{\n     private List _names;\n\n     public ContactManager()\n     {\n          _names = new List();\n          _names.Add(\"Cameron\");\n          _names.Add(\"Geoff\");\n          _names.Add(\"Phillip\");\n     }\n\n     public void PrintNames()\n     {\n          foreach (var name in _names)\n              Console.WriteLine(name);\n     }\n\n     public void SetNames(List names)\n     {\n          foreach (var name in names)\n               _names.Add(name);\n     }\n\n    public bool DoesNameExist(string name)\n    {\n         var results = _names.IndexOf(name);\n         if (results != -1)\n              return true;\n\n         return false;\n     }\n}\n\npublic class ContactFinder\n{\n     private ContactManager _manager;\n\n     public ContactFinder(ContactManager manager)\n     {\n         _manager = manager;\n     }\n\n     public void FindContacts(List names)\n     {\n          foreach (var name in names)\n          {\n               if (_manager.DoesNameExist(name))\n                    Console.WriteLine(\"Found \" + name);\n               else\n                    Console.WriteLine(\"Couldn't find \" + name);\n          }\n      }\n}\n</code></pre> <p>So far, so good, the ContactManager is responsible for holding onto the list of names and some basic methods and the ContactFinder is responsible for determining which contacts are in our list.</p> <p>However, there is a problem with this example code. We have this ContactManager class that has a lot methods defined, but the only method that\u2019s required is the DoesNameExist method.</p> <p>Another concept ISP tells in a roundabout fashion is that objects should depend on interfaces, not concrete classes. This allows us to switch out dependencies much easier when we code to the interface.</p>"},{"location":"articles/2013/12/12/beginner-basics-establishing-a-solid-foundation--the-interface-segregation-principle/#whats-the-big-deal-i-dont-see-what-the-issue-is","title":"What\u2019s the big deal, I don\u2019t see what the issue is","text":"<p>The big issue that comes up is that it\u2019s hard to figure out what methods that the ContactFinder actually needs. We say that it needs a ContactManager which would lead us to assume that it needs all of those methods. However, that\u2019s not the case. So by violating ISP, it\u2019s easy to make the wrong design decision.</p> <p>Another issue arises when it comes to creating the interface for the dependency. If we assume that all methods for the ContactManager is required, then any class that implements that interface has to also implement those unneeded methods. How many times have you seen an object implement an interface with a lot of methods that did nothing or just threw exceptions?</p> <pre><code>public interface BloatedInterface\n{\n     void SetContent();\n     bool IsContentSet();\n     void RemoveContent(string content);\n     void AddContent(string content);\n     void ImportantMethod();\n}\n\npublic class BloatedObject : BloatedInterface\n{\n     public void SetContent()\n     {\n     }\n     public bool IsContentSet()\n     {\n          throw new NotImplementedException();\n     }\n     public void RemoveContent(string content)\n     {\n          throw new NotImplementedException();\n     }\n     public void AddContent(string content)\n     {\n          throw new NotImplementedException();\n     }\n     public void ImportantMethod()\n     {\n     }\n}\n</code></pre>"},{"location":"articles/2013/12/12/beginner-basics-establishing-a-solid-foundation--the-interface-segregation-principle/#fixing-the-issue","title":"Fixing the issue","text":"<p>Alright, alright, I hear you say, you\u2019ve convinced me, how do I fix this problem? The steps are simple:</p> <ol> <li>First, you need to identify which methods the client needs</li> <li>Next, you need to create an interface that contains the methods that the client uses</li> <li>After creating the interface, have the dependency implement the interface</li> <li>Finally, change the signature client so that it uses the interface instead of the concrete class</li> </ol> <p>Using our code base, first, we look at the ContactFinder class and see what methods from ContactManager that it uses.</p> <p>So far, it looks like it only needs the DoesNameExist method. So let\u2019s create an interface, called IContactSearcher that contains the single method.</p> <pre><code>public interface IContactSearcher\n{\n     bool DoesNameExist(string name);\n}\n</code></pre> <p>Now that we\u2019ve extracted the interface, it\u2019s time to have the ContactManager class implement the interface:</p> <pre><code>public class ContactManager : IContactSearcher\n{\n     private List _names;\n\n     public ContactManager()\n     {\n          _names = new List();\n          _names.Add(\"Cameron\");\n          _names.Add(\"Geoff\");\n          _names.Add(\"Phillip\");\n     }\n\n     public void PrintNames()\n     {\n          foreach (var name in _names)\n          Console.WriteLine(name);\n     }\n\n     public void SetNames(List names)\n     {\n          foreach (var name in names)\n               _names.Add(name);\n     }\n\n     public bool DoesNameExist(string name)\n     {\n          var results = _names.IndexOf(name);\n          if (results != -1)\n               return true;\n\n          return false;\n     }\n}\n</code></pre> <p>Finally, we update the references in ContactFinder to use the IContactSearcher interface instead of the concrete class ContactManager.</p> <pre><code>public class ContactFinder\n{\n     private IContactSearcher _searcher;\n\n     public ContactFinder(IContactSearcher searcher)\n     {\n         _searcher = searcher;\n     }\n\n     public void FindContacts(List names)\n     {\n          foreach (var name in names)\n          {\n               if (_searcher.DoesNameExist(name))\n                    Console.WriteLine(\"Found \" + name);\n               else\n                    Console.WriteLine(\"Couldn't find \" + name);\n          }\n     }\n}\n</code></pre> <p>With this last step, we\u2019ve now resolved the ISP violation.</p>"},{"location":"articles/2013/12/12/beginner-basics-establishing-a-solid-foundation--the-interface-segregation-principle/#tldr","title":"TL;DR","text":"<p>In summary, the Interface Segregation Principle (ISP) tells us that we should interfaces instead of concrete classes for our dependencies and that we should use the smallest interface for our client to work. If we don\u2019t follow these rules, then it\u2019s easy to create bloated interfaces that clutter up readability. In order to resolve violations, we need to determine what methods our client require and code an interface to contains those methods. Finally, we have our class implement those methods and pass it to the client. By following the ISP, we reduce the complexity required by our code and reduce the coupling between client and dependency. As always, don\u2019t forget to refactor as you go along.</p>"},{"location":"articles/2013/12/12/beginner-basics-establishing-a-solid-foundation--the-interface-segregation-principle/#establishing-a-solid-foundation-series","title":"Establishing a SOLID Foundation Series","text":"<ul> <li>Introduction</li> <li>The Single Responsibility Principle (SRP)</li> <li>The Open/Closed Principle (OCP)</li> <li>The Liskov Substitution Principle (LSP)</li> <li>The Interface Segregation Principle (ISP)</li> <li>The Dependency Inversion Principle (DIP)</li> </ul>"},{"location":"articles/2013/11/04/establish-solid-intro/","title":"Establish solid intro","text":"<p>If you\u2019ve been working at any dev shop worth its salt, it\u2019s a safe bet that you\u2019ve heard someone mention writing SOLID code or that something isn\u2019t SOLID. Well, what exactly do they mean by SOLID?</p> <p>As part of this Beginner Basics series, we\u2019re going to first look at what does SOLID mean and why is it so important. For the next five weeks, we\u2019ll explore a different aspect of SOLID by in terms of which principle does each letter represent, some code samples that follow the principle and code samples that break the principle. . As this series progresses, I\u2019ll be adding the code samples to my public Bitbucket account for you to clone (You do know how to use Mercurial and Bitbucket, right?)</p>"},{"location":"articles/2013/11/04/establish-solid-intro/#what-is-solid","title":"What is SOLID?","text":"<p>In a nutshell, SOLID is a mnemonic created by Michael Feathers to help developers remember the five principles of great code construction introduced by Robert C. Martin (\u201cUncle Bob\u201d). By following these principles, it\u2019s much more likely that the code designed will be easier to maintain and to extend. As such, SOLID code follow these principles:</p> <p>(S)ingle Responsibly Principle \u2013 Every class should do just one thing and do it well (O)pen/Closed Principle \u2013 Code should be open for extension, but closed for modifications (L)iskov Substitution Principle \u2013 Make sure that two classes that are interchangeable have the same behavior (I)nterface Segregation Principle \u2013 Better to use multiple specific interfaces then to use a single general interface (D)ependency Inversion Principle \u2013 Code should depend on abstractions, not on concrete classes</p>"},{"location":"articles/2013/11/04/establish-solid-intro/#whats-so-important-about-being-solid","title":"What\u2019s so important about being SOLID?","text":"<p>When building software, its paramount that you have a solid foundation. A common analogy for building software is that it\u2019s like building a house. If the foundation is weak, then code starts becoming stiff, changes become much harder to make and the next thing you know the shower is draining into the kitchen. If your code isn\u2019t SOLID, then it\u2019s going to be harder to add new features/modifications, more difficult to support, and easier to introduce features bugs. As a developer, you should strive to write code that is bug free, easily maintainable, and simple to expand. By following SOLID principles, we can carry out these goals and our code can be SOLID.</p>"},{"location":"articles/2013/11/04/establish-solid-intro/#next-steps","title":"Next Steps","text":"<p>Stay tuned for the next part of the series when we begin exploring the first part of SOLID, the Single Responsibility Principle. Until then, stay sharp, keep learning, and code on.</p>"},{"location":"articles/2013/11/04/establish-solid-intro/#establishing-a-solid-foundation-series","title":"Establishing a SOLID Foundation Series","text":"<ul> <li>Introduction</li> <li>The Single Responsibility Principle (SRP)</li> <li>The Open/Closed Principle (OCP)</li> <li>The Liskov Substition Principle (LSP)</li> <li>The Interface Segregation Principle (ISP)</li> <li>The Dependency Inversion Principle (DIP)</li> </ul>"},{"location":"articles/2013/11/29/beginner-basics-establishing-a-solid-foundation--the-liskov-substitution-principle/","title":"Beginner Basics: Establishing a SOLID Foundation \u2013 The Liskov Substitution Principle","text":"<p>Welcome to the third installment of Establishing a SOLID Foundation series. In this post, we\u2019ll be exploring the third part of SOLID, the Liskov Substitution Principle and how following this principle can lead to loose coupling of your code.</p>"},{"location":"articles/2013/11/29/beginner-basics-establishing-a-solid-foundation--the-liskov-substitution-principle/#so-what-is-the-liskov-substitution-principle","title":"So what is the Liskov Substitution Principle?","text":"<p>Before diving into the Liskov Substitution Principle (LSP), let\u2019s look at a code example demonstrating the violation.</p> <p>Let\u2019s say that we have a Rectangle class:</p> <pre><code># A Rectangle can have height and width\n# set to any value\nclass Rectangle\n  def height=(height)\n      @height = height\n  end\n  def width=(width)\n      @width = width\n  end\n  def height\n      @height\n  end\n  def width\n      @width\n  end\n  def area\n      height * width\n  end\nend\n</code></pre> <p>If we run the following implementation, it\u2019s pretty clear that it works like we would expect:</p> <pre><code>rect = Rectangle.new\nrect.height = 5\nrect.height = 6\nputs rect.area # =&gt; 30\n</code></pre> <p>Seems pretty simple, we have a Rectangle class with two public properties, height and width and the class behaves the way we would expect.</p> <p>Now let\u2019s add a new class, called Square. Since all Squares are also Rectangles, it makes sense that the Square class should inherit from the Rectangle class. However, since Squares have to maintain the same height and width, we need to add some additional logic for that:</p> <pre><code># A Square must maintain the same height and width\nclass Square &lt; Rectangle\n  def height=(height)\n      @height = height\n      @width = height\n  end\n  def width=(width)\n      @width = width\n      @height = width\n  end\nend\n</code></pre> <p>Using a Square instead of a Rectangle and running the same input, we get the following output:</p> <pre><code>square = Square.new\nsquare.height = 10\nsquare.width = 5\nputs square.area # =&gt; 25\n</code></pre> <p>Hold up, why is the area 25? If I read this code, then the height should be 10 and the width should be 5, giving an area of 50. This is no longer the case because of the domain constraint caused by the Square class. By using a Square where the code expected a Rectangle, we get different behavior then we would expect. This is the heart of the Liskov Substitution Principle.</p> <p>In short, the Liskov Substitution Principle states that if we have an object (Rectangle) in our code and it works correctly, then we should be able to use any sub-type (Square) without the results being modified.</p> <p>The most common example of LSP violations are when the \u201cis-a\u201d phrase from Object-Oriented Design break down. In the Rectangle-Square example, we say that a Square \u201cis-a\u201d Rectangle which is true. However, when we covert that relationship to code and use inheritance, the relationship does not hold up.</p> <p>I don\u2019t know, this sounds confusing, what\u2019s the point? To me, the Liskov Substitution Principle is the hardest part of SOLID to understand. It\u2019s heavy on the theoretical and it\u2019s not blatantly obvious when a violation has occurred until testing.</p> <p>However, there are plenty of benefits of following LSP.</p> <p>First, following LSP reduces the tight coupling involved in your code. Let\u2019s look back at our Recipes class from the Open/Closed Principle post and examine the MakeOrder method:</p> <pre><code>class Recipes\n     def initialize\n          @recipes = {}\n          @recipes[RecipeNames::ChickenWithBroccoli] = ChickenWithBroccoli.new()\n          @recipes[RecipeNames::SteakWithPotatoes] = SteakWithPotatoes.new()\n          @recipes[RecipeNames::PorkWithApples] = PorkWithApples.new()\n     end\n     def MakeOrder(order)\n          recipe = @recipes[order]\n          if recipe == nil\n               puts \"Can't cook \" + order\n          else\n               recipe.Cook()\n          end\n     end\nend\n</code></pre> <p>In this class, you see that we load different recipes and when one\u2019s requested, we call the Cook method. We don\u2019t have to do any set-up, special handling, or other logic, we just trust that the Cook method for whatever recipe we choose works as expected. By following this design, code will be easier to read and to maintain.</p> <p>Going back to our Square/Rectangle example, if we wanted a method that would return a new Square or Rectangle, it would have to look something like this:</p> <pre><code>def CreateShape(classType, height, width)\n     shape = nil\n     if classType == \"Rectangle\"\n          shape = Rectangle.new\n          shape.height = height\n          shape.width = width\n     else\n          shape = Square.new\n          shape.height = height\n     end\n     return shape\nend\n</code></pre> <p>This code works, but there is one major problem. When someone is looking at this code, they\u2019re going to get confused of why the Rectangle and Square are setup differently.</p> <p>For example, when I see that the Square\u2019s height is being set, but not the width, my first thought is that this is a bug. Then, I\u2019d have to look into the Square\u2019s class definition and then I\u2019d see the logic of where setting the height also sets the width.</p> <p>Long story short, by identifying and resolving LSP violations, we can make the code easier to read and maintain.</p>"},{"location":"articles/2013/11/29/beginner-basics-establishing-a-solid-foundation--the-liskov-substitution-principle/#so-it-looks-like-lsp-is-pretty-useful-but-how-do-i-fix-violations","title":"So it looks like LSP is pretty useful, but how do I fix violations?","text":"<p>Now that we\u2019ve talked about spotting LSP violations and why it\u2019s important to follow LSP, let\u2019s discuss how to fix the violations.</p> <p>To be honest, fixing a LSP violation is not easy. Since the nature of the problem is caused by a broken abstraction, discarding the abstraction is the best option. However, if you absolutely need to use the abstraction, then one solution is to remove the method that causes the violation.</p> <p>In the Square/Rectangle example, we would remove the setters for height and width from our Rectangle class because that is how the violation can occur. After removing the setters, we need to modify the initialize method of Square to only take one parameter, size, and send that twice to the Rectangle class. Now, our classes look something like this:</p> <pre><code># A Rectangle can have height and width\n# set to any value\nclass Rectangle\n  def initialize(height, width)\n      @height = height\n      @width = width\n  end\n  def height\n      @height\n  end\n  def width\n      @width\n  end\n  def area\n      height * width\n  end\nend\n\n# A Square must maintain the same height and width\nclass Square &lt; Rectangle\n  def initialize(size)\n      super(size, size)\n  end\nend\n</code></pre> <p>With sample implementation and output</p> <pre><code>rect = Rectangle.new(10, 5)\nputs rect.area # =&gt; 50\n\nsquare = Square.new(5)\nputs square.area # =&gt; 25\n</code></pre>"},{"location":"articles/2013/11/29/beginner-basics-establishing-a-solid-foundation--the-liskov-substitution-principle/#tldr","title":"TL;DR","text":"<p>In short, the Liskov Substitution Principle (LSP) enforces the idea that if a class has a sub-type (through inheritance or interfaces), then by passing the sub-type, the program should still produce the same results. If you run across a class that violates LSP, then you know that your abstraction is not complete and you can either</p> <ul> <li>Remove the offending methods/properties or</li> <li>Abandon the abstraction</li> </ul> <p>As always, don\u2019t forget to refactor and reorganize your code as needed.</p>"},{"location":"articles/2013/11/29/beginner-basics-establishing-a-solid-foundation--the-liskov-substitution-principle/#establishing-a-solid-foundation-series","title":"Establishing a SOLID Foundation Series","text":"<ul> <li>Introduction</li> <li>The Single Responsibility Principle (SRP)</li> <li>The Open/Closed Principle (OCP)</li> <li>The Liskov Substitution Principle (LSP)</li> <li>The Interface Segregation Principle (ISP)</li> <li>The Dependency Inversion Principle (DIP)</li> </ul>"},{"location":"articles/2013/11/18/beginner-basics-establishing-a-solid-foundation---the-openclosed-principle/","title":"Beginner Basics: Establishing a SOLID Foundation - The Open/Closed Principle","text":"<p>Welcome to the second installment of Establishing a SOLID Foundation series. In this post, we\u2019ll be exploring the second part of SOLID, the Open/Closed Principle and how following this principle can lead to great design choices.</p> <p>So what is the Open/Closed Principle? In order to set the context for discussion, let\u2019s revisit our last example of the Chef class:</p> <pre><code>class Chef\n  def CookFood(order, tableNumber)\n    if order == \"chicken with broccoli\"\n      CookChickenWithBroccoli()\n    end\n  end\n\n  def CookChickenWithBroccoli\n    puts \"Cooked chicken with broccoli\"\n  end\nend\n</code></pre> <p>So it looks like this Chef is pretty simple, it only has one public method of CookFood and he can only cook ChickenWithBroccoli. However, a Chef that can only cook one thing isn\u2019t very useful. So how about we add some more menu items?</p> <pre><code>class Chef\n  def CookFood (order)\n    if order == \"chicken with broccoli\"\n      CookChickenWithBroccoli()\n    elsif order == \"steak with potatoes\"\n      CookSteakWithPotatoes()\n    elsif order == \"pork with apples\"\n      CookPorkWithApples()\n    else\n      puts \"Don't know how to cook \" + order\n    end\n  end\n\n  def CookChickenWithBroccoli\n    puts \"Cooked chicken with broccoli\"\n  end\n\n  def CookSteakWithPotatoes\n    puts \"Cooked steak with potatoes\"\n  end\n\n  def CookPorkWithApples\n    puts \"Cooked pork with apples\"\n  end\nend\n</code></pre> <p>So our new Chef can cook more food, but the code base expanded quite a bit. In order to add more menu choices, we need to add an additional if check in CookFood and to define a new method for CookFood to call. This might not sound like a lot of work because each of these Cook* methods just print something to the screen. However, what if the steps to create each food item were much more complex?</p> <pre><code>def CookChickenWithBroccoli\n  CookChicken()\n  CookBroccoli()\nend\n\ndef CookChicken\n  print \"Cooked chicken \"\nend\n\ndef CookBroccoli\n  puts \"with broccoli\"\nend\n</code></pre> <p>Also, what if we modified how the CookChickenWithBroccoli method worked? We would need to modify the Chef class, but that doesn\u2019t make sense. In the real world, we would modify the recipe and the Chef would then follow the new recipe. This concept that we would have to modify an unrelated object in order to add new functionality is the inspiration for the Open/Closed Principle.</p> <p>In short, the Open/Closed Principle means that an object should be Open for extension, but Closed for modification. This principle relies on the idea that new functionality is added by creating new classes, not by modifying pre-existing classes\u2019 behavior. By doing this, we\u2019re decreasing the chances of breaking current functionality and increasing code stability.</p>"},{"location":"articles/2013/11/18/beginner-basics-establishing-a-solid-foundation---the-openclosed-principle/#this-sounds-good-but-is-it-worth-the-additional-design-time","title":"This sounds good, but is it worth the additional design time?","text":"<p>Now that we\u2019ve discussed the Open/Closed Principle, you might be wondering what some of the benefits are of cleaning up this design.</p> <p>First, classes that follow the Open/Closed Principle are small and focused in nature playing off the idea of the Single Responsibility Principle. Looking back at our Chef class, it\u2019s very clear that by adding new functionality, Chef is going to be handling way too many things.</p> <p>Next, by following OCP, there won\u2019t be multiple classes modified just to add new functionality. There\u2019s nothing like a change set containing tons of modified files to make even the most experienced developer shudder in fear.</p> <p>By definition of OCP, we won\u2019t be modifying a lot of files (ideally only one file should be modified) and we\u2019re adding new classes. Since we\u2019re adding in these new classes, we inherently have the opportunity to bake in testing.</p>"},{"location":"articles/2013/11/18/beginner-basics-establishing-a-solid-foundation---the-openclosed-principle/#alright-i-get-it-ocp-is-awesome-but-how-do-i-refactor-the-chef-class","title":"Alright, I get it OCP is awesome, but how do I refactor the Chef class?","text":"<p>In order to fix the violation, we\u2019re going to take each menu item and make them into their own class</p> <pre><code>class ChickenWithBroccoli\n  def initialize\n    @name = \"Chicken with Broccoli\"\n  end\n\n  def Cook\n    CookChicken()\n    CookBroccoli()\n  end\n\n  def CookChicken\n    print \"Cooked chicken \"\n  end\n\n  def CookBroccoli\n    puts \"with broccoli\"\n  end\nend\n\nclass SteakWithPotatoes\n  def initialize\n    @name = \"Steak with Potatoes\"\n  end\n\n  def Cook\n    CookSteak()\n    CookPotatoes()\n  end\n\n  def CookSteak\n    print \"Cooked steak \"\n  end\n\n  def CookPotatoes\n    puts \"with potatoes\"\n  end\nend\n\nclass PorkWithApples\n  def initialize\n    @name = \"Pork with Apples\"\n  end\n\n  def Cook\n    CookPork()\n    CookApples()\n  end\n\n  def CookPork\n    print \"Cooked pork \"\n  end\n\n  def CookApples\n    puts \"with apples\"\n  end\n end\n</code></pre> <p>Now that we have these different classes, we need to come up with some way for our Chef to interact with them. So why don\u2019t we organize these menu items into a Recipes class?</p> <pre><code>class Recipes\n  def initialize\n      @recipes = {}\n      @recipes[:chicken] = ChickenWithBroccoli.new()\n      @recipes[:steak] = SteakWithPotatoes.new()\n      @recipes[:pork] = PorkWithApples.new()\n  end\n\n  def MakeOrder(order)\n      recipe = nil\n      if order == \"chicken with broccoli\"\n          recipe = @recipes[:chicken]\n      elsif order == \"steak with potatoes\"\n          recipe = @recipes[:steak]\n      elsif order == \"pork with apples\"\n          recipe = @recipes[:pork]\n      end\n      if recipe == nil\n          puts \"Can't cook \" + order\n      else\n          recipe.Cook()\n      end\n  end\nend\n</code></pre> <p>Now we have this Recipes class that contains all of the menu items for our Chef to use. When adding new menu items to Recipes, all we have to add is the class in the initialize method and add an additional if check in the MakeOrder method. But hold on, I hear you say, This is the same as what we had with the Chef at the beginning, why is this design better? Before, we would have to modify the Chef in order to add more menu items which doesn\u2019t really make sense, now we\u2019ve moved that logic to Recipes which makes sense that it needs to be modified if a new menu item is added.</p> <p>On the topic of our Chef, after cleaning up to use the Recipes class, our Chef is simpler and relies on Recipes for the menu items, not itself:</p> <pre><code>class Chef\n  def initialize\n    @recipes = Recipes.new()\n  end\n\n  def CookFood (order)\n    @recipes.MakeOrder(order)\n  end\nend\n</code></pre> <p>Now that we\u2019ve fixed the violation, let\u2019s go ahead and refactor some. Looking at the menu choices, it\u2019s pretty clear that we can abstract the behavior to a base class called MenuItem for them all to share (Note: By defining Cook by raising an exception, I\u2019m forcing all classes to provide their own implementation).</p> <pre><code>class MenuItem\n  def initialize(name)\n      @name = name\n  end\n\n  def Cook\n      raise \"This should be overridden in child class\"\n  end\nend\n</code></pre> <p>Also, as part of this refactoring, we\u2019re going to move some of the strings into constants as part of the RecipeNames module so that the Chef and Recipes can communicate with one another:</p> <pre><code>module RecipeNames\n  ChickenWithBroccoli = \"Chicken with Broccoli\"\n  SteakWithPotatoes = \"Steak with Potatoes\"\n  PorkWithApples = \"Pork with Apples\"\nend\n</code></pre> <p>With these additions, let\u2019s update the menu choices to use the module and the MenuItem base class:</p> <pre><code>class ChickenWithBroccoli &lt; MenuItem\n  def initialize\n      super(RecipeNames::ChickenWithBroccoli)\n  end\n\n  def Cook\n      CookChicken()\n      CookBroccoli()\n  end\n\n  def CookChicken\n      print \"Cooked chicken \"\n  end\n\n  def CookBroccoli\n      puts \"with broccoli\"\n  end\nend\n\nclass SteakWithPotatoes &lt; MenuItem\n  def initialize\n      super(RecipeNames::SteakWithPotatoes)\n  end\n\n  def Cook\n      CookSteak()\n      CookPotatoes()\n  end\n\n  def CookSteak\n      print \"Cooked steak \"\n  end\n\n  def CookPotatoes\n      puts \"with potatoes\"\n  end\nend\n\nclass PorkWithApples &lt; MenuItem\n  def initialize\n      super(RecipeNames::PorkWithApples)\n  end\n\n  def Cook\n      CookPork()\n      CookApples()\n  end\n\n  def CookPork\n      print \"Cooked pork \"\n  end\n\n  def CookApples\n      puts \"with apples\"\n  end\n end\n</code></pre> <p>With these changes, we need to update the Recipes class to use the RecipeNames module:</p> <pre><code>class Recipes\n  def initialize\n      @recipes = {}\n      @recipes[RecipeNames::ChickenWithBroccoli] = ChickenWithBroccoli.new()\n      @recipes[RecipeNames::SteakWithPotatoes] = SteakWithPotatoes.new()\n      @recipes[RecipeNames::PorkWithApples] = PorkWithApples.new()\n  end\n  def MakeOrder(order)\n      recipe = @recipes[order]\n      if recipe == nil\n          puts \"Can't cook \" + order\n      else\n          recipe.Cook()\n      end\n  end\nend\n</code></pre> <p>With this current layout, if we needed to add another menu item (let\u2019s say Fish and Chips), we would need to:</p> <ol> <li>Create a new class that extends MenuItem called FishAndChips</li> <li>Add another string constant to RecipeNames</li> <li>Add another line in the Recipes initialize method to add it to the array</li> </ol>"},{"location":"articles/2013/11/18/beginner-basics-establishing-a-solid-foundation---the-openclosed-principle/#tldr","title":"TL;DR","text":"<p>In short, the Open/Closed Principle (OCP) reinforces the idea that every class should be open for extension and closed to modifications. By following this principle, you\u2019re much more likely to create separated code that allows you to increase functionality and decrease the odds of breaking current functionality. If you run across a class that is doing way too much, use the Single Responsibility Principle to separate the classes and then use a new object that serves as the middle man. In our case, the Recipes class was the middle man between the Chef and the different menu items. As always, don\u2019t forget to refactor and reorganize your code as needed.</p>"},{"location":"articles/2013/11/18/beginner-basics-establishing-a-solid-foundation---the-openclosed-principle/#establishing-a-solid-foundation-series","title":"Establishing a SOLID Foundation Series","text":"<ul> <li>Introduction</li> <li>The Single Responsibility Principle (SRP)</li> <li>The Open/Closed Principle (OCP)</li> <li>The Liskov Substitution Principle (LSP)</li> <li>The Interface Segregation Principle (ISP)</li> <li>The Dependency Inversion Principle (DIP)</li> </ul>"},{"location":"articles/2013/11/11/beginner-basics-establishing-a-solid-foundation--the-single-responsibility-principle/","title":"Beginner Basics: Establishing a SOLID Foundation \u2013 The Single Responsibility Principle","text":"<p>Welcome to the first installment of Establishing a SOLID Foundation series. In this post, we\u2019ll be exploring the first part of SOLID, the Single Responsibility Principle and how following this principle can lead to great design choices.</p>"},{"location":"articles/2013/11/11/beginner-basics-establishing-a-solid-foundation--the-single-responsibility-principle/#so-what-is-the-single-responsibility-principle","title":"So what is the Single Responsibility Principle?","text":"<p>Before diving into code, let\u2019s take a look at a real life example. Let\u2019s say that we open a new restaurant. Clearly, we need to hire a fantastic head chef to prepare the food. Between these two candidates, which one seems to be the better fit?</p> <ul> <li>Chef A \u2013 spends their time on creating new dishes and preparing the best food possible</li> <li>Chef B \u2013 spends their time on taking orders, preparing food, and busing tables</li> </ul> <p>Well that\u2019s easy, you say, we should hire Chef A because he\u2019s focusing on cooking. We can hire other people to take orders and clean tables. It\u2019s pretty obvious that Chef A is the better choice because of the focusing on a single job. So if this is what happens in real life, why is that we see code that is doing way too many things?</p> <p>For example here\u2019s an example of what the Chef B(efore) class might look like</p> <pre><code>class Chef\n  def initialize\n    @position = 0\n    @order = nil\n    @orderReady = false\n  end\n\n  def CookFood(order, tableNumber)\n    if order == \"chicken with broccoli\"\n      CookChickenWithBroccoli()\n      DeliverFood(order, tableNumber)\n    end\n  end\n\n  def CookChickenWithBroccoli\n    @orderReady = true\n  end\n\n  def DeliverFood(order, tableNumber)\n    GoToTable(tableNumber)\n    GiveFood(order)\n  end\n\n  def GoToTable(tableNumber)\n    @position = tableNumber\n  end\n\n  def GiveFood(order)\n    puts \"Food delivered\"\n  end\n\n  def BusTables(tableNumber)\n    GoToTable(tableNumber);\n    CleanTable(tableNumber)\n  end\n\n  def CleanTable(tableNumber)\n    puts \"Table # \" + tableNumber.to_s + \" cleaned\"\n  end\n\n  def TakeOrders(tableNumber)\n    GoToTable(tableNumber)\n    order = AskForOrder()\n    return order\n  end\n\n  def AskForOrder()\n    return \"chicken with broccoli\"\n  end\nend\n</code></pre> <p>with an example implementation usage:</p> <p><pre><code>tableNumber = 3\nchef = Chef.new()\norder = chef.TakeOrders(tableNumber)\nchef.CookFood(order, tableNumber)\nchef.BusTables(tableNumber)\n</code></pre> As you can tell in the Chef example, there are a lot of methods that need to be defined in order to get the different pieces of main functionality working. If any of these main pieces needed to be changed, we would have to modify the Chef class.</p> <p>Because of the dependencies, another way to describe the SRP is that the class should only have one reason to change. In this case, the Chef class has three reasons for changing. (Fun fact: when dealing with classes that have a lot of reasons to change, it can be a sign that the class is following the God-Object anti-pattern.)</p>"},{"location":"articles/2013/11/11/beginner-basics-establishing-a-solid-foundation--the-single-responsibility-principle/#i-dont-know-whats-in-it-for-me","title":"I don\u2019t know, what\u2019s in it for me?","text":"<p>Now that you have a good understanding of the SRP, you might be asking what are some of the benefits of cleaning up your design.</p> <p>First, classes that only do one job have less dependencies to worry about. Looking back at our code example, it\u2019s clear that the Chef class would have to change if we needed to change the business rules for taking orders, cleaning tables, or for cooking food.</p> <p>Next, by following the SRP, it\u2019s easier for a team to solve issues. For example. let\u2019s say that you had to fix an error in cleaning the tables and someone else on your team was assigned to update the taking an order scenario. Using the Chef B class definition, the two of you would have to make different changes to the same class.</p> <p>Finally, by following the SRP, we\u2019re more closely following the idea behind object-oriented design. By definition, we should take complex problems and break them down into their individual actors. Since we define that each class can only have one responsibility, we are ensuring that the problem is being broken down to its smallest pieces.</p>"},{"location":"articles/2013/11/11/beginner-basics-establishing-a-solid-foundation--the-single-responsibility-principle/#ok-ok-youve-convinced-me-how-do-i-take-a-busy-class-and-make-it-simple","title":"Ok, ok, you\u2019ve convinced me, how do I take a busy class and make it simple?","text":"<p>Fortunately, if you have a class that has is doing too many things, there\u2019s a really simple fix. Just create more objects that contain the different pieces.</p> <p>Using our Chef example, I\u2019m going to separate the responsibilities into two new classes. First, I\u2019m going to move all the methods involved in taking orders to a new Waiter class.</p> <pre><code>class Waiter\n  def initialize\n    @position = 0\n  end\n\n  def DeliverFood(order, tableNumber)\n    GoToTable(tableNumber)\n    GiveFood(order)\n  end\n\n  def GoToTable(tableNumber)\n    @position = tableNumber\n  end\n\n  def GiveFood(order)\n    puts \"Food delivered\"\n  end\n\n  def TakeOrders(tableNumber)\n    GoToTable(tableNumber)\n    order = AskForOrder()\n    return order\n  end\n\n  def AskForOrder()\n    return \"chicken with broccoli\"\n  end\nend\n</code></pre> <p>Next, I\u2019m going to extract every method needed to bus tables into our new Busboy class:</p> <pre><code>class Busboy\n  def initialize\n    @position = 0\n  end\n\n  def BusTables(tableNumber)\n    GoToTable(tableNumber);\n    CleanTable(tableNumber)\n  end\n\n  def GoToTable(tableNumber)\n    @position = tableNumber\n  end\n\n  def CleanTable(tableNumber)\n    puts \"Table # \" + tableNumber.to_s + \" cleaned\"\n  end\nend\n</code></pre> <p>Now that we\u2019ve broken up the responsibilities, the next step is to look at some common functionality that classes might share. For example, it looks like the Waiter and Busboy class both use @position and the GoToTable method, so why don\u2019t we create a new class called BaseService</p> <pre><code>class BaseService\n  def initialize\n    @position = 0\n  end\n  def GoToTable(tableNumber)\n    @position = tableNumber\n  end\nend\n</code></pre> <p>and allow both the Waiter and Busboy classes to inherit?</p> <pre><code>class Waiter &lt; BaseService\n  def initialize\n    super\n  end\n\n  def DeliverFood(order, tableNumber)\n    GoToTable(tableNumber)\n    GiveFood(order)\n  end\n\n  def GiveFood(order)\n    puts \"Food delivered\"\n  end\n\n  def TakeOrders(tableNumber)\n    GoToTable(tableNumber)\n    order = AskForOrder()\n    return order\n  end\n\n  def AskForOrder()\n    return \"chicken with broccoli\"\n  end\nend\n\nclass Busboy &lt; BaseService\n  def initialize\n    super\n  end\n\n  def BusTables(tableNumber)\n    GoToTable(tableNumber);\n    CleanTable(tableNumber)\n  end\n\n  def CleanTable(tableNumber)\n    puts \"Table # \" + tableNumber.to_s + \" cleaned\"\n  end\nend\n</code></pre> <p>Great, we\u2019ve finished refactoring the overly-obsessive Chef from having control on everything in the restaurant to just keep his attention on cooking great food.</p> <pre><code>class Chef\n  def initialize\n    @orderReady = false\n  end\n\n  def CookFood(order, tableNumber)\n    if order == \"chicken with broccoli\"\n      CookChickenWithBroccoli()\n    end\n  end\n\n  def CookChickenWithBroccoli\n    @orderReady = true\n  end\nend\n</code></pre> <p>Wow, after separating the concerns, our chef class has been massively condensed down to focus on just cooking food, but how do all of these individual classes interact with one another? Does it look like the classes are now focusing on performing one job well?</p> <pre><code>tableNumber = 3\nchef = Chef.new()\nwaiter = Waiter.new()\nbusboy = Busboy.new()\norder = waiter.TakeOrders(tableNumber)\nchef.CookFood(order, tableNumber)\nwaiter.DeliverFood(order, tableNumber)\nbusboy.BusTables(tableNumber)\n</code></pre> <p>Something to keep in mind when separating responsibilities is that a single responsibility does not equal a single method. If the methods are all related to performing the same task, then it\u2019s not a violation of the SRP.</p>"},{"location":"articles/2013/11/11/beginner-basics-establishing-a-solid-foundation--the-single-responsibility-principle/#tldr","title":"TL;DR","text":"<p>In short, the Single Responsibility Principle (SRP) reinforces the idea that every class should have one job and should do that job well. By following this principle, you\u2019re much more likely to create more readable and maintainable code. When you run across classes that are doing too much, the best solution is to extract the extra functionality into another class. After extraction, don\u2019t forget to refactor and reorganize as needed.</p>"},{"location":"articles/2013/11/11/beginner-basics-establishing-a-solid-foundation--the-single-responsibility-principle/#establishing-a-solid-foundation-series","title":"Establishing a SOLID Foundation Series","text":"<ul> <li>Introduction</li> <li>The Single Responsibility Principle (SRP)</li> <li>The Open/Closed Principle (OCP)</li> <li>The Liskov Substitution Principle (LSP)</li> <li>The Interface Segregation Principle (ISP)</li> <li>The Dependency Inversion Principle (DIP)</li> </ul>"},{"location":"articles/2025/08/12/testing-non-deterministic-code---debugging-shuffle-with-property-based-testing/","title":"Testing Non Deterministic Code - Debugging Shuffle with Property Based Testing","text":""},{"location":"articles/2025/08/12/testing-non-deterministic-code---debugging-shuffle-with-property-based-testing/#background","title":"Background","text":"<p>Now that I've gotten a handle on my work situation, I've been focusing my time to complete work on my upcoming Functional Blackjack course. I've gotten the business rules implemented so I've been playing some games to make sure everything is working the way I'd expect.</p> <p>For example, here's what the UI looks like for when the game is dealt out.</p> <p></p> <p>So far, so good. However, every now then, I'd see error messages like this one where I'd be trying to calculate the score of a null card.</p> <p></p> <p>Doing some digging, I discovered that when I was creating my player, they could have a null card!</p> <p></p> <p>What could be happening? I'm leveraging functional programming techniques (immutable data structures, pure functions for business rules). Given that it doesn't happen all the time, it must be in some of my side-effect code. Combine that thought with the fact the game would break upon start, I had an idea of what might be the cause.</p>"},{"location":"articles/2025/08/12/testing-non-deterministic-code---debugging-shuffle-with-property-based-testing/#identifying-possible-problem","title":"Identifying Possible Problem","text":"<p>In order to create a player, we need to have an id (some arbitrary number) and two cards. These cards are coming from the deck, so let's take a deeper look at how that's being created.</p> <pre><code>// Deck type\nexport type Deck = {\n  tag: \"deck\";\n  value: Card[];\n};\n\nexport function create(): Deck {\n  const cards: Card[] = [];\n  for (const r of getRanks()) {\n    for (const s of getSuits()) {\n      // build up the card type\n      const card:Card = {rank:r, suit:s};\n      // add it to the list\n      cards.push(card);\n    }\n  }\n\n  // return the deck shuffled\n  return { tag: \"deck\", value: shuffle(cards) };\n}\n</code></pre> <p>So far, so good. the first section is building all possible combinations of <code>Rank</code>/<code>Suit</code> to model the deck. The only thing that is suspect is the <code>shuffle</code> function, so let's take a look at it.</p> <pre><code>// Implementation of Fisher-Yates algorithm (see https://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle#Modern_method)\nexport function shuffle&lt;T&gt;(ts: T[]): T[] {\n  const copy = [...ts];\n  for (let i = copy.length; i &gt; -1; i--) {\n    const j = Math.floor(Math.random() * i);\n    [copy[j], copy[i]] = [copy[i], copy[j]];\n  }\n  return copy;\n}\n</code></pre> <p>Hmm, I don't see anything obviously wrong, however, I've got a hunch this is the problem because it's leveraging <code>Math.random()</code> which would explain why sometimes it works and sometimes it doesn't. Since <code>Math.random()</code> looks at the system clock, this isn't a pure function, so <code>shuffle</code> can't be a pure function (which makes sense, if <code>shuffle</code> always gave the same output for the same input, then we'd clean house, right :) )</p> <p>My first instinct is to write some unit tests on this function. But how would you unit test <code>shuffle</code>? You could write a test case easily enough for an empty list and a list of one item, however, things get messy once you try to work around <code>Math.random</code>.</p> <p>Let's try a different approach, using a property testing approach.</p>"},{"location":"articles/2025/08/12/testing-non-deterministic-code---debugging-shuffle-with-property-based-testing/#verifying-the-problem-using-property-based-testing","title":"Verifying the Problem Using Property Based Testing","text":"<p>Instead of thinking in concrete terms (given this specific input, I should get this specific output), let's think about what properties a properly shuffled array would have.</p> <ol> <li>Should maintain length (i.e. if we shuffle an array of 10 elements, we should have 10 elements back)</li> <li>Should contain all original items (i.e. if we shuffle the numbers 1-10, then we shouldn't have an 11 or null in the array)</li> </ol>"},{"location":"articles/2025/08/12/testing-non-deterministic-code---debugging-shuffle-with-property-based-testing/#implementing-maintains-length-property","title":"Implementing Maintains Length Property","text":"<p>Using the fast-check library, we can start modeling the first property:</p> <pre><code>import fc from \"fast-check\"; // brining in the fast-check library\n\ndescribe(\"shuffle\", () =&gt; {\n    it(\"should maintain length\", () =&gt; {\n\n      // Fast Check assert\n      fc.assert(\n        // that for any array of any elements, \n        fc.property(fc.array(fc.anything()), (elements: any[]) =&gt; {\n          const result = shuffle(elements);\n\n          // the length of the shuffled result should be the same length as elements\n          expect(result.length).toBe(elements.length);\n        })\n      );\n    });\n});\n</code></pre> <p>Seems simple enough, let's see what we get:</p> <p></p> <p>Whelp, that was easy enough, let's tweak our shuffle function to make that property pass:</p> <pre><code>export function shuffle&lt;T&gt;(ts: T[]): T[] {\n  const copy = [...ts];\n  // note that we tweaked i to start from length-1 instead of length\n  for (let i = copy.length - 1; i &gt; -1; i--) {\n    const j = Math.floor(Math.random() * i);\n    [copy[j], copy[i]] = [copy[i], copy[j]];\n  }\n  return copy;\n}\n</code></pre> <p>Running the test suite again, we get a passing run, hooray!</p>"},{"location":"articles/2025/08/12/testing-non-deterministic-code---debugging-shuffle-with-property-based-testing/#implementing-must-have-all-original-items-property","title":"Implementing Must Have All Original Items Property","text":"<p>With the first property written up, let's take a shot at writing the second property. For this test, we need to keep track of the original elements before they were shuffled up. </p> <pre><code>it(\"should have all original elements\", () =&gt; {\n  fc.assert(\n    fc.property(fc.array(fc.anything()), (elements: any[]) =&gt; {\n      // keeping the items in array called original\n      const original = [...elements];\n\n      const result = shuffle(elements);\n    })\n  );\n});\n</code></pre> <p>Next, we need to go through each element of <code>result</code> and remove it from the <code>original</code> array. If we find an item that can't be removed, we should fail the test. Otherwise, we can remove it from <code>original</code>.</p> <pre><code>it(\"should have all original elements\", () =&gt; {\n  fc.assert(\n    fc.property(fc.array(fc.anything()), (elements: any[]) =&gt; {\n      const original = [...elements];\n\n      const result = shuffle(elements);\n\n      for (const x of result) {\n        const foundIndex = original.findIndex((y) =&gt; y === x);\n        // If we found the item\n        if (foundIndex !== -1) {\n          // let's remove it\n          original.splice(foundIndex, 1);\n        } else {\n          // otherwise we found an item that shouldn't have been there, \n          // so let's log the failure and fail the test\n          console.log(\"couldn't find \", x, \"in original list\");\n          expect(true).toBeFalsy();\n        }\n      }\n    })\n  );\n});\n</code></pre> <p>Note: Astute readers might have noticed we're not asserting that the original array is empty after the for loop. Since we have the test that verifies that we're maintaining length, that check is not needed here.</p> <p>Running the test suite, it looks like everything is passing. So to make sure that our test would catch issues, let's inject a bug into the shuffle algorithm!</p> <pre><code>export function shuffle&lt;T&gt;(ts: T[]): T[] {\n  const copy = [...ts];\n  for (let i = copy.length - 1; i &gt; -1; i--) {\n    const k = Math.floor(Math.random() * i);\n    [copy[k], copy[i]] = [copy[i], copy[k]];\n  }\n  // casting 2 as any makes TypeScript cool with this code\n  copy.push(2 as any);\n  return copy;\n}\n</code></pre> <p>If we run the test now, we get the following:</p> <p></p> <p>Based on the tests and the code change that we made, I'm feeling confident that my bug was found and has been solved.</p>"},{"location":"articles/2025/08/12/testing-non-deterministic-code---debugging-shuffle-with-property-based-testing/#wrapping-up","title":"Wrapping Up","text":"<p>In this post, we looked at an issue I was dealing with in my codebase (a mysterious introduction of null). From there, we were able to narrow down to a possible culprit, the shuffle function as it's not a pure function. Using fast check and property testing principals we were able to derive two properties that shuffle should follow and then encode that logic.</p> <p>The next time you find yourself needing to write tests and are struggling with good inputs, try thinking about properties instead!</p>"},{"location":"articles/2023/06/13/five-minutes-at-five-guys---when-metrics-conflict-with-ux/","title":"Five Minutes at Five Guys - When Metrics Conflict with UX","text":"<p>In a recent post, I spoke about the flaw of using a single metric to tell the story and how Goodhart's Law tells us that once we start measuring a metric, it stops being a useful metric.</p> <p>Let's look at a real-world example with the popular fast food chain, Five Guys.</p>"},{"location":"articles/2023/06/13/five-minutes-at-five-guys---when-metrics-conflict-with-ux/#all-i-wanted-was-a-burger","title":"All I Wanted Was a Burger","text":"<p>Five Guys is known for making good burgers and delivering a mountain of piping hot fries as part of your order. Seriously, an order of small fries is a mountain of spuds. Five Guys make their fries to order, so they're not sitting around under a heat lamp.</p> <p></p> <p>This approach works great when ordering in person, but what happens if you order online? The process is essentially the same, the crew works on the burgers, but they won't start the fries until you're at the restaurant, so they're always guaranteeing that you get fresh made fries.</p> <p>At this point, it's clear that receiving a mountain of hot, cooked-to-order fries is part of the experience and what customers expect, right?</p>"},{"location":"articles/2023/06/13/five-minutes-at-five-guys---when-metrics-conflict-with-ux/#in-the-name-of-progress","title":"In the Name Of Progress","text":"<p>I recently ordered Five Guys online, and after placing the order, I got a link to a new feature, the ability to let the restaurant know when you're on the way over.</p> <p>Oh, that's cool. I bet it gives the kitchen a better time of when to start cooking the burger, so that it's not sitting around.</p> <p>From a process perspective, I see this making sense as Lean Manufacturing tells us to minimize inventory (in this case, the amount of food waiting to be picked up) and it helps prioritize the kitchen.</p> <p>Curious, I click the button to let them know I'm on the way and start heading over there.</p>"},{"location":"articles/2023/06/13/five-minutes-at-five-guys---when-metrics-conflict-with-ux/#crash-course-in-restaurant-metrics","title":"Crash Course In Restaurant Metrics","text":"<p>After arriving, I notice that the interface has changed, and I now have an option of letting them know I'm there.</p> <p>Hmmm, interesting. I'm not sure what this does, but I'm here, so I'll click it.</p> <p>I click the button and start heading inside. And this is where the story gets interesting.</p> <p>It's a slow day, so I'm talking to one of the cooks as the fries are being made and we start chatting about the new system.</p> <p>For those who've never worked fast food you might be surprised to know that employees and managers are graded on various metrics. For example, how long does it take an order to get made? Or how long is someone waiting in the drive-through?</p> <p>For Five Guys, they have a metric for online orders and time. In this case, a customer should not wait more than five minutes from when they arrive to when they have received their food.</p> <p>As a customer, I like the sound of this as it means I'm not waiting around all day, and the food will be hot, though I have no clue how realistic this metric is.</p> <p>But here's the question - how do you determine someone's arrival time? For in-person orders, this is easy as it's the time the order is placed. But what about online pick-up orders?</p> <p>It turns out that once you click the I'm Here button, that starts the timer. And herein lies the issue. There's nothing in the interface that tells me that I should click on that button once I'm inside, only when I'm here.</p> <p>So when I clicked the button while sitting in my car, it put the kitchen in a weird spot because they don't see anyone there to pick up the food. Should they start cooking the fries in the hopes that I show up? Or should they wait until I'm physically there? What if I had clicked the button by mistake? What if there was a software bug?</p>"},{"location":"articles/2023/06/13/five-minutes-at-five-guys---when-metrics-conflict-with-ux/#when-metrics-conflict-with-user-experience-which-should-win","title":"When Metrics Conflict With User Experience, Which Should Win?","text":"<p>There's no good answer because there's waste in this process, whether it's product (e.g., cold fries that would need to be replaced) or time (e.g., me waiting or the kitchen cooking again).</p> <p>There's another motivation at stake here as well, bonuses. Remember the metrics that I mentioned before? Managers (and sometimes employees) receive bonuses for achieving performance goals. But what happens when the company has implemented a vague process (like the above)? It rewards the wrong behavior (getting the food out) instead of keeping the experience good (receiving hot fries).</p> <p>Here's one more wrinkle. Now that I know the button controls when the fries start cooking, why shouldn't I hit the button when I'm five minutes away? That way when, I show up, the fries are done, and I have zero minutes of wait. This, in turn now changes consumer behavior, which feeds back into the process.</p>"},{"location":"articles/2023/06/13/five-minutes-at-five-guys---when-metrics-conflict-with-ux/#revisiting-the-feature","title":"Revisiting the Feature","text":"<p>Let's revisit this new piece of functionality, letting the kitchen know when you're on the way and when you arrive. Is it helpful? Did it make my experience any better? Am I more likely to order from there again?</p> <p>Honestly, not really. I already enjoy the food, but I don't think letting the kitchen know when I'm on the way helps them very much (remember, I already get an estimated time from when I placed the order). The ability to let them know when I'm here doesn't make the process any faster for me, and it causes confusion for the kitchen.</p> <p>For those who have social anxiety or other issues speaking with others, I could see how something like this might be helpful. We could change the approach by adding a kiosk in the restaurant where they could enter their order number or email address which lets the kitchen know that they're here to pick up their food.</p>"},{"location":"articles/2023/06/13/five-minutes-at-five-guys---when-metrics-conflict-with-ux/#wrapping-up","title":"Wrapping Up","text":"<p>So what's the lesson learned here? It's reasonable to use metrics to give you a general idea of your business and process. However, when we start implementing processes to improve metrics, we need to keep in mind the customer experience and really ask Does this truly help our customers? Or does this just make the numbers happy?</p>"},{"location":"articles/2024/05/30/functional-foundations---functions/","title":"Functional Foundations - Functions","text":"<p>When leveraging functional programming, you're not going to go far without functions. It's literally in the name.</p> <p>In this post, let's take a deeper look at what functions are and some of the benefits we gain.</p>"},{"location":"articles/2024/05/30/functional-foundations---functions/#what-is-a-function","title":"What is a Function?","text":"<p>When we talk about functions, we're not talking about a programming construct (like the <code>function</code> keyword), but instead we're talking about functions from mathematics.</p> <p>As such, a function is a mapping from two sets such that for every element of the first set, it maps to a single element in the second set.</p> <p>Words are cool, but pictures are better. So let's look at the mapping for the <code>square</code> function.</p> <p></p> <p>In this example, we have an arrow coming from an element on the left where it maps to an element on the right. To read this image, we have a mapping called Square that maps all possible numbers to the set of positive numbers. So -3 maps to 9 (-3-3), 2 maps to 4 (22), so on and so forth.</p> <p>To check if our mapping is a function, we need to check that every element on the left is mapped to a single element on the right. If so, then we've got a function!</p> <p>Sounds easy, right? Let's take a look at a mapping that isn't a function.</p>"},{"location":"articles/2024/05/30/functional-foundations---functions/#love-in-the-air","title":"Love in the Air?","text":"<p>When working with dates, it's common to figure out how many days are in the month. Not only does this help with billable days, but it also makes sure that we don't try to send invoices on May 32nd.</p> <p>So let's take a look at a mapping from month to the number of days it has.</p> <p></p> <p>Looking at the mapping, we can tell that January, March, May map to 31, April and June both map to 30. But take a look at February. It's got two arrows coming out of it, one to 28 and the other to 29. Because there are two arrows coming out, this mapping isn't a function. Let's try to implement this mapping in TypeScript.</p> <pre><code>type Month = \"Jan\" | \"Feb\" | \"Mar\" | \"Apr\"\n           | \"May\" | \"Jun\" | \"Jul\" | \"Aug\"\n           |\"Sept\" | \"Oct\" | \"Nov\" | \"Dec\";\n\ntype DaysInMonth = 28 | 29 | 30 | 31;\n\nfunction getDaysInMonth(month: Month): DaysInMonth {\n  switch (month) {\n    case \"Jan\":\n    case \"Mar\":\n    case \"May\":\n    case \"Jul\":\n    case \"Oct\":\n    case \"Dec\":\n      return 31;\n\n    case \"Feb\":\n      // what should this be?\n\n    case \"Apr\":\n    case \"Jun\":\n    case \"Aug\":\n    case \"Sept\":\n    case \"Nov\":\n      return 30;\n  }\n}\n</code></pre> <p>We can't return 28 all the time (we'd be wrong 25% of the time) and we can't return 29 all the time (as we'd be wrong 75% of the time). So how do we know? We need to know something about the year. One approach would be to check if the current year is a leap year (algorithm).</p> <pre><code>function isLeapYear(): boolean {\n  const year = new Date().getFullYear();\n  if (year % 400 === 0) return true;\n  if (year % 100 === 0) return false;\n  if (year % 4 === 0) return true;\n  return false;\n}\n\n// Updated switch\ncase 'Feb':\n  return isLeapYear() ? 29 : 28;\n</code></pre> <p>The problem with this approach is that the determination of what to return isn't from the function's inputs, but outside state (in this case, time). So while this \"works\", you can get bit when you have tests that start failing when the calendar flips over because it assumed that February always had 28 days.</p> <p>If we look at the type signature of <code>isLeapYear</code>, we can see that it takes in no inputs, but returns a boolean. How can that be possible except if it always returned a constant value? This is a clue that <code>isLeapYear</code> is not a function.</p> <p>The better approach is to change our mapping to instead of taking just a month name, it takes two arguments, a <code>monthName</code> and <code>year</code>.</p> <p></p> <p>With this new mapping, our implementation would look like the following:</p> <pre><code>function isLeapYear(year:number): boolean {\n  if (year % 400 === 0) return true;\n  if (year % 100 === 0) return false;\n  if (year % 4 === 0) return true;\n  return false;\n}\n\nfunction getDaysInMonth(month: Month, year:number): DaysInMonth {\n  const isLeap = isLeapYear(year);\n  switch (month) {\n    case \"Jan\":\n    case \"Mar\":\n    case \"May\":\n    case \"Jul\":\n    case \"Oct\":\n    case \"Dec\":\n      return 31;\n\n    case \"Feb\":\n      return isLeap ? 29 : 28\n\n    case \"Apr\":\n    case \"Jun\":\n    case \"Aug\":\n    case \"Sept\":\n    case \"Nov\":\n      return 30;\n  }\n}\n</code></pre>"},{"location":"articles/2024/05/30/functional-foundations---functions/#benefits-of-functions","title":"Benefits of Functions","text":"<p>Now that we've covered what functions are and aren't, let's cover some of the reasons why we prefer functions for our logic.</p> <p>First, mappings help us make sure that we've covered all our bases. We saw in the <code>getDaysInMonth</code> function we found a bug for when the month was February. Mappings can also be great conversation tools with non-engineers as they're intuitive to understand and to explain.</p> <p>Second, functions are simple to test. Since the result is based solely on inputs, they are great candidates for unit testing and require little to no mocking to write them. I don't know about you, but I like simple test cases that help us build confidence that our application is working as intended.</p> <p>Third, we can combine functions to make bigger functions using composition. At a high level, composition says that if we have two functions <code>f</code> and <code>g</code>, we can write a new function, <code>h</code> which takes the output of <code>f</code> and feeds it as the input for <code>g</code>.</p> <p>Sounds theoretical, but let's take a look at a real example.</p> <p>In the Mars Rover kata, we end up building a basic console application that takes the input from the user (a string) and will need to convert it to the action that the rover takes.</p> <p>In code, the logic looks like the following:</p> <pre><code>let rover:Rover = {x:0, y:0, direction:'North'};\nconst action = input.split('').map(convertStringToCommand).map(convertCommandToAction);\nrover = action(rover);\n</code></pre> <p>The annoying part is that we're iterating the list twice (once for each <code>map</code> call), and it'd be nice to get it down to a single iteration. This is where composition helps.</p> <p>When we're running the <code>map</code>s back-to-back, we're accomplish the following workflow</p> <p></p> <p>Because each mapping is a function, we can compose the two into a new function, <code>stringToActionConverter</code>.</p> <pre><code>// using our f and g naming from earlier, convertString is f, convertCommand is g\nconst stringToActionConverter = (s:string)=&gt;convertCommandToAction(convertStringToCommand(s));\n\nlet rover = {x:0, y:0, direction:'North'}\nconst action = input.split('').map(stringToActionConverter);\nrover = action(rover);\n</code></pre>"},{"location":"articles/2024/05/30/functional-foundations---functions/#why-not-function-all-the-things","title":"Why Not Function All the Things?","text":"<p>Functions can greatly simplify our mental model as we don't have to keep track of state or other side effects. However, our applications typically deal with side affects (getting input from users, reading from files, interacting with databases) in order to do something useful. Because of this limitation, we strive to put all of our business rules into functions and keep the parts that interact with state as dumb as possible (that way we don't have to troubleshoot as much).</p> <p>What I've found is that when working with applications, you end up with a workflow where you have input come in, gets processed, and then the result gets outputted.</p> <p>Here's what an example workflow would look like</p> <pre><code>// Logic to determine the 'FizzBuzziness' of a number\nfunction determineFizzBuzz(input:number): string {\n  if (input % 15 === 0) return 'FizzBuzz';\n  if (input % 3 === 0) return 'Fizz';\n  if (input % 5 === 0) return 'Buzz';\n  return `${input}`;\n}\n\nfunction workflow(): void {\n  // Input Boundary\n  var prompt = require('prompt-sync')();\n  const input = prompt();\n\n  // Business Rules\n  const result = (+input) ? `${input} FizzBuzz value is ${determineFizzBuzz(+input)}` : `Invalid input`;\n\n  // Output boundary\n  console.log(result);\n}\n</code></pre>"},{"location":"articles/2024/05/30/functional-foundations---functions/#whats-next","title":"What's Next?","text":"<p>Now that we have a rough understanding of functions, we can start exploring what happens when things go wrong. For example, could there have been a cleaner way of implementing the business rules of our workflow?</p>"},{"location":"articles/2015/10/20/using-f-to-solve-a-constraints-problem/","title":"Using F# To Solve a Constraints Problem","text":"<p>In this post, I\u2019m going to solve a logic puzzle using C# and F#. First, I\u2019ll define the problem being solved and what our restrictions are. Next, I\u2019ll show how I\u2019d break down the problem and write an easy-to-read, extendable solution using idiomatic C#. Afterwards, I\u2019ll solve the same problem and write an easy-to-read, extendable solution writing in idiomatic F#. Finally, we\u2019ll compare the two solutions and see why the F# solution is the better solution.</p>"},{"location":"articles/2015/10/20/using-f-to-solve-a-constraints-problem/#the-problem","title":"The Problem","text":"<p>For this problem, I\u2019m going to write a constraint solver (thanks to Geoff Mazeroff for the inspiration).</p> <p>If you\u2019re not familiar with the concept, a constraint is simply some rule that must be followed (such as all numbers must start with a 4). So a constraint solver is something that takes all the constraints and a source of inputs and returns all values that fit all the constraints.</p> <p>With that being said, our source will be a list of positive integers and our constraints are the following:</p> <ul> <li>4 digits long (so 1000 \u2013 9999)</li> <li>Must be even (so 1000, 1002, 1004, etc\u2026)</li> <li>The first digit must match the last digit (2002, 2012, 2022, etc\u2026)</li> </ul> <p>To further restrict solutions, all code will be production ready. This includes handling error conditions (like input being null), being maintainable (easily adding more constraints) and easy to read.</p> <p>To quickly summarize, we need to find a robust, maintainable, and readable solution to help us find all 4 digit number that are even and that the first and last digit are equal.</p>"},{"location":"articles/2015/10/20/using-f-to-solve-a-constraints-problem/#implementing-a-solution-in-c","title":"Implementing a Solution in C","text":"<p>For the C# solution, I\u2019m going to need a class for every constraint, a class to execute all constraints against a source (positive integers) and a runner that ties all the pieces together.</p> <p>Starting with the smaller blocks and building up, I\u2019m going to start with the constraint classes. Each constraint is going to take a single number and will return true if the number follows the constraint, false otherwise.</p> <p>With that being said, I\u2019d first implement the constraint that all numbers must be 4 digits long</p> <pre><code>class MustBeFourDigitsLongConstraint\n{\n    public bool FollowsConstraint(int value)\n    {\n        return value.ToString().Length == 4;\n    }\n}\n</code></pre> <p>Second, I\u2019d write the constraint that all numbers must be even</p> <pre><code>class MustBeEvenConstraint\n{\n    public bool FollowsConstraint(int value)\n    {\n        return value % 2 == 0;\n    }\n}\n</code></pre> <p>Third, I\u2019d implement the constraint that all numbers must have the same first digit and the last digit</p> <pre><code>class FirstDigitMustEqualLastDigitConstraint\n{\n    public bool FollowsConstraint(int value)\n    {\n        var valueString = value.ToString();\n        return valueString[0] == valueString[valueString.Length-1];\n    }\n}\n</code></pre> <p>At this point, I have the constraints written, but I need them to follow a general contract so that the Constraint Solver (about to be defined) can take a list of these constraints. I\u2019ll introduce an interface, IConstraint and update my classes to implement that interface.</p> <pre><code>public interface IConstraint\n{\n    bool FollowsConstraint(int value);\n}\nclass MustBeFourDigitsLongConstraint : IConstraint {/* Implementation Details Omitted */}\n\nclass MustBeEvenConstraint : IConstraint {/* Implementation Details Omitted */}\n\nclass FirstDigitMustEqualLastDigitConstraint : IConstraint {/* Implementation Details Omitted */}\n</code></pre> <p>So now I have the constraints defined and they\u2019re now implementing a uniform interface, I can now create the constraint solver. This class is responsible for taking the list of numbers and the list of constraints and then returning a list of numbers that follow all constraints.</p> <pre><code>class ConstraintSolver\n{\n    public List FindValues(List constraints, List values)\n    {\n        if (constraints == null) throw new ArgumentNullException(\"constraints\");\n        if (values == null) throw new ArgumentNullException(\"values\");\n\n        var result = values;\n        foreach (var constraint in constraints)\n        {\n            result = result.Where(x =&gt; constraint.FollowsConstraint(x)).ToList();\n        }\n        return result;\n    }\n}\n</code></pre> <p>Finally, I can put all the pieces together using LINQPad (Full C# solution can be found here).</p> <pre><code>void Main()\n{\n    var numbers = Enumerable.Range(0, 10000).ToList();\n    var constraints = new List&lt;IConstraint&gt;{new MustBeFourDigitsLongConstraint(), new MustBeEvenConstraint(), \n             new FirstDigitMustEqualLastDigitConstraint()};\n\n    var constraintSolver = new ConstraintSolver();\n    var result = constraintSolver.FindValues(constraints, numbers.ToList());\n\n    result.Dump();\n}\n</code></pre> <p>This solution is easily extendable because if we need to add another constraint, we just add another class that implements the IConstraint interface and change the Main method to add an instance of the new constraint to the list of constraints.</p>"},{"location":"articles/2015/10/20/using-f-to-solve-a-constraints-problem/#implementing-a-solution-in-f","title":"Implementing a Solution in F","text":"<p>Now that we have the C# solution, let\u2019s take a look at how I would solve the problem using F#.</p> <p>Similar to the C# solution, I\u2019m going to create a function for every constraint, a function to execute all constraints against a source (positive integers) and a runner that ties all the pieces together.</p> <p>Also similar to the C# solution, I\u2019m going to start with creating the constraints and then work on the constraint solver function.</p> <p>First, I\u2019d implement that the number must be four digits long constraint.</p> <pre><code>let mustBeFourDigit number = \n    number.ToString().Length = 4\n</code></pre> <p>Next, the number must be even constraint.</p> <pre><code>let mustBeEven number =\n    number % 2 = 0\n</code></pre> <p>Lastly, the first digit is the same as the last digit constraint.</p> <pre><code>let firstDigitMustBeEqualLast number =\n    let numberString = number.ToString().ToCharArray()\n    let firstDigit = numberString.GetValue(0)\n    let lastDigit = numberString.GetValue(numberString.Length-1)\n    firstDigit = lastDigit\n</code></pre> <p>At this stage in the C# solution, I had to create an interface, IConstraint, so that the constraint solver could take a list of constraints. What\u2019s cool with F# is that I don\u2019t have to define the interface. The F# type inference is saying that each of these functions are taking the same input (some generic `a) and returning a bool, so I can add all of them to the list. This is pretty convenient since I don\u2019t have to worry about this piece of plumbing.</p> <p>Now that the different constraints are defined, I\u2019d go ahead and write the last function that takes a list of constraints and a list of numbers and returns the numbers that the constraints fit. (Confused by this function? Take a look at Implementing your own version of # List.Filter)</p> <pre><code>let rec findValidNumbers numbers constraints = \n    match constraints with\n    | [] -&gt; numbers\n    | firstConstraint::remainingConstraints -&gt;\n        let validNumbers = numbers |&gt; List.filter firstConstraint\n        findValidNumbers validNumbers remainingConstraints\n</code></pre> <p>Finally, all the pieces are in place, so I can now put all the pieces together using LINQPad.</p> <pre><code>let numbers = [1000 .. 9999]\nlet constraints = [mustBeFourDigits; mustBeEven; firstDigitMustEqualLast;]\n\nlet result = findValidNumbers numbers constraints\n\nprintf \"%A\" result\n</code></pre>"},{"location":"articles/2015/10/20/using-f-to-solve-a-constraints-problem/#comparing-both-solutions","title":"Comparing Both Solutions","text":"<p>Now that we have both solutions written up, let\u2019s compare and see which solution is better.</p> <p>First, the same design was used for both solutions. I decided to use this design for both because it\u2019s flexible enough that we could add new constraints if needed (such as, the 2nd digit must be odd). As an example, for the C# solution, I\u2019d create a new class that implemented IConstraint and then update the runner to use the new class. For the F# solution, I\u2019d create a new function and update the runner. So, I\u2019d think it\u2019s safe to say that both solutions score about the same from a maintainability and extendability point of view.</p> <p>From an implementation perspective, both solutions are production ready since the code handles possible error conditions (C# with null checks in the ConstraintSolver class, F# with none because it doesn\u2019t support null). In addition to being robust, both solutions are readable by using ample whitespace and having all variables, classes, and interfaces clearly described.</p> <p>With that being said, this is where the similarities end. When we look at how much code was written to solve the problem, we have a stark difference. For the C# solution, I ended up with 48 lines of code (omitting blank lines), however, for the F# solution, it only took 19. Now you could argue that I could have written the C# solution in fewer lines of code by removing curly braces around one line statements or ignoring null inputs. However, this would lead the code to be less robust.</p> <p>Another difference between the F# solution and C# is that I was able to focus on the solution without having to wire up an interface. You\u2019ll often hear developers talk about the how little plumbing you need for F# to \u201cjust work\u201d and this small example demonstrates that point.</p> <p>Another difference (albeit subtle) is that the F# solution is that I can use the findValidNumbers function with any generic list of values and any generic list of functions that take the generic type and return true/false.</p> <p>In other words, if I had another constraint problem using strings, I\u2019d still implement the different constraints, but I could use the same findValidNumbers function. At that point, however, I\u2019d probably rename it to findValidValues to signify the generic solution.</p> <p>What\u2019s awesome about this is that I didn\u2019t have to do any more work to have a generic solution, F# did that for me. To be fair, the C# solution can easily be made generic, but that would have to be a conscious design decision and I think that\u2019s a downside.</p>"},{"location":"articles/2015/10/20/using-f-to-solve-a-constraints-problem/#tldr","title":"TL;DR","text":"<p>In this post, we took a look at solving a number constraint problem by using idiomatic C# and F#. Even though both solutions are easy to read and easy to extend, the F# version was less than 1/2 the size of the C# solution. In addition, I didn\u2019t have to do any plumbing for the F# version, but had to do some for the C# solution, and to top it off, the F# solution is generically solved, whereas the C# solution is not.</p>"},{"location":"articles/2024/08/27/functional-design-patterns---reduce-and-monoids/","title":"Functional Design Patterns - Reduce and Monoids","text":"<p>When learning about a loops, a common exercise is to take an array of elements and calculate some value based on them.  For example, let's say that we have an array of numbers and we want to find their sum. One approach would be the following:</p> <pre><code>const values = [1, 2, 3, 4, 5];\nlet total = 0;\nfor (const val in values) {\n    total += val;\n}\n</code></pre> <p>This code works and we'll get the right answer, however, there's a pattern sitting here. If you find yourself writing code that has this shape (some initial value and some logic in the loop), then you have a reducer in disguise.</p> <p>Let's convert this code to use the reduce function instead.</p> <pre><code>const values = [1, 2, 3, 4, 5];\nconst total = values.reduce((acc, curr) =&gt; acc + curr, 0);\n</code></pre> <p>The <code>reduce</code> function takes two parameters, the reducer and the initial value. For the initial value, this is the value that the result should be if the array is empty. Since we're adding numbers together, zero makes the most sense.</p> <p>The reducer, on the other hand, says that if you give me the accumulated result so far (called acc in the example) and an element from the array (called curr in the above example), what's the new accumulation?</p> <p>Reduce is an extremely powerful tool (in fact, I give a presentation where we rewrite some of C#'s LINQ operators as reduce).</p> <p>But there's another pattern sitting here. If you find that the initial value and elements of the array have the same type, you most likely have a monoid.</p>"},{"location":"articles/2024/08/27/functional-design-patterns---reduce-and-monoids/#intro-to-monoids","title":"Intro to Monoids","text":"<p>The concept of Monoid comes from the field of category theory where a Monoid contains three things</p> <ol> <li>A set of values (you can think about this as a type)</li> <li>Some binary operation (e.g., a function that takes two inputs and returns a single output of said type)</li> <li>Some identity element such that if you pass the id to the binary operation, you get the other element back.</li> </ol> <p>There's a lot of theory sitting here, so let's put it in more concrete terms.</p> <p>If we have a Monoid over the a type A, then the following must be true:</p> <pre><code>function operation&lt;A&gt;(x:A, y:A): A \n{\n    // logic for operation\n}\nconst identity: A = // some value of type A\noperation(x, identity) === operation(identity, x) === x\n\noperation(x, operation(y, z)) === operation(operation(x, y), z)\n</code></pre> <p>Here's how we would define such a thing in TypeScript.</p> <pre><code>export interface Monoid&lt;A&gt;{\n    operation: (x:A, y:A)=&gt; A;\n    identity:A;\n}\n</code></pre>"},{"location":"articles/2024/08/27/functional-design-patterns---reduce-and-monoids/#exploring-total","title":"Exploring Total","text":"<p>With more theory in place, let's apply it to our running total from before. It turns out that addition forms a Monoid over positive numbers with the following:</p> <pre><code>const additionOverNumber: Monoid&lt;number&gt; = {\n    identity: 0,\n    operation: (a:number, b:number) =&gt; a+b;\n}\n</code></pre> <p>Wait a minute... This looks like what the <code>reduce</code> function needed from before! </p> <p>In the case where we have a Monoid, we have a way of reducing an array to a single value for free because of the properties that Monoid gives us.</p>"},{"location":"articles/2024/08/27/functional-design-patterns---reduce-and-monoids/#exploring-booleans","title":"Exploring Booleans","text":"<p>Thankfully, we're not limited to just numbers. For example, let's take a look at booleans with the <code>&amp;&amp;</code> and <code>||</code> operators.</p> <p>In the case of <code>&amp;&amp;</code>, that's our operation, so now we need to find the identity element. In other words, what value must <code>id</code> be if the following statements are true?</p> <pre><code>const id: boolean = ...\nid &amp;&amp; true === true\nid &amp;&amp; false === false\n</code></pre> <p>Since <code>id</code> has to be a boolean, the answer is <code>true</code>. Therefore, we can define our Monoid like so</p> <pre><code>const AndOverBoolean: Monoid&lt;boolean&gt; = {\n    identity: true,\n    operation: (a:boolean, b:boolean) =&gt; a &amp;&amp; b\n}\n</code></pre> <p>With this monoid defined, let's put it to use. Let's say that we wanted to check if every number in a list is even. We could write the following:</p> <pre><code>const values = [1, 2, 3, 4, 5];\n\nconst areAllEven = values.map(x=&gt;x%2===0).reduce(AndOverBoolean.operation, AndOverBoolean.identity);\n</code></pre> <p>Huh, that looks an awful lot like how we could have used every.</p> <pre><code>const values = [1, 2, 3, 4, 5];\nconst areAllEven = values.every(x=&gt;x%2===0);\n</code></pre> <p>Let's take a look at the <code>||</code> monoid now. We have the operation, but we now we need to find the identity. In other words, what value must <code>id</code> be if the following statements are true?</p> <pre><code>const id: boolean = ...;\nid || true === true\nid || false === false\n</code></pre> <p>Since <code>id</code> has to be a boolean, the answer is <code>false</code>. Therefore, we can define our monoid as such.</p> <pre><code>const OrOverBoolean: Monoid&lt;boolean&gt; = {\n    identity: false,\n    operation: (x:boolean, y:boolean) =&gt; x || y\n} \n</code></pre> <p>With this monoid defined, let's put it to use. Let's say that we wanted to check if some number in a list is even. We could write the following:</p> <pre><code>const values = [1, 2, 3, 4, 5];\n\nconst areAllEven = values.map(x=&gt;x%2===0).reduce(OrOverBoolean.operation, OrOverBoolean.identity);\n</code></pre> <p>Similar to the <code>AndOverBoolean</code>, this looks very similar to the code we would have written if we had leveraged some.</p> <pre><code>const values = [1, 2, 3, 4, 5];\n\nconst isAnyEven = values.some(x =&gt; x%2 === 0);\n</code></pre>"},{"location":"articles/2024/08/27/functional-design-patterns---reduce-and-monoids/#wrapping-up","title":"Wrapping Up","text":"<p>When working with arrays of items, it's common to need to reduce the list down to a single element. You can start with a for loop and then refactor to using the reduce function. If the types are all the same, then it's likely that you also have a monoid, which can give you stronger guarantees about your code.</p>"},{"location":"articles/2024/09/04/functional-design-patterns---missing-values/","title":"Functional Design Patterns - Missing Values","text":"<p>When working in software, a common problem that we run into is what should we do if we can't get a value or compute an answer.</p> <p>For example, let's say that you go to the library to checkout the latest book from Nathalia Holt, but when you search for it, you realize that they don't have any copies in stock.</p> <p>How do you handle the absence of value? From an Object Oriented Programming (OOP), you might be able to leverage a reasonable default or the Null Object Pattern. Another approach would be to return to always return a list and if there are no values, then the list would be empty.</p> <p>No matter the choice you make, we need some way to model the lack of a value. Tony Hoare referred to null as his billion dollar mistake as its inclusion in languages have created countless bugs and errors.</p> <p>In most functional languages, the idea of <code>null</code> doesn't exist, so how do they handle missing values?</p>"},{"location":"articles/2024/09/04/functional-design-patterns---missing-values/#revisiting-functions","title":"Revisiting Functions","text":"<p>In a previous post, we mention that a function is a mapping between two sets such that every element on the left (first set) is mapped to a single element on the right (the second set). So what happens when we have a mapping that's not a function?</p> <p>In these cases, we have two solutions to choose from:</p> <ol> <li>Restrict the function input to only be values that can be mapped (e.g., restrict the domain)</li> <li>Expand the possible values that the function can map to (e.g., expand the range)</li> </ol> <p>When refactoring a mapping, I tend to stick with option 1 because if we can prevent bad data from being created or passed in, that's less error handling that needs to be written and it becomes simpler to reason about. However, there are plenty of times where our type system (or business rules) can't enforce the valid inputs.</p>"},{"location":"articles/2024/09/04/functional-design-patterns---missing-values/#number-math","title":"Number Math","text":"<p>Let's say that we're working on a \"word calculator\" where the user can enter a phrase like \"one thousand twelve\" and it returns \"1012\" as output. If we think about our input set (domain) and outputs, we would have the following (mapping from string to number).</p> <p></p> <p>The issue is that we can't guarantee that the strings the user gives us would represent a number (for example, how would you convert \"platypus\" to a number?)</p> <p>Since we can't restrict the input, we have to expand the output. So let's update our output to be <code>number | 'invalid'</code> </p> <p></p> <p>With this change, anytime we call <code>convertToNumber</code>, we'll need to add some handling on what to do if the output is <code>invalid</code>. </p> <p>Here's some pseudocode of what this mapping could look like</p> <pre><code>type WordCalculatorResult = number | 'invalid'\n\nfunction convertToNumber(phrase:string): WorldCalculatorResult {\n    if (phrase === 'one') return 1;\n    if (phrase === 'two') return 2;\n    // ... other logic\n    return 'invalid';\n}\n\nconsole.log(convertToNumber('one')); // prints \"1\";\nconsole.log(convertToNumber('kumquats')) // prints invalid\n</code></pre>"},{"location":"articles/2024/09/04/functional-design-patterns---missing-values/#leaky-context","title":"Leaky Context","text":"<p>So far, so good as we have a function that converts words to numbers. Let's now say that we want to square a number. Writing such a function is straightforward.</p> <pre><code>function square (x:number): number {\n    return x*x;\n}\n</code></pre> <p>With both <code>convertToNumber</code> and <code>square</code> defined, let's try to combine these together: </p> <p></p> <p>This doesn't work because <code>convertToNumber</code> can return <code>invalid</code> which there's no way for the <code>square</code> function to work with that. Due to this limitation, we could rewrite the code to check for <code>invalid</code> before calling <code>square</code>.</p> <pre><code>const result = convertToNumber('one');\n\nif (result !== 'invalid') {\n    console.log(square(result));\n}\n</code></pre> <p>This approach will work, but the downside is that we need to add error handling logic in our pipeline now. If this was the only place where it's being used, then it's not too bad. However, if we were using <code>convertToNumber</code> in multiple places, then we have to remember to add this error handling logic. In fact, we would have almost the same <code>if</code> check everywhere.</p> <p>Let's take a look at a different way of solving this problem by using the Maybe type.</p>"},{"location":"articles/2024/09/04/functional-design-patterns---missing-values/#introducing-the-maybe-type","title":"Introducing the Maybe Type","text":"<p>In our original approach, we already had a type called <code>WordCalculatorResult</code> for the computation. Instead of it being a number or <code>invalid</code>, we could instead create a new type, called <code>Maybe</code> that looks like the following:</p> <pre><code>type Maybe&lt;T&gt; = { label: 'some', value: T } | { label: 'none' }\n</code></pre> <p>By leveraging generics and tagged unions, we have a way to represent a missing value for any type. With this new definition, we can rewrite the <code>convertToNumber</code> function to use the Maybe type.</p> <pre><code>function convertToNumber(phrase:string): Maybe&lt;number&gt; {\n    if (phrase === 'one') return {label:'some', value:1};\n    if (phrase === 'two') return {label:'some', value:2};\n    // other logic\n    return {label:'none'};\n}\n</code></pre> <p>I don't know about you, but typing out <code>{label:'some'}</code> is going to get tiring, so let's create two helper functions, <code>some</code> and <code>none</code> that handle putting the right label if we have a value or not.</p> <pre><code>function some&lt;T&gt;(value:T): Maybe&lt;T&gt; {\n    return {label:'some', value};\n}\n\nfunction none&lt;T&gt;(): Maybe&lt;T&gt; {\n    return {label:'none'};\n}\n</code></pre> <p>Which allows us to replace all of the <code>{label:'some', value:x}</code> with <code>some(x)</code> and <code>{label:'none'}</code> with <code>none()</code>.</p> <pre><code>function convertToNumber(phrase:string): Maybe&lt;number&gt; {\n    if (phrase === 'one') return some(1);\n    if (phrase === 'two') return some(2);\n    // other logic\n    return none();\n}\n</code></pre> <p>Now that <code>convertToNumber</code> returns a Maybe, our pipeline would look like the following:</p> <pre><code>const result = convertToNumber('one');\n\nif (result.label === 'some') { // checking if we're a Some type\n    console.log(square(result.value));\n}\n</code></pre>"},{"location":"articles/2024/09/04/functional-design-patterns---missing-values/#introducing-transformations-with-map","title":"Introducing Transformations With Map","text":"<p>Now the problem we're wanting to solve is to get rid of having to do an if check on <code>some</code> in order to call the <code>square</code> function. Given that we're wanting to transform our number into a different number, we can write another function, called <code>map</code> that takes two parameters: the <code>Maybe</code> we want transformed and the function that knows how to transform it. Let's take a look at how to write such a function.</p> <pre><code>function map&lt;T,U&gt;(m:Maybe&lt;T&gt;, f:(t:T)=&gt;U): Maybe&lt;U&gt; {\n    if (m.label === 'some') { // Is Maybe a Some?\n        const transformedValue = f(m.value); // Grab the value and transform it by calling function f\n        return some(transformedValue) // return a new Maybe with the transformed value\n    }\n    return none(); // Return none since there's no value to transform\n}\n</code></pre> <p>With <code>map</code> implemented, let's update our code to use it!</p> <pre><code>const convertResult = convertToNumber('one'); // \nconst transformedResult = map(convertResult, square);\n\nif (transformedResult.label === 'some') { // checking if we're a Some type\n    console.log(transformedResult.value);\n}\n\n/* The above could be simplified as the following\nconst result = map(convertToNumber('one'), square);\nif (result.label === 'some') {\n  console.log(result.value)\n}\n*/\n</code></pre> <p>Thanks to <code>map</code> and <code>Maybe</code>, we're able to write two pure functions (<code>convertToNumber</code> and <code>square</code>), glue them together, and delegate our error handling to one spot (instead of having to sprinkle this throughout the codebase). </p>"},{"location":"articles/2024/09/04/functional-design-patterns---missing-values/#calling-side-affects-using-tee","title":"Calling Side Affects Using Tee","text":"<p>With the addition of <code>map</code>, we've now simplified our <code>if</code> check to only call <code>console.log</code> now (instead of calling the square function then console.log). If we wanted to truly get rid of this if statement, then we need a way to invoke the <code>console.log</code> as a side effect. We don't want to combine <code>console.log</code> into <code>square</code> because <code>square</code> is business rules and we don't want to add side effect logic to our business rules.</p> <p>Similar to what we did for <code>map</code>, let's add a new function, called <code>tee</code>, that takes in three arguments: a <code>Maybe</code>, a function to call if the <code>Maybe</code> has a value, and then a function to call if <code>Maybe</code> doesn't have a value.</p> <p>Let's take a look at what an implementation would look like:</p> <pre><code>/*\n Inputs:\n    m: A Maybe to work with\n    ifSome: A function that takes in a T (which comes from the Maybe) and returns void\n    ifNone: A function that takes no inputs and returns void\n Outputs:\n    Returns m so that we can continue chaining calls\n*/\nfunction tee&lt;T&gt;(m:Maybe&lt;T&gt;, ifSome:(t:T)=&gt;void, ifNone:()=&gt;void): Maybe&lt;T&gt; {\n    if (m.label === 'some') {\n        ifSome(m.value);\n    } else {\n        ifNone();\n    }\n    return m;\n}\n</code></pre> <p>With <code>tee</code> implemented, we can update our pipeline to take advantage of it:</p> <pre><code>const convertResult = convertToNumber('one'); // \nconst transformedResult = map(convertResult, square);\n\nconst ifSome = (t:number) =&gt; console.log(`Result is ${t}`);\nconst ifNone = () =&gt; console.log('Failed to calculate result.');\n\ntee(transformedResult, ifSome, ifNone);\n\n// if we wanted to get rid of the temp values, we could do the following\n// const result = tee(map(convertToNumber('one'), square)), ifSome, ifNone);\n</code></pre>"},{"location":"articles/2024/09/04/functional-design-patterns---missing-values/#cleaning-up-by-using-an-object","title":"Cleaning Up by using an Object","text":"<p>We've now centralized the <code>if</code> logic to the functions that work on <code>Maybe</code>s which is helpful, but as we're already seeing, the code can be hard to parse as you need to read from the outside in (which is very Clojure or Scheme in nature).</p> <p>Instead of building up functions this way, we can instead create a TypeScript class and use a <code>fluent interface</code> pattern by doing the following:</p> <pre><code>export class Maybe&lt;T&gt;\n{\n  private constructor(private readonly value:T|undefined=undefined){}\n  map&lt;U&gt;(f:(t:T)=&gt;U): Maybe&lt;U&gt; {\n    return this.value !== undefined ? Maybe.some(f(this.value)) : Maybe.none();\n  }\n  tee(ifSome:(t:T)=&gt;void, ifNone:()=&gt;void) {\n    if (this.value !== undefined) {\n      ifSome(this.value);\n    } else {\n      ifNone();\n    }\n    return this;\n  }\n  static some&lt;T&gt;(value:T): Maybe&lt;T&gt; {\n    return new Maybe(value);\n  }\n  static none&lt;T&gt;(): Maybe&lt;T&gt; {\n    return new Maybe();\n  }\n}\n// Defined in index.ts\nfunction convertToNumber(phrase:string): Maybe&lt;number&gt; {\n    if (phrase === 'one') return Maybe.some(1);\n    if (phrase === 'two') return Maybe.some(2);\n    // other logic\n    return Maybe.none();\n}\n\nfunction square(x:number): number {\n    return x*x;\n}\n\n// By using the fluent interface pattern, we can chain methods together and have this code read\n// more naturally\nconvertToNumber(\"one\")\n  .map(square)\n  .tee(\n    (v)=&gt; `Answer is ${v}`, \n    () =&gt; \"Unable to calculate result\"\n  );\n</code></pre>"},{"location":"articles/2024/09/04/functional-design-patterns---missing-values/#wrapping-up","title":"Wrapping Up","text":"<p>When writing software, a common problem we run into is what to do when we can't calculate a value or a result. In this post, we looked at techniques to restrict the function's input or expand their output to handle scenarios. From there, we looked into a type called Maybe and how it can represent the absence of value, but still provide a way to remove having to explicitly write error handling code by consolidating the checks into a <code>map</code> call. Lastly, we look into taking the various functions that we wrote and combine them into a formal <code>Maybe</code> class that allows us to leverage a fluent interface and chain our interactions together.</p>"},{"location":"articles/2023/06/26/having-coffee-with-deno---inspiration/","title":"Having Coffee with Deno - Inspiration","text":"<p>In a previous post, I mention my strategy of building relationships through one-on-ones. One approach in the post was leveraging a Slack plugin, Random Coffee, to automate scheduling these impromptu conversations.</p> <p>I wanted to leverage the same idea at my current company; however, we don't use Slack, so I can't just use that bot.</p>"},{"location":"articles/2023/06/26/having-coffee-with-deno---inspiration/#high-level-breakdown","title":"High-Level Breakdown","text":"<p>Thinking more about it, the system wouldn't be too complicated as it has three moving parts:</p> <ul> <li>Get list of people</li> <li>Create random pairs</li> <li>Post message</li> </ul> <p>To make it even easier, I could hardcode the list of people, and instead of posting the message to our message application, I could print it to the screen.</p> <p>With these two choices made, I would need to build something that can shuffle a list and create pairs.</p>"},{"location":"articles/2023/06/26/having-coffee-with-deno---inspiration/#technology-choices","title":"Technology Choices","text":"<p>Even though we're hardcoding the list of names and printing a message to the screen, I know that the future state is to get the list of names dynamically, most likely through an API call. In addition, most messaging systems support using webhooks to create a message, so that would be the future state as well.</p> <p>With these restrictions in mind, I know I need to use a language that is good at making HTTP calls. I also want this automation to be something that other people outside of me can maintain, so if I can use a language that we typically use, that makes this more approachable.</p> <p>In my case, TypeScript fit the bill as we heavily use it in my space, the docs are solid, and it's straightforward to make HTTP calls. I'm also a fan of functional programming, which TypeScript supports nicely.</p> <p>My major hurdle at this point is that I'd like to execute this single file of TypeScript, and the only way I knew how to do that was by spinning up a Node application and using something like ts-node to execute the file.</p> <p>Talking to a colleague, they recommended I check out Deno as a possible solution. The more I learned about it, the more I thought this would fit perfectly. It supports TypeScript out of the box (no configuration needed), and a file can be ran with <code>deno run</code>, no other tools needed.</p> <p>This project is simple enough that if Deno wasn't a good fit, I could always go back to Node.</p> <p>With this figured out, we're going to create a Deno application using TypeScript as our language of choice.</p>"},{"location":"articles/2023/06/26/having-coffee-with-deno---inspiration/#getting-started-with-deno","title":"Getting Started With Deno","text":"<ol> <li>Install Deno via these instructions</li> <li>Setup your dev environment - I use VS Code, so adding the recommended extension was all I needed.</li> </ol>"},{"location":"articles/2023/06/26/having-coffee-with-deno---inspiration/#trying-deno-out","title":"Trying Deno Out","text":"<p>Once Deno has been installed and configured, you can run the following script and verify everything is working correctly. It creates a new directory called <code>deno-coffee</code>, writes a new file and executes it via deno.</p> <pre><code>mkdir deno-coffee\ncd deno-coffee\necho 'console.log(\"Hello World\");' &gt;&gt; coffee.ts\ndeno run coffee.ts\n</code></pre> <p>We've got something working, so let's start building out the random coffee script.</p>"},{"location":"articles/2023/06/26/having-coffee-with-deno---inspiration/#lets-get-percolating","title":"Let's Get Percolating","text":"<p>As mentioned before, we're going to hardcode a list of names and print to the screen, so let's build out the rough shape of the script:</p> <pre><code>const names = [\n  \"Batman\",\n  \"Superman\",\n  \"Green Lantern\",\n  \"Wonder Woman\",\n  \"Static Shock\", // one of my favorite DC heroes!\n  \"The Flash\",\n  \"Aquaman\",\n  \"Martian Manhunter\",\n];\nconst pairs = createPairsFrom(shuffle(names));\nconst message = createMessage(pairs);\nconsole.log(message);\n</code></pre> <p>This code won't compile as we haven't defined what <code>shuffle</code>, <code>createPairsFrom</code>, or <code>createMessage</code> does, but we can tackle these one at a time.</p>"},{"location":"articles/2023/06/26/having-coffee-with-deno---inspiration/#lets-get-random","title":"Let's Get Random","text":"<p>Since we don't want the same people meeting up every time, we need a way to shuffle the list of names. We could import a library to do this, but what's the fun in that?</p> <p>In this case, we're going to implement the Fisher-Yates Shuffle (sounds like a dance move).</p> <pre><code>function shuffle(items: string[]): string[] {\n  // create a copy so we don't mutate the original\n  const result = [...items];\n  for (let i = result.length - 1; i &gt; 0; i--) {\n    // create an integer between 0 and i\n    const j = Math.floor(Math.random() * i);\n    // short-hand for swapping two elements around\n    [result[i], result[j]] = [result[j], result[i]];\n  }\n  return result;\n}\n\nconst words = [\"apples\", \"bananas\", \"cantaloupes\"];\nconsole.log(shuffle(words)); // [ \"bananas\", \"cantaloupes\", \"apples\" ]\n</code></pre> <p>Excellent, we have a way to shuffle. One refactor we can make is to have <code>shuffle</code> be generic as we don't care what array element types are, as long as we have an array.</p> <p>Making this refactor gives us the following:</p> <pre><code>function shuffle&lt;T&gt;(items: T[]): T[] {\n  const result = [...items];\n  for (let i = result.length - 1; i &gt; 0; i--) {\n    const j = Math.floor(Math.random() * i);\n    [result[i], result[j]] = [result[j], result[i]];\n  }\n  return result;\n}\n</code></pre> <p>Now, we can shuffle an array of anything. Nice!</p>"},{"location":"articles/2023/06/26/having-coffee-with-deno---inspiration/#two-of-a-kind","title":"Two of a Kind","text":"<p>Let's take a look at the next function, <code>createPairsFrom</code>. We know its type signature is going from<code>string[]</code> to something, but what?</p> <p>In the ideal world, our total list of names is even, so we always have equal pairs.</p> <pre><code>{first: 'Batman', second: 'Superman'},\n{first: 'Green Lantern', second: 'Wonder Woman'},\n{first: 'Static Shock', second: 'The Flash'},\n{first: 'Aquaman', second: 'Martian Manhunter'}\n</code></pre> <p>But what happens if <code>Martian Manhunter</code> is called away and isn't available? That would leave <code>Aquaman</code> without a pair to have coffee with (sad trombone noise).</p> <p>In the case that we have an odd number of heroes, the last pair should instead be a triple which would look like the following:</p> <pre><code>{first: 'Batman', second: 'Superman'},\n{first: 'Green Lantern', second: 'Wonder Woman'},\n{first: 'Static Shock', second: 'The Flash', third: 'Martian Manhunter'}\n</code></pre> <p>Given that we've been using the word <code>Pair</code> to represent this grouping, we have a domain term we can use. This also means that <code>createPairsFrom</code> has the following type signature.</p> <pre><code>function createPairsFrom(names: string[]): Pair[] {\n  // logic\n}\n</code></pre> <p>But what does the <code>Pair</code> type look like? We can model either using an optional property or by using a discriminated union.</p> <pre><code>// Using an optional property\ntype Pair {\n  first: string,\n  second: string,\n  third?: string\n}\n\n// Using Discriminated Unions\ntype Pair = {kind: 'double', first:string, second: string}\n          | {kind: 'triple', first:string, second: string; third: string}\n</code></pre> <p>For now, I'm thinking of going with the optional property and if we need to tweak it later, we can.</p> <p>Let's go ahead and implement <code>createPairsFrom</code>.</p> <pre><code>function createPairsFrom(names: string[]): Pair[] {\n  // if we don't have at least two names, then there are no pairs\n  if (names.length &lt; 2) {\n    return [];\n  }\n  const results = [];\n  for (let i = 0; i &lt;= names.length - 2; i += 2) {\n    const pair: Pair = { first: names[i], second: names[i + 1] };\n    results.push(pair);\n  }\n  if (names.length % 2 === 1) {\n    // we have an odd length\n    // Assign the left-over name to the third of the triple\n    results[results.length - 1].third = names[names.length - 1];\n  }\n  return results;\n}\n\n// Example execution\nconsole.log(createPairsFrom([\"apples\", \"bananas\", \"cantaloupes\", \"dates\"])); // [{first:\"apples\", second:\"bananas\"}, {first:\"cantaloupes\", second:\"dates\"}]\nconsole.log(createPairsFrom([\"ants\", \"birds\", \"cats\"])); // [{first:\"ants\", second:\"birds\", third:\"cats\"}]\n</code></pre> <p>Similarly to <code>shuffle</code>, we can make this function generic as it doesn't matter what the array element types are, as long as we have an array to work with.</p> <p>Refactoring to generics gives us the following:</p> <pre><code>type Pair&lt;T&gt; = { first: T; second: T; third?: T };\n\nfunction createPairsFrom&lt;T&gt;(items: T[]): Pair&lt;T&gt;[] {\n  // same function as before\n}\n</code></pre>"},{"location":"articles/2023/06/26/having-coffee-with-deno---inspiration/#to-the-presses","title":"To the Presses!","text":"<p>For the last part, we need to implement <code>createMessage</code>. We know it has to have the following type signature:</p> <pre><code>function createMessage(pairs: Pair&lt;string&gt;[]): string {}\n</code></pre> <p>We know the following rules.</p> <ul> <li>When it's a double, we want the message to say, <code>X meets with Y</code></li> <li>When it's a triple, we want the message to say, <code>X meets with Y and Z</code></li> </ul> <p>Based on this, we need a way to map from <code>Pair</code> to the above string. So let's write that logic.</p> <pre><code>function createMessage(pairs: Pair&lt;string&gt;[]): string {\n  const mapper = (p: Pair&lt;string&gt;) =&gt;\n    `${p.first} meets with ${p.second}${p.third ? ` and ${p.third}` : \"\"}`;\n\n  pairs.map(mapper);\n}\n</code></pre> <p>From here, we can join the strings together using the <code>\\n</code> (newline) character.</p> <pre><code>function createMessage(pairs: Pair&lt;string&gt;[]): string {\n  const mapper = (p: Pair&lt;string&gt;) =&gt;\n    `${p.first} meets with ${p.second}${p.third ? ` and ${p.third}` : \"\"}`;\n\n  return pairs.map(mapper).join(\"\\n\");\n}\n</code></pre>"},{"location":"articles/2023/06/26/having-coffee-with-deno---inspiration/#all-coming-together","title":"All Coming Together","text":"<p>With the implementation of <code>createMessage</code>, we can execute our script by running <code>deno run coffee.ts</code></p> <pre><code>deno run coffee.ts\n\n\"Superman meets with Wonder Woman\nBatman meets with The Flash\nMartian Manhunter meets with Aquaman\nStatic Shock meets with Green Lantern\"\n</code></pre> <p>From here, we have a working proof of concept of our idea. We could run this manually on Mondays and then post this to our messaging channel (though you might want to switch the names out). If you wanted to be super fancy, you could have this scheduled as a cron job or through Windows Task Scheduler.</p> <p>The main takeaway is that we've built something we didn't have before and can continue to refine and improve. If no one likes the idea, guess what? We only had a little time invested. If it takes off, then that's great; we can spend more time making it better.</p>"},{"location":"articles/2023/06/26/having-coffee-with-deno---inspiration/#wrapping-up","title":"Wrapping Up","text":"<p>In this post, we built the first version of our Random Coffee script using TypeScript and Deno. We focused on getting our tooling working and building out the business rules for shuffling and creating the pairs.</p> <p>In the next post, we'll look at making this script smarter by having it retrieve a list of names dynamically from GitHub's API!</p> <p>As always, you can find a full working version of this bot on my GitHub.</p>"},{"location":"articles/2023/06/26/having-coffee-with-deno---inspiration/#other-posts-in-the-series","title":"Other Posts In The Series","text":"<ul> <li>Having Coffee with Deno - Dynamic Names</li> <li>Having Coffee with Deno - Sharing the News</li> <li>Having Coffee with Deno - Automating All the Things</li> </ul>"},{"location":"articles/2023/07/10/having-coffee-with-deno---dynamic-names/","title":"Having Coffee with Deno - Dynamic Names","text":"<p>Welcome to the second installment of our Deno series, where we build a script that pairs up people for coffee.</p> <p>In the last post, we built a script that helped the Justice League meet up for coffee.</p> <p>As of right now, our script looks like the following.</p> index.ts<pre><code>const names = [\n  \"Batman\",\n  \"Superman\",\n  \"Green Lantern\",\n  \"Wonder Woman\",\n  \"Static Shock\", // one of my favorite DC heroes!\n  \"The Flash\",\n  \"Aquaman\",\n  \"Martian Manhunter\",\n];\nconst pairs = createPairsFrom(shuffle(names));\nconst message = createMessage(pairs);\nconsole.log(message);\n\nfunction shuffle&lt;T&gt;(items: T[]): T[] {\n  const result = [...items];\n  for (let i = result.length - 1; i &gt; 0; i--) {\n    const j = Math.floor(Math.random() * i);\n    [result[i], result[j]] = [result[j], result[i]];\n  }\n  return result;\n}\ntype Pair&lt;T&gt; = { first: T; second: T; third?: T };\nfunction createPairsFrom&lt;T&gt;(items: T[]): Pair&lt;T&gt;[] {\n  if (items.length &lt; 2) {\n    return [];\n  }\n  const results = [];\n  for (let i = 0; i &lt;= items.length - 2; i += 2) {\n    const pair: Pair = { first: items[i], second: items[i + 1] };\n    results.push(pair);\n  }\n  if (items.length % 2 === 1) {\n    results[results.length - 1].third = items[items.length - 1];\n  }\n  return results;\n}\nfunction createMessage(pairs: Pair&lt;string&gt;[]): string {\n  const mapper = (p: Pair&lt;string&gt;) =&gt;\n    `${p.first} meets with ${p.second}${p.third ? ` and ${p.third}` : \"\"}`;\n\n  return pairs.map(mapper).join(\"\\n\");\n}\n</code></pre> <p>Even though this approach works, the major problem is that every time there's a member change in the Justice League (which seems to happen more often than not), we have to go back and update the list manually.</p> <p>It'd be better if we could get this list dynamically instead. Given that the League are great developers, they have their own GitHub organization. Let's work on integrating with GitHub's API to get the list of names.</p>"},{"location":"articles/2023/07/10/having-coffee-with-deno---dynamic-names/#the-approach","title":"The Approach","text":"<p>To get the list of names from GitHub, we'll need to do the following.</p> <ol> <li>First, we need to figure out which GitHub endpoint will give us the members of the League. This, in turn, will also tell us what permissions we need for our API scope.</li> <li>Now that we have a secret, we need to update our script to read from an <code>.env</code> file.</li> <li>Once we have the secret being read, we can create a function to retrieve the members of the League.</li> <li>Miscellaneous refactoring of the main script to handle a function returning complex types instead of strings.</li> </ol> <p></p>"},{"location":"articles/2023/07/10/having-coffee-with-deno---dynamic-names/#laying-the-foundation","title":"Laying the Foundation","text":"<p>Before we start, we should reactor our current file. It works, but we have a mix of utility functions (<code>shuffle</code> and <code>createPairsFrom</code>) combined with presentation functions (<code>createMessage</code>). Let's go ahead and move <code>shuffle</code> and <code>createPairsFrom</code> to their own module.</p> utility.ts<pre><code>type Pair&lt;T&gt; = { first: T; second: T; third?: T };\n\nfunction shuffle&lt;T&gt;(items: T[]): T[] {\n  const result = [...items];\n  for (let i = result.length - 1; i &gt; 0; i--) {\n    const j = Math.floor(Math.random() * i);\n    [result[i], result[j]] = [result[j], result[i]];\n  }\n  return result;\n}\n\nfunction createPairsFrom&lt;T&gt;(items: T[]): Pair&lt;T&gt;[] {\n  if (items.length &lt; 2) {\n    return [];\n  }\n  const results: Pair&lt;T&gt;[] = [];\n  for (let i = 0; i &lt;= items.length - 2; i += 2) {\n    const pair: Pair&lt;T&gt; = { first: items[i], second: items[i + 1] };\n    results.push(pair);\n  }\n  if (items.length % 2 === 1) {\n    results[results.length - 1].third = items[items.length - 1];\n  }\n  return results;\n}\n\nexport { createPairsFrom, shuffle };\nexport type { Pair };\n</code></pre> <p>With these changes, we can update <code>index.ts</code> to be:</p> index.ts<pre><code>import { Pair, createPairsFrom, shuffle } from \"./module.ts\";\n\nconst names = [\n  \"Batman\",\n  \"Superman\",\n  \"Green Lantern\",\n  \"Wonder Woman\",\n  \"Static Shock\", // one of my favorite DC heroes!\n  \"The Flash\",\n  \"Aquaman\",\n  \"Martian Manhunter\",\n];\nconst pairs = createPairsFrom(shuffle(names));\nconst message = createMessage(pairs);\nconsole.log(message);\n\nfunction createMessage(pairs: Pair&lt;string&gt;[]): string {\n  const mapper = (p: Pair&lt;string&gt;) =&gt;\n    `${p.first} meets with ${p.second}${p.third ? ` and ${p.third}` : \"\"}`;\n\n  return pairs.map(mapper).join(\"\\n\");\n}\n</code></pre>"},{"location":"articles/2023/07/10/having-coffee-with-deno---dynamic-names/#getting-github","title":"Getting GitHub","text":"<p>Now that our code is more tidy, we can focus on figuring out which GitHub endpoint(s) to use to figure out the members of the Justice League.</p> <p>Taking a look at the docs, we see that there are two different options.</p> <ol> <li>Get members of an Organization</li> <li>Get members of a Team</li> </ol> <p>What's the difference between the two? In GitHub parlance, an Organization is an overarching entity that consists of members which, in turn, can be part of multiple teams.</p> <p>Using the Justice League as an example, it's an organization that contains Batman, and Batman can be part of the Justice League Founding Team and a member of the Batfamily Team.</p> <p>Since we want to pair everyone up in the Justice League, we'll use the Get members of an Organization approach.</p>"},{"location":"articles/2023/07/10/having-coffee-with-deno---dynamic-names/#working-with-secrets","title":"Working with Secrets","text":"<p>To interact with the endpoint, we'll need to create an API token for GitHub. Looking over the docs, our token needs to have the <code>read:org</code> scope. We can create this token by following the instructions here about creating a Personal Auth Token (PAT).</p> <p>Once we have the token, we can invoke the endpoint with cURL or Postman to verify that we can communicate with the endpoint correctly.</p> <p>After we've verified, we'll need a way to get this API token into our script. Given that this is sensitive information, we absolutely should NOT check this into the source code.</p>"},{"location":"articles/2023/07/10/having-coffee-with-deno---dynamic-names/#creating-an-env-file","title":"Creating an ENV File","text":"<p>A common way of dealing with that is to use an .env file which doesn't get checked in, but our application can use it during runtime to get secrets.</p> <p>Let's go ahead and create the <code>.env</code> file and put our API token here.</p> .env<pre><code>GITHUB_BEARER_TOKEN=\"INSERT_TOKEN_HERE\"\n</code></pre> <p>Our problem now is that if we check <code>git status</code>, we'll see this file listed as a change. We don't want to check this in, so let's add a <code>.gitignore</code> file.</p>"},{"location":"articles/2023/07/10/having-coffee-with-deno---dynamic-names/#adding-a-gitignore-file","title":"Adding a .gitignore File","text":"<p>With the <code>.env</code> file created, we need to create a .gitignore file, which tells Git not to check in certain files.</p> <p>Let's go ahead and add the file. You can enter the below, or you can use the Node gitignore file (found here)</p> .gitignore<pre><code>.env # ignores all .env files in the root directory\n</code></pre> <p>We can validate that we've done this correctly if we run <code>git status</code> and don't see <code>.env</code> showing up anymore as a changed file.</p>"},{"location":"articles/2023/07/10/having-coffee-with-deno---dynamic-names/#loading-our-env-file","title":"Loading Our Env File","text":"<p>Now that we have the file created, we need to make sure that this file loads at runtime.</p> <p>In our <code>index.ts</code> file, let's make the following changes.</p> index.ts<pre><code>import { config as loadEnv } from \"https://deno.land/x/dotenv@v3.2.2/mod.ts\";\n// other imports\n\n// This loads the .env file and adds them to the environment variable list\nawait loadEnv({ export: true });\n// Deno.env.get(\"name\") retrieves the value from an environment variable named \"name\"\nconsole.log(Deno.env.get(\"GITHUB_BEARER_TOKEN\"));\n\n// remaining code\n</code></pre> <p>When we run the script now with <code>deno run</code>, we get an interesting prompt:</p>  Deno requests read access to \".env\".  - Requested by `Deno.readFileSync()` API.  - Run again with --allow-read to bypass this prompt  - Allow?  <p>This is one of the coolest parts about Deno; it has a security system that prevents scripts from doing something that you hadn't intended through its Permissions framework.</p> <p>For example, if you weren't expecting your script to read from the <code>env</code> file, it'll prompt you to accept. Since packages can be taken over and updated to do nefarious things, this is a terrific idea.</p> <p>The permissions can be tuned (e.g., you're only allowed to read from the .env file), or you can give blanket permissions. In our cases, two resources are being used (the ability to read the <code>.env</code> file and the ability to read the <code>GITHUB_BEARER_TOKEN</code> environment variable).</p> <p>Let's run the command with the <code>allow-read</code> and <code>allow-env</code> flags.</p> <p><code>deno run --allow-run --allow-env ./index.ts</code></p> <p>If the bearer token gets printed, we've got the <code>.env</code> file created correctly and can proceed to the next step.</p>"},{"location":"articles/2023/07/10/having-coffee-with-deno---dynamic-names/#lets-get-dynamic","title":"Let's Get Dynamic","text":"<p>Now that we have the bearer token, we can work on calling the GitHub Organization endpoint to retrieve the members.</p> <p>Since this is GitHub related, we should create a new file, <code>github.ts</code>, to host our functions and types.</p>"},{"location":"articles/2023/07/10/having-coffee-with-deno---dynamic-names/#adding-axiod","title":"Adding axiod","text":"<p>In the <code>github.ts</code> file, we're going to be use axiod for communication. It's similar to axios in Node and is better than then the built-in fetch API.</p> <p>Let's go ahead and bring in the import.</p> github.ts<pre><code>import axiod from \"https://deno.land/x/axiod@0.26.2/mod.ts\";\n</code></pre>"},{"location":"articles/2023/07/10/having-coffee-with-deno---dynamic-names/#calling-the-organization-endpoint","title":"Calling the Organization Endpoint","text":"<p>With <code>axiod</code> pulled in, let's write the function to interact with the GitHub API.</p> github.ts<pre><code>// Brining in the axiod library\nimport axiod from \"https://deno.land/x/axiod@0.26.2/mod.ts\";\n\nasync function getMembersOfOrganization(orgName: string): Promise&lt;any[]&gt; {\n  const url = `https://api.github.com/orgs/${orgName}/members`;\n  // Necessary headers are found on the API docs\n  const headers = {\n    Accept: \"application/vnd.github+json\",\n    Authorization: `Bearer ${Deno.env.get(\"GITHUB_BEARER_TOKEN\")}`,\n    \"X-GitHub-Api-Version\": \"2022-11-28\",\n  };\n\n  try {\n    const resp = await axiod.get&lt;any[]&gt;(url, {\n      headers: headers,\n    });\n    return resp.data;\n  } catch (error) {\n    // Response was received, but non-2xx status code\n    if (error.response) {\n      return Promise.reject(\n        `Failed to get members: ${error.response.status}, ${error.response.statusText}`\n      );\n    } else {\n      // Response wasn't received\n      return Promise.reject(\n        \"Failed for non status reason \" + JSON.stringify(error)\n      );\n    }\n  }\n}\n</code></pre> <p>To prove this is working, we can call this function in the <code>index.ts</code> file and verify that we're getting a response.</p> index.ts<pre><code>import { config as loadEnv } from \"https://deno.land/x/dotenv@v3.2.2/mod.ts\";\nimport { getMembersOfOrganization } from \"./github.ts\";\nimport { Pair, createPairsFrom, shuffle } from \"./utility.ts\";\n\nawait loadEnv({ export: true });\n\nconst membersOfOrganization = await getMembersOfOrganization(\"JusticeLeague\");\nconsole.log(JSON.stringify(membersOfOrganization));\n// rest of the file\n</code></pre> <p>Now let's rerun the script.</p> <pre><code>deno run --allow-read --allow-env ./main.ts\n</code></pre>  Deno requests net access to \"api.github.com\"  - Requested by `fetch` API.  - Run again with --allow-net to bypass this prompt. <p>Ah! Our script is now doing something new (making network calls), so we'll need to allow that permission by using the <code>--allow-net</code> flag.</p> <pre><code>deno run --allow-read --allow-env --allow-net ./main.ts\n</code></pre> <p>If everything has worked, you should see a bunch of JSON printed to the screen. Success!</p>"},{"location":"articles/2023/07/10/having-coffee-with-deno---dynamic-names/#creating-the-response-type","title":"Creating the Response Type","text":"<p>At this point, we're making the call, but we're using a pesky <code>any</code> for the response, which works, but it doesn't help us with what properties we have to work with.</p> <p>Looking at the response schema, it seems the main field we need is <code>login</code>. So let's go ahead and create a type that includes that field.</p> github.ts<pre><code>type GetOrganizationMemberResponse = {\n  login: string;\n};\n\nasync function getMembersOfOrganization(\n  orgName: string\n): Promise&lt;GetOrganizationMemberResponse[]&gt; {\n  //code\n  const resp = await axiod.get&lt;GetOrganizationMemberResponse[]&gt;(url, {\n    headers: headers,\n  });\n  // rest of the code\n}\n</code></pre> <p>We can rerun our code and verify that everything is still working, but now with better type safety.</p>"},{"location":"articles/2023/07/10/having-coffee-with-deno---dynamic-names/#cleaning-up","title":"Cleaning Up","text":"<p>Now that we have this function written, we can work to integrate it with our <code>index.ts</code> script.</p> index.ts<pre><code>import { config as loadEnv } from \"https://deno.land/x/dotenv@v3.2.2/mod.ts\";\nimport { getMembersOfOrganization } from \"./github.ts\";\nimport { Pair, createPairsFrom, shuffle } from \"./utility.ts\";\n\nawait loadEnv({ export: true });\n\nconst names = await getMembersOfOrganization(\"JusticeLeague\");\nconst pairs = createPairsFrom(shuffle(names));\nconst message = createMessage(pairs);\nconsole.log(message);\n</code></pre> <p>So far, so good. The only change we had to make was to replace the hardcoded array of names with the call to <code>getMembersOfOrganization</code>.</p> <p>Not an issue, right?</p> <p>Hmmm, what's up with this? </p> <p>It looks like <code>createMessage</code> is expecting <code>Pair&lt;string&gt;[]</code>, but is receiving <code>Pair&lt;GetOrganizationMemberResponse&gt;[]</code>.</p> <p>To solve this problem, we'll modify <code>createMessage</code> to work with <code>GetOrganizationMemberResponse</code>.</p> index.ts<pre><code>// Need to update the input to be Pair&lt;GetOrganizationMemberResponse&gt;\nfunction createMessage(pairs: Pair&lt;GetOrganizationMemberResponse&gt;[]): string {\n  // Need to update mapper function to get the login property\n  const mapper = (p: Pair&lt;string&gt;): string =&gt;\n    `${p.first.login} meets with ${p.second.login}${\n      p.third ? ` and ${p.third.login}` : \"\"\n    }`;\n\n  return pairs.map(mapper).join(\"\\n\");\n}\n</code></pre> <p>With this last change, we run the script and verify that we're getting the correct output, huzzah!</p>"},{"location":"articles/2023/07/10/having-coffee-with-deno---dynamic-names/#current-status","title":"Current Status","text":"<p>Congratulations! We now have a script that is dynamically pulling in heroes from the Justice League organization instead of always needing to see if Green Lantern is somewhere else or if another member of Flash's SpeedForce is here for the moment.</p> <p>A working version of the code can be found on GitHub.</p>"},{"location":"articles/2023/07/10/having-coffee-with-deno---dynamic-names/#other-posts-in-the-series","title":"Other Posts In The Series","text":"<ul> <li>Having Coffee with Deno - Inspiration</li> <li>Having Coffee with Deno - Sharing the News</li> <li>Having Coffee with Deno - Automating All the Things</li> </ul>"},{"location":"articles/2023/07/24/having-coffee-with-deno---sharing-the-news/","title":"Having Coffee with Deno - Sharing the News","text":"<p>Welcome to the third installment of our Deno series, where we build a script that pairs up people for coffee.</p> <p>In the last post, we're dynamically pulling members of the Justice League from GitHub instead of a hardcoded list.</p> <p>Like any good project, this approach works, but now the major problem is that we have to run the script, copy the output, and post it into our chat tool so everyone knows the schedule.</p> <p>It'd be better if we could update our script to post this message instead. In this example, we're going to use Slack and their incoming webhook, but you could easily tweak this approach to work with other tools like Microsoft Teams or Discord.</p>"},{"location":"articles/2023/07/24/having-coffee-with-deno---sharing-the-news/#the-approach","title":"The Approach","text":"<p>In order to for the script to post to Slack, we'll need to make the following changes:</p> <ol> <li>Follow these docs from Slack to create an application and enable the incoming webhooks.</li> <li>Test that we can post a simple message to the channel</li> <li>From here, we'll need to add code to make a POST call to the webhook with a message</li> <li>Tweak the formatting so it looks nicer</li> </ol> <p></p>"},{"location":"articles/2023/07/24/having-coffee-with-deno---sharing-the-news/#creating-the-slack-application-and-creating-the-webhook","title":"Creating the Slack Application and creating the Webhook","text":"<p>For this step, we'll follow the instructions in the docs, ensuring that we're hooking it up to the right channel.</p> <p>After following the steps, you should see something like the following:</p> <p></p> <p>We can test that things are working correctly by running the <code>curl</code> command provided. If the message <code>Hello World</code> appears in the channel, congrats, you've got the incoming webhook created!</p>"},{"location":"articles/2023/07/24/having-coffee-with-deno---sharing-the-news/#modifying-the-script-to-post-to-webhook","title":"Modifying the Script to POST to Webhook","text":"<p>We have the Slack app created and verified that the incoming webhook is working, so we'll need to add this integration to our script.</p> <p>Since we have this incoming webhook URL and Slack recommends treating this as a secret, we'll need to add this to our <code>.env</code> file.</p> <p></p> .env<pre><code>GITHUB_API_TOKEN=\"&lt;yourTokenHere&gt;\"\nSLACK_WEBHOOK=\"&lt;yourWebHookHere&gt;\"\n</code></pre> <p>With this secret added, we can write a new function, <code>sendMessage</code>, that'll make the POST call to Slack. Since this is a new integration, we'll add a new file, <code>slack.ts</code> to put it in.</p> slack.ts<pre><code>// Using axiod for the web connection\nimport axiod from \"https://deno.land/x/axiod@0.26.2/mod.ts\";\n\n// function to send a message to the webhook\nasync function sendMessage(message: string): Promise&lt;void&gt; {\n  // Get the webhookUrl from our environment\n  const webhookUrl = Deno.env.get(\"SLACK_WEBHOOK\")!;\n\n  try {\n    // Send the POST request\n    await axiod.post(webhookUrl, message, {\n      headers: {\n        \"Content-Type\": \"application/json\",\n      },\n    });\n  } catch (error) {\n    // Error handling\n    if (error.response) {\n      return Promise.reject(\n        `Failed to post message: ${error.response.status}, ${error.response.statusText}`\n      );\n    }\n    return Promise.reject(\n      \"Failed for non status reason \" + JSON.stringify(error)\n    );\n  }\n}\n\nexport { sendMessage };\n</code></pre> <p>With <code>sendMessage</code> done, let's update <code>index.ts</code> to use this new functionality.</p> index.ts<pre><code>import { load } from \"https://deno.land/std@0.195.0/dotenv/mod.ts\";\nimport {\n  GetOrganizationMemberResponse,\n  getMembersOfOrganization,\n} from \"./github.ts\";\nimport { sendMessage } from \"./slack.ts\";\nimport { Pair, createPairsFrom, shuffle } from \"./utility.ts\";\n\nawait load({ export: true });\n\n// Replace this with your actual organization name\nconst organizationName = \"JusticeLeague\";\nconst names = await getMembersOfOrganization(organizationName);\nconst pairs = createPairsFrom(shuffle(names));\nconst message = createMessage(pairs);\n// Slack expects the payload to be an object of text, so we're doing that here for now\nawait sendMessage(JSON.stringify({ text: message }));\n\nfunction createMessage(pairs: Pair&lt;GetOrganizationMemberResponse&gt;[]): string {\n  const mapper = (p: Pair&lt;GetOrganizationMemberResponse&gt;): string =&gt;\n    `${p.first.login} meets with ${p.second.login}${\n      p.third ? ` and ${p.third.login}` : \"\"\n    }`;\n  return pairs.map(mapper).join(\"\\n\");\n}\n</code></pre> <p>And if we were to run the above, we can see the following message get sent to Slack.</p> <p></p> <p>Nice! We could leave it here, but we could make the message prettier (having an unordered list and italicizing names), so let's work on that next.</p>"},{"location":"articles/2023/07/24/having-coffee-with-deno---sharing-the-news/#pretty-printing-the-message","title":"Pretty Printing the Message","text":"<p>So far, we could leave the messaging as is, however; it's a bit muddled. To help it pop, let's make the following changes.</p> <ul> <li>Italicize the names</li> <li>Start each pair with a bullet point</li> </ul> <p>Since Slack supports basic Markdown in the messages, we can use the <code>_</code> for italicizing and <code>-</code> for the bullet points. So let's modify the <code>createMessage</code> function to add this formatting.</p> index.ts<pre><code>function createMessage(pairs: Pair&lt;GetOrganizationMemberResponse&gt;[]): string {\n  // Let's wrap each name with the '_' character\n  const formatName = (s: string) =&gt; `_${s}_`;\n\n  const mapper = (p: Pair&lt;GetOrganizationMemberResponse&gt;): string =&gt;\n    // and start each pair with '-'\n    `- ${formatName(p.first.login)} meets with ${formatName(p.second.login)}${\n      p.third ? ` and ${formatName(p.third.login)}` : \"\"\n    }`;\n  return pairs.map(mapper).join(\"\\n\");\n}\n</code></pre> <p>By making this small change, we now see the following message:</p> <p></p> <p>The messaging is better, but we're still missing some clarity. For example, what date is this for? Or what's the purpose of the message? Looking through these docs, it seems like we could add different text blocks (like titles). So let's see what this could look like.</p> <p>One design approach is to encapsulate the complex logic for dealing with Slack and only expose a \"common-sense\" API for consumers. In this regard, I think using a Facade pattern would make sense.</p> <p>We want to expose the ability to set a title and to set a message through one or more lines of text. Here's what that code would look like</p> slack.ts<pre><code>// This class allows a user to set a title and lines and then use the\n// 'build' method to create the payload to interact with Slack\n\nclass MessageFacade {\n  // setting some default values\n  private header: string;\n  private lines: string[];\n  constructor() {\n    this.header = \"\";\n    this.lines = [];\n  }\n\n  // I like making these types of classes fluent\n  // so that it returns itself.\n  public setTitle(title: string): MessageFacade {\n    this.header = title;\n    return this;\n  }\n  public addLineToMessage(line: string | string[]): MessageFacade {\n    if (Array.isArray(line)) {\n      this.lines.push(...line);\n    } else {\n      this.lines.push(line);\n    }\n    return this;\n  }\n\n  // Here's where we take the content that the user provided\n  // and convert it to the JSON shape that Slack expects\n  public build(): string {\n    // create the header block if set, otherwise null\n    const headerBlock = this.header\n      ? {\n          type: \"header\",\n          text: { type: \"plain_text\", text: this.header, emoji: true },\n        }\n      : null;\n    // convert each line to it's own section\n    const lineBlocks = this.lines.map((line) =&gt; ({\n      type: \"section\",\n      text: { type: \"mrkdwn\", text: line },\n    }));\n    return JSON.stringify({\n      // take all blocks that have a value and set it here\n      blocks: [headerBlock, ...lineBlocks].filter(Boolean),\n    });\n  }\n}\n</code></pre> <p>With the facade in place, let's look at implementing this in <code>index.ts</code></p> index.ts<pre><code>// ... code to get the pairs and formatted lines\n\n// using the facade with the fluent syntax\nconst message = new MessageFacade()\n  .setTitle(`\u2615 Random Coffee \u2615`)\n  .addLineToMessage(formattedPairs)\n  .build();\n\nawait sendMessage(message);\n</code></pre> <p>When we run the script now, we get the following message:</p> <p></p>"},{"location":"articles/2023/07/24/having-coffee-with-deno---sharing-the-news/#wrapping-up","title":"Wrapping Up","text":"<p>In this post, we changed our script from posting its Random Coffee message to the console window to instead posting it into a Slack channel using an Incoming Webhook. By making this change, we were able to remove a manual step (e.g., us copying the message into the channel), and we were able to use some cool emojis and better formatting.</p> <p>In the final post, we'll take this one step further by automating the script using scheduled jobs via GitHub Actions.</p> <p>As always, you can find a full working version of this bot on my GitHub.</p>"},{"location":"articles/2023/07/24/having-coffee-with-deno---sharing-the-news/#other-posts-in-the-series","title":"Other Posts In The Series","text":"<ul> <li>Having Coffee with Deno - Inspiration</li> <li>Having Coffee with Deno - Dynamic Names</li> <li>Having Coffee with Deno - Automating All the Things</li> </ul>"},{"location":"articles/2023/07/28/having-coffee-with-deno---automating-all-the-things/","title":"Having Coffee with Deno - Automating All the Things","text":"<p>Welcome to the final installment of our Deno series, where we build a script that pairs up people for coffee.</p> <p>In the last post, we added the ability to post messages into a Slack channel instead of copying from a console window.</p> <p>The current major problem is that we have to remember to run the script. We could always set up a cron job or scheduled task, however, what happens when we change machines? What if our computer stops working? What if someone else changes the script, how will we remember to get the latest and run it?</p> <p>It'd be nice if we had a centralized location that could run our script on a schedule. When it ran, it'd grab the latest version of the script and execute it. No more need for it to be on our machine.</p> <p>Wait a minute; we have a tool for this, our Continuous Integration (CI) server. Whether you're using tools like Jenkins, TeamCity, Azure DevOps, GitHub Actions, or CircleCI, at the end of the day, these tools are doing the following:</p> <ol> <li>Start when the trigger happens (e.g., changes are merged or when a certain time happens).</li> <li>Get the latest version of code from source control.</li> <li>Run one or more steps.</li> </ol> <p>The fun thing about this is that no one said that CI couldn't execute side effects like kicking off scripts. In this post, let's take a look at how we can setup a GitHub Action to execute our script.</p>"},{"location":"articles/2023/07/28/having-coffee-with-deno---automating-all-the-things/#the-approach","title":"The Approach","text":"<p>For the script to post to Slack, we'll need to make the following changes:</p> <ol> <li>Add our secrets/variables to the repository so that our Action can access them.</li> <li>Creating the Action.</li> <li>Setup a schedule for when the script should run.</li> </ol>"},{"location":"articles/2023/07/28/having-coffee-with-deno---automating-all-the-things/#adding-secrets-to-the-repository","title":"Adding Secrets to the Repository","text":"<p>In our <code>.env</code> file, we have two secrets, our <code>GITHUB_BEARER_TOKEN</code>, which is used to get members of the organization and <code>SLACK_WEBHOOK</code>, the URL to post our messages to.</p> <p>Since these values are secrets, we'll need to follow these docs to add them to the repository level.</p> <p>There are additional places we could place these secrets (in an environment or at an organizational level). For now, we'll keep at the repository level.</p> <p>While adding your secrets, you'll note that we can't have a secret start with <code>GITHUB</code> (this is forbidden according to the docs). We'll rename this to <code>GH_BEARER_TOKEN</code> for now (we'll see later why renaming isn't a big deal).</p> <p>Once we've added our secrets, we should have a screen that looks like the following:</p> <p></p> <p>With this done, let's work on creating our GitHub Action.</p>"},{"location":"articles/2023/07/28/having-coffee-with-deno---automating-all-the-things/#creating-the-github-action","title":"Creating the GitHub Action","text":"<p>In the root of our repository, let's create the <code>.github</code> folder followed by <code>workflows</code> inside of it. From there, we'll create <code>run-script.yml</code> with the following:</p> run-script.yml<pre><code>name: Run Random Coffee\n\n# This is the trigger for the build and there quite a few options\n# Details can be found at https://docs.github.com/en/actions/using-workflows/events-that-trigger-workflows\non:\n  workflow-dispatch: # Execute on demand - useful for testing\n\njobs:\n  run-script:\n    runs-on: ubuntu-latest # using Ubuntu as the base image\n    steps:\n      # Installing Deno on the container\n      # Instructions can be found at https://github.com/denoland/setup-deno\n      - name: Setup Deno\n        uses: denoland/setup-deno@61fe2df320078202e33d7d5ad347e7dcfa0e8f31 # v1.1.2\n        with:\n          deno-version: v1.x\n\n      # Ensures that we have deno installed an on the path\n      - name: Check Deno Version\n        run: \"deno --version\"\n\n      # Checkout the repository\n      - name: Checkout Repo\n        uses: actions/checkout@v3\n\n      # Execute the Deno script\n      - name: Run script\n        run: |\n          deno run --allow-read --allow-net --allow-env ./index.ts\n        # Here's where we can pass in the secrets from GitHub to the script\n        env:\n          GITHUB_BEARER_TOKEN: ${{ secrets.GH_BEARER_TOKEN }}\n          SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK }}\n</code></pre> <p>With these changes, we can go to our Actions and kick this off manually. If everything works correctly, we should see our message get posted!</p>"},{"location":"articles/2023/07/28/having-coffee-with-deno---automating-all-the-things/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>If you receive a 401, it's most likely the GITHUB_BEARER_TOKEN is not being set. Make sure that you added your bearer token as a secret for the repository and that you're using <code>${{ secrets.GH_BEARER_TOKEN }}</code> to refer to it.</li> <li>If you receive a 404, it's most likely that the SLACK_WEBHOOK is not being set. Make sure that you added your webhook as a secret for the repository and that you're using <code>${{ secrets.SLACK_WEBHOOK }}</code> to refer to it.</li> </ul>"},{"location":"articles/2023/07/28/having-coffee-with-deno---automating-all-the-things/#creating-the-schedule","title":"Creating the Schedule","text":"<p>Now that we have the script working, our last change is to add another trigger so that our script runs on a schedule.</p> <p>GitHub uses cron syntax for scheduling, so the times will be in UTC (consider this when determining your time).</p> <p>I live in the Eastern Time Zone, so let's schedule our message to run at 8 am on Mondays.</p> <p>(If you're not familiar with cron syntax, I highly recommend checking out crontab, an online tool to help validate your syntax, it's terrific!)</p> <p>Since we want to make sure that our heroes have time to meet up during the week, let's schedule this for 8 am on Mondays.</p> run-script.yml<pre><code>name: Run Random Coffee\n\n# This is the trigger for the build and there quite a few options\n# Details can be found at https://docs.github.com/en/actions/using-workflows/events-that-trigger-workflows\non:\n  workflow-dispatch: # Execute on demand - useful for testing\n  schedule:\n    - cron: \"0 12 * * 1\" # Executes on Mondays at 12pm UTC\n\n# Rest of the action file...\n</code></pre> <p>Nice! At this point, our script will run at 8 am on Mondays (and sometimes at 7 am on Mondays, yay, Daylight Savings Time!). With this final piece, we've completed our Random Coffee script for the Justice League.</p> <p></p> Credit to How It Should Have Ended for the image and video"},{"location":"articles/2023/07/28/having-coffee-with-deno---automating-all-the-things/#wrapping-up","title":"Wrapping Up","text":"<p>In this series, we created our first bit of automation using Deno and TypeScript, pairing up the members of the Justice League. We learned how to make web requests, how Deno's permission scheme works, creating a Slack Webhook, and how to execute our scripts on a schedule using GitHub Actions.</p> <p>You can find the full working version of this repo on my GitHub.</p>"},{"location":"articles/2023/07/28/having-coffee-with-deno---automating-all-the-things/#other-posts-in-the-series","title":"Other Posts In The Series","text":"<ul> <li>Having Coffee with Deno - Inspiration</li> <li>Having Coffee with Deno - Dynamic Names</li> <li>Having Coffee with Deno - Sharing the News</li> </ul>"},{"location":"articles/2015/10/13/implementing-your-own-version-of-fs-listfilter/","title":"Implementing Your Own Version of F#\u2019s List.Filter","text":"<p>As I\u2019ve been thinking more about F#, I began to wonder how certain methods in the F# stack work, so I decided to implement F#\u2019s List.filter method.</p> <p>For those of you who aren\u2019t familiar, List.Filter takes a function that returns true or false and a list of values. The result of the call is all values that fulfill the fuction.</p> <p>For example, if we wanted to keep just the even numbers in our list, then the following would accomplish that goal.</p> <pre><code>let values = [1;2;3;4]\nlet isItEven x = x % 2 = 0\n\n\nlet evenValues = List.filter isItEven values\n// val it : int list = [2; 4]\n</code></pre> <p>Now that we know the problem, how would we begin to implement? First, we need to define a function called filter:</p> <pre><code>let filter () =\n</code></pre> <p>However, to match the signature for List.filter, it needs to take a function that maps integers to bools and the list of values to work on</p> <pre><code>let filter (func:int-&gt;bool) (values:int List) =\n</code></pre> <p>Now that we have the signature, let\u2019s add some logic to match on the list of values. When working with lists, there are two possibilities, an empty list and a non-empty list. Let\u2019s first explore the empty list option.</p> <p>In the case of an empty list of values, then it doesn\u2019t matter what the func parameter does, there are no possible results, so we should return an empty list for the result.</p> <pre><code>let filter (func:int-&gt;bool) (values:int List) =\n    match values with\n    | [] -&gt; []\n</code></pre> <p>Now that we\u2019ve handled the empty list, let\u2019s explore the non-empty list scenario. In this branch, the list must have a head and a tail, so we can deconstruct the list to follow that pattern.</p> <pre><code>let filter (func:int-&gt;bool) (values:int List) =\n    match values with\n    | [] -&gt; []\n    | head::tail -&gt; // what goes here?\n</code></pre> <p>Now that we\u2019ve deconstructed the list, we can now use the func parameter with the head element. If the value satisfies the func parameter, then we want to add the head element to the list of results and continue processing the rest of the list. To do that, we can use recursion to call back into filter with the same func parameter and the rest of the list:</p> <pre><code>let rec filter (func:int-&gt;bool) (values:int List) =\n    match values with\n    | [] -&gt; []\n    | head::tail -&gt; \n         if func head then head :: filter func tail\n</code></pre> <p>At this point, we need to handle the case where the head element does not satisfy the func parameter. In this case, we should not add the element to the list of results and we should let filter continue the work</p> <pre><code>let rec filter (func:int-&gt;bool) (values:int List) =\n    match values with\n    | [] -&gt; []\n    | head::tail -&gt; \n         if func head then head :: filter func tail\n         else filter func tail\n</code></pre> <p>By handling the base case first (an empty list), filter can focus on the current element in the list (head) and then recurse to process the rest of the list. This solution works, but we can make this better by removing the type annotations. Interestingly enough, we don\u2019t care if we\u2019re working with integers, strings, or whatever. Just as long as the function takes some type and returns bool and the list of values matches the same type as the func parameter, it works. So then we end up with the following:</p> <pre><code>let rec filter func values =\n    match values with\n    | [] -&gt; []\n    | head::tail -&gt; if func head then head :: filter func tail else filter func tail\n</code></pre> <p>In general, when working with lists, I tend to start by matching the list with either an empty list or non-empty. From there, I\u2019ve got my base case, so I can focus on the implementation for the first element. After performing the work for the first element, I can then recurse to the next element.</p>"},{"location":"articles/2024/08/15/an-alternative-approach-to-sprint-planning---introducing-hippogriff/","title":"An Alternative Approach to Sprint Planning - Introducing Hippogriff","text":""},{"location":"articles/2024/08/15/an-alternative-approach-to-sprint-planning---introducing-hippogriff/#background","title":"Background","text":"<p>One of my first jobs in engineering was working for a medical device company. This is was pretty cool as I wrote software that interacted with the device to show measurements and give recommendations to the physicians about those measurements.</p> <p>To ensure that things were working correctly, we had a department called V&amp;V (Validation and Verification). I had never heard of this term before in my career, so my boss told me that it was responsible for ensuring that we built the right thing because it's engineering's job to build it right.</p> <p>These two principles (build the right thing and build it right) have stuck with me during my career. So much so, I believe this may have been the start of my interest in product engineering and wanting to understand the \"why\" behind the stuff that I build.</p> <p>It was the same job that I was introduced to the concept of Kanban, and the idea of eliminating waste from our process as it inherently slows us down by focusing on the wrong things. I'm known to be a process improver at heart and the idea of Kaizen (continuous improvement) resonates with me.</p> <p>So when I think about how many teams are tackling work breakdown and estimating, I can't help but think that we're spending our time on the wrong things and not enough on the right things.</p>"},{"location":"articles/2024/08/15/an-alternative-approach-to-sprint-planning---introducing-hippogriff/#my-experiences-with-engineering-teams-today","title":"My Experiences with Engineering Teams Today","text":"<p>A common piece of feedback with teams that are following Scrum principles is that they feel like they are in meetings all the time and there's no time to do the actual work. As someone who has a love/hate relationship with meetings, this is totally understandable.</p> <p>There are a set number of meetings (\"activities\" in Scrum parlance) that teams follow, one of which is sprint planning, a time to make sure that what the team is working on is aligned with priorities from product.</p> <p>While I find value in the synchronization with product, I don't find very much value in the estimation portion of the meeting as it gets the focus on the wrong things.</p> <p>If we look at the value that estimates provide, the goal is to ensure that the team is taking on a reasonable load for the sprint (e.g., what are our commitments) and not over-extending as that can cause burn-out.</p> <p>I have two problems with estimations. First, they're way too easy to be taken as a deadline (so much so, that you've probably seen this meme spread around). Which in turn, causes the team to go deeper into estimating stories (breaking them down to single tasks), which causes a feedback loop. </p> <p>If this keeps up, you'll eventually find yourself going into Waterfall mode where the team needs every single requirement up front before they can do any development, the opposite of what we're trying to do.</p> <p>Instead of the team focusing on estimations, I'd much rather see them strategize on an approach to the problem and let that be the driver to the work.</p> <p>One approach that I've seen teams take is to use \"relative sizing\", so instead of saying that work will take four hours, they might say that it's a \"2\" (if using the Fibonacci approach) or it's a \"Small\" (if using T-Shirt sizing).</p> <p>Side Note: I've also seen fruit and animals as relative sizing options.</p> <p>The problem I have with relative sizing is that it can be helpful for the team, it's total nonsense for business stakeholders. For example, let's say that we're working on the new Shiny Widget 9000 and Marketing wants to know when it's gong to be ready so that they can start getting the marketing materials ready and start promotion. You can't tell them that it's going to take 108 story points or 16 Mediums as that is meaningless. </p> <p>They need a timeline, so what does engineering do? They look up historic trends for the team and say something like \"We typically get 4 Mediums done in 2 weeks, so we're looking at 2 months, give or take\". Which means that we're correlating a relative size to a unit of time. So what value did we gain?</p> <p>When I think about planning, I'm focusing on stories that are independent, deliver value, and can be accomplished within a sprint. I don't necessarily care if the story is a 2, 3, or 5 as long as the team has a rough approach to the work and understands why we need the functionality.</p>"},{"location":"articles/2024/08/15/an-alternative-approach-to-sprint-planning---introducing-hippogriff/#a-different-approach","title":"A Different Approach","text":"<p>Instead of spending time on estimates, what if we approached planning this way?</p> <ol> <li>Product and Engineering work together to break priorities down to smaller items that can be accomplished in a sprint (remembering to keep them independent).</li> <li>Team works together to move the stories in priority order (taking into account dependencies).</li> <li>Team takes on the first item.</li> <li>Product and Engineering can move/re-prioritize items as needed, but can't modify work in flight.</li> <li>As an item is completed, the team takes on the next important item.</li> </ol> <p>This focuses the meeting on the important things (what's important for us to work on, how would we approach it) and less on the things that don't matter (fine-tuned estimations).</p> <p>For those keeping track, this sounds pretty similar to Kanban (which it absolutely is \ud83d\ude00), but if you're looking for a catchy name, you can call this the Highest Priority Goes First ( or Hippogriff) framework.</p> <p>Long story short, I'd like to see teams spending less time on estimations and more time on figuring out what the problem is, solutioning, getting feedback, and iterating.</p> <p>Isn't that what the Agile Manifesto is all about?</p>"},{"location":"articles/2024/08/15/an-alternative-approach-to-sprint-planning---introducing-hippogriff/#recommended-reading","title":"Recommended Reading","text":"<p>While working on this post, I found this article by Mountain Goat Software to be immensely helpful in capturing the goals of sprint planning and some external validation that I'm not the only one who's experienced this problem as well.</p>"},{"location":"articles/2024/06/18/leadership-playbook---leading-through-change/","title":"Leadership Playbook - Leading Through Change","text":"<p>There are few fundamental truths in life, one of which is that the only consistent thing is change. Whether that's through a reorganization, someone leaving the team, or the start of a new initiative, we know that what happens today is different from yesterday, and as a leader, the team is looking to you to figure out how to navigate these changes.</p> <p>In this post, I'll share some tips and tricks for leading the team through change.</p>"},{"location":"articles/2024/06/18/leadership-playbook---leading-through-change/#explain-the-reasoning","title":"Explain the Reasoning","text":"<p>A common mistake I see leaders make is introducing a change without discussing why the change is happening. Let's look at a hypothetical situation where you're introducing the team to pull request templates.</p> <p>Hey team! Starting next sprint, we're going to start using this new PR template. You can find it ....</p> <p>On the one hand, the message is clear on what's changing (using new PR template). However, it completely missed the point, why are we making this change? When we don't include the why, we catch people off-guard because they may not immediately understand the problem that the change is supposed to solve.</p> <p>When we put people in an information vacuum, they draw their own conclusions, which can give the wrong impression behind the change. This in turn, can cause the rumor mill to go into overdrive, making your job much harder.</p> <p>Let's revisit the same scenario but include the \"why\" this time.</p> <p>Hey team! During our last retro, it was brought up that our pull requests descriptions aren't consistent, which makes reviewing them more difficult. To help build consistency, we're going to start using pull request templates. You can find it ....</p> <p>By making this small change, we can squash misinformation and potential rumor mills because we're clear on the reasoning.</p> <p>Which is a great segue to...</p>"},{"location":"articles/2024/06/18/leadership-playbook---leading-through-change/#be-transparent-with-the-team","title":"Be Transparent With the Team","text":"<p>When explaining the reason, don't lie or sugarcoat the reasoning, even if it makes you feel uncomfortable. Your team is smart and they'll know if you're lying to them.</p> <p>A hot topic nowadays are Return to Office (RTO) plans, with a common reason being \"need more in-person collaboration.\" Even though this provides a \"why,\" it's not backed by metrics or anything measurable. In addition, in-person doesn't necessarily mean more collaboration.</p> <p>A better approach is to use employee engagement surveys or customer satisfaction surveys to measure the effectiveness of team or company. If you can't use these metrics (or other relevant metrics), then why are you introducing this change?</p> <p>Going back to our hypothetical RTO, let's say that the reason we're going back to the office is that we're a start-up whose investors have already paid for our space. To them, having people in the space helps them feel better that their money is well spent (in addition, some companies have a tax break if they moved to your state as long as they have a percentage of people on-site).</p> <p>This is what you should be telling your team. They may not like the reason and they might disagree, but they know the why and then they can make their own decisions. Long story short, you're giving them the autonomy to make their own decisions because they have all the information.</p>"},{"location":"articles/2024/06/18/leadership-playbook---leading-through-change/#repetition","title":"Repetition","text":"<p>Humans don't have perfect memories, right? So why would we expect that once we introduce a change that's the last time we need to talk about it? </p> <p>Regardless of the change, don't be surprised if you need to mention it 3 or 4 times before people finally start understanding and applying the change. A great mentor of mine once told me that he would tell people about the change time and time again until it stuck. While this was happening, he would show patience, repeat the messaging, and answer questions and concerns.</p> <p>When we hear of change, our first step is start processing the message and the immediate impacts. Some people will have questions immediately while others need time to stew on it. </p> <p>Because of this, be available to answer questions as they come up (even if you've already answered them before). Be prepared for questions during one-on-ones, after meetings, or whenever they come up. </p> <p>Acknowledge the questions and answer them. If you don't know the answer, tell them that you don't know and that you're going to find the answer. </p>"},{"location":"articles/2024/06/18/leadership-playbook---leading-through-change/#giving-space","title":"Giving Space","text":"<p>Don't be surprised that your team exhibits a wide range of emotions for more significant changes. </p> <p>For example, if your company is doing layoffs, then it's reasonable for people to be upset, depressed, or mentally checked out. </p> <p>When this happens, you have to give people space to process. This doesn't mean isolating them but being aware that they need some time and accommodate accordingly.</p> <p>A common mistake I see leaders make here is introducing a change, thinking it was low impact, so they start making other changes. In reality, this change had a high impact, and now the team is under pressure to handle the original change and whatever new commitments are coming their way.</p> <p>Not only does this reduce your odds of success, but this prevents your team from dealing with the changes, which can turn into stress or frustration. If this happens enough times, people will change teams (or even jobs) just to get a change of scenery and be able to process.</p>"},{"location":"articles/2024/06/18/leadership-playbook---leading-through-change/#reducing-change-fatigue","title":"Reducing Change Fatigue","text":"<p>Even though changes are going to happen, do not introduce one change this week, another next week, and then one more two weeks later.</p> <p>As engineers, we learned that refactoring a codebase should be done in small steps to prevent functionality from breaking. </p> <p>While this works great for code, this is terrible advice for humans. </p> <p>When frequent changes happen, it becomes difficult to get into a rhythm with the work and the team, reducing the effectiveness of the team.</p> <p>When we get changes happening like this, it becomes difficult to get into a rhythm with the work and the team. Especially if large changes keep happening every few weeks.</p> <p>In the current landscape, we're seeing companies go through multiple rounds of layoffs. While this may keep them out of the news (no one reports that a company laid off ten people, even if it's the fourth time it's happened this year), it causes a feeling of dread for the survivors, as now they're thinking when they'll be next.</p> <p>Instead of having multiple layoffs, if companies had one (albeit larger) layoff, this would allow people to have time to adjust and proceed without as much paranoia.</p>"},{"location":"articles/2024/06/18/leadership-playbook---leading-through-change/#wrapping-up","title":"Wrapping Up","text":"<p>Navigating change is hard - especially when you aren't just going through the change yourself, but also leading others at the same time. </p> <p>The next time you're going through a change, try starting here and see how it goes</p> <ul> <li>Explain the \"why\" behind the change.</li> <li>Be transparent about the changes, even if it's uncomfortable.</li> <li>Be mindful that everyone processes change differently and may feel a greater impact or need more time to recover from teh change.</li> </ul> <p>By starting here, your chances of moving forward without significant disruption will improve dramatically.</p>"},{"location":"articles/2024/04/16/learning-from-failures-leveraging-postmortems-for-good/","title":"Learning From Failures: Leveraging Postmortems for Good","text":"<p>As engineers, we love solving problems and digging into the details. I know that I feel a particular sense of joy when shipping a system to production for the first time.</p> <p>However, once a system goes into production, it's going to fail. That's not a knock on engineering, that's a fact of our industry. We cannot build a system with 100% uptime, no matter how much we plan ahead or think about.</p> <p>When this happens, our job is to fix the issue and bring the systems back up.</p> <p>A common mistake that I see teams make is that they'll fix the problem, but never dig into the why it happened. Fast forward three months, and the team will run into the same problem, making the same mistakes. I'm always surprised when teams don't share their knowledge because if you've paid the tax of learning from the first outage, why would you pay the same tax to learn the lesson again?</p> <p>This is where having a postmortem meeting comes into play. Borrowing from medicine, a postmortem is performed when the team gets together to analyze the outage and what could have been done differently to prevent it from happening. Other industries have similar mechanisms (for example, professional athletes review their games to learn where they made mistakes so that they can train differently).</p> <p>One of the key concepts of the postmortem is that the goal isn't to assign blame, but to understand what happened and why. People aren't perfect and it's not reasonable for them to be. This concept is so fundamental that another term you might here for a postmortem is a blameless incident report (BIR).</p> <p>Are you interesting learning how to run your own BIR process? Drop me a line and if there's enough interest, I'll write a follow-up post!</p>"},{"location":"articles/2024/04/16/learning-from-failures-leveraging-postmortems-for-good/#getting-started","title":"Getting Started","text":"<p>Every company has their own process when they have an outage, but health process should be able to answer these five questions at a minimum.</p> <ol> <li>What stopped working?</li> <li>Why did it stop working?</li> <li>How did we fix it?</li> <li>What led to the system breaking?</li> <li>What are we doing to prevent this issue from happening in the future?</li> </ol> <p>Organizations may have additional questions for their process, for example</p> <ul> <li>What was the impact? (X customers were impacted for Y minutes)</li> <li>How was this discovered? (Was it user reported, support found it, our monitoring tools paged on-call)</li> </ul> <p>You can always make a process more complicated, but it's hard to simplify a process, so my recommendation is start with the 5 questions and then expand as your team evolves and matures.</p>"},{"location":"articles/2024/04/16/learning-from-failures-leveraging-postmortems-for-good/#what-stopped-working","title":"What Stopped Working?","text":"<p>For there to be an outage, something had to stop working, right? So what was it? It's okay/normal to be a bit technical here, however, don't forget that this outage caused an impact to our users, so we should strive to explain the outage in those terms.</p> <p>Another way to think about this is \"What stopped working and what impact did it have for our users?\"</p> <p>Here's a not-great answer to the question</p>    The Data Sync Java container stopped working.  <p>While this does capture what stopped working, the details are quite vague. For example, did it not start? Did it start crashing? Was it the whole process that failed or only one part?</p> <p>In addition, there's not a clear delineation of what the user impact was. For example, could users not log into the application at all? Were there certain parts of the application that stopped working?</p> <p>We can improve this by including a bit more details in what specifically stopped working.</p>    The Data Sync process was failing to connect to the UserHistory database.  <p>Ah, okay, now we know that the sync process wasn't able to connect to a specific database and we can start getting a sense of why that would be a big deal. We're still not including the user impact, so let's add that bit of detail in.</p>    The Data Sync process was failing to connect to the UserHistory database. As such, when users logged into their account, they could not see the latest transactions for their account.  <p>Much better, now I know that our users couldn't see recent transactions and it was something to do with the Data Sync process.</p> <p>As a side benefit, if I'm a new engineer to this codebase or team, I know know more about the architecture and that this process is involved for when users start transactions.</p>"},{"location":"articles/2024/04/16/learning-from-failures-leveraging-postmortems-for-good/#why-did-it-stop-working","title":"Why Did It Stop Working?","text":"<p>At some point, the system was working and if it's no longer working, something had to have happened, so what was it? Was there a code deployment to production? Did a feature flag get toggled that had an adverse effect? What about infrastructure changes like DNS entries or firewall?</p> <p>This is a key critical step because if we don't know why it stopped working, then we don't have a good spot to start when we start diving into the circumstances behind what led to the outage.</p> <p>This doesn't have to be a page worth of technical deep dive, a couple of sentences can suffice here. In our outage, the issue was due to a port being blocked by the firewall (where it wasn't before).</p>    There was an infrastructure change for the database that blocked port 1433, which is the default port for the database. Because of this change, no application was able to successfully connect to the database."},{"location":"articles/2024/04/16/learning-from-failures-leveraging-postmortems-for-good/#how-did-we-fix-it","title":"How Did We Fix It?","text":"<p>If you've gotten to the point of writing the BIR, then you've fixed the issue and the system is up and running again, right? So what did you do to fix the issue? Did you rollback the deployment? Disable a feature flag? Burn down the application, change your name and get a new job? This is a cool part of the BIR because you're leveling up others that if they run into a similar issue, here's how you were able to get back up and running.</p> <p>In our example, we were able to unblock the port, so we can answer this question with:</p>    Once we realized that port 1433 was being blocked by the firewall, we worked with the Infrastructure team to unblock the port. Once that change was completed, the Data Sync service was able to start syncing data to the UserHistory database."},{"location":"articles/2024/04/16/learning-from-failures-leveraging-postmortems-for-good/#what-led-to-the-system-breaking","title":"What Led To the System Breaking?","text":"<p>This is where the meat of the conversation should take place. In a healthy organization, we assume that people are wanting to do the right thing (if not, you have a much bigger problem that BIRs). So we've got to figure out how did we get here, what were the motivations and why did we do the things that we did?</p> <p>One common approach is 5 Whys, made popular by the Toyota Production System. The idea is that we keep digging into why something happened and not stop at symptoms.</p> <p>An example 5 Whys breakdown for this outage could be the following:</p> <pre><code>- Why did the Data Sync service started failing to connect to the UserHistory database?\n    - Because the port that the Data Sync service was communicating with got blocked\n    - Why did the port become blocked?\n        - One of our security initiatives is to block default ports to lessen the changes of someone gaining access to our systems\n        - Why is this an initiative?\n            - Our current firewall solution doesn't support a way to have an _allowList_ of dynamic IP addresses. Since most vulnerability tools scan a network, they'll typically use default ports to see if there's a service running there and if so, try to compromise it.\n            - Why doesn't our current firewall solution have support for dynamic IP addresses?\n    - Why did we not see this issue in the lower environments?\n        - The lower environments are not configured the same as our production environment\n        - Why are these environments different?\n            - Given that lowers receive less traffic than production, we have multiple databases installed on the same server, none of which are on the default port. By doing this, we're saving money on infrastructure costs.\n        - Why did the team not realize that the lowers are configured differently?\n            - The Data Sync process is an older part of our application that most of the team doesn't have knowledge of.\n    - Why did our monitoring tools not catch the issue after deployment?\n        - For the Data Sync process, we currently only have a health check, which only checks to see if the app is up, it doesn't check that it has line-of-sight to all its dependencies.\n        - Why doesn't the health check include dependency checking?\n            - Health checks are used to tell our cloud infrastructure to restart a service. Since restarting the service wouldn't have resolved the problem, that's why we don't have it included in the checks\n        - Why don't we have other checks?\n            - The Data Sync process predates our existing monitoring solutions and has been stable, so the work was never prioritized.\n</code></pre> <p>As you can see, this approach brings up lots of questions, including the motivation behind the work and why the team was doing it anyway. It is possible</p>"},{"location":"articles/2024/04/16/learning-from-failures-leveraging-postmortems-for-good/#what-are-we-doing-to-prevent-similar-issues-in-the-future","title":"What Are We Doing To Prevent Similar Issues in the Future?","text":"<p>The system is going to have an outage, that's not up for debate. However, it would be foolish to have an outage and not do anything to prevent it from happening in the future. If we already paid the learning tax once, let's not pay it again for the same issue.</p> <p>This should be a concrete list of action items that the team takes ownership of. In some cases, it's work that they can do to prevent the issue going forward. In some cases, it could be working with others to help them fix things in their process.</p> <p>In our hypothetical outage, we could have the following action items</p> <ul> <li>Add Additional Monitoring for the Data Sync process</li> <li>Work with Security to determine approach for securing our database instance</li> <li>Create environment diagram for Data Sync process</li> <li>Create architecture diagram for Data Sync process</li> </ul>"},{"location":"articles/2024/04/16/learning-from-failures-leveraging-postmortems-for-good/#example-blameless-incident-report","title":"Example Blameless Incident Report","text":"<p>In this post, we covered the 5 main questions to answer for a BIR and what good responses look like. In this section, I wanted to go over an example BIR for the database issue that we've been exploring. As you'll see, it's not a verbose document, however, it does capture the main points and this is easily consumable for other teams to learn from our mistakes.</p> <pre><code># Title: Users Unable To See Latest Transactions\n\n##  What Stopped Working?\nThe Data Sync process stopped being able to connect to the UserHistory Database. Because of this failure, when users logged into their account to see transactions, they were not able to see any new transactions.\n\n## Why Did It Stop Working?\n\nA change was made to the database infrastructure to block port 1433. This is the default port for a SQL Server database so when it was blocked, no application was able to communicate with the database.\n\n## How Did We Fix It?\n\nThe firewall port change was reverted.\n\n## What Led to the System Breaking?\n\nTo improve our security posture, we wanted to block default ports to our systems so that if someone was to gain access, they couldn't \"guess\" into the connection for the database.\n\nWhen making these firewall changes, we start in the lower environments so that if there is a problem, we impact dev or staging and not production.\n\nUnknown to the team, in the lower environments, we have multiple databases installed on a server, none of which are on port 1433. Because of this, we had false confidence that our changes were safe to deploy forward.\n\nIn production, each database has their own server, running on port 1433.\n\n##  What Are We Doing To Prevent Issues In The Future?\n\n- **Check the environment differences** - Before making infrastructure changes, we're going to check what the differences are in our lower environments vs production.\n- **Create architecture diagram** - Since one of the main issues is that the team didn't understand the architecture of the Data Sync process, we're going to create an architecture diagram that covers the flow of the service.\n- **Create environment diagram** - To better understand our system, we're going to create an environment diagram that covers the databases at play and how the Data Sync process communicates.\n- **Work with Security on Approaches for Securing Database** - We'll work with the Security team to either setup a way to have dynamic IPs work with our firewall technology or to change our Data Sync process to have a static IP.\n</code></pre>"},{"location":"articles/2020/06/16/mars-rover--implementing-rover--moving-backward/","title":"Mars Rover \u2013 Implementing Rover : Moving Backward","text":"<p>Welcome to the sixth installment of Learning Through Example \u2013 Mars Rover! In this post, we\u2019ll pick up from where we left off with <code>Rover</code> and start implementing the rules for the <code>Rover</code> to move backward!</p> <p>Just like last time, we\u2019ll first examine what it means for the <code>Rover</code> to move backward by looking over the requirements in deeper detail. Once we have a better understanding, we\u2019ll start driving out the functionality by focusing on a simple case and building more complexity. By the end of this post, we\u2019ll have a <code>Rover</code> that will know how to move backward when facing any <code>Direction</code>!</p>"},{"location":"articles/2020/06/16/mars-rover--implementing-rover--moving-backward/#so-what-does-it-mean-to-move-backward","title":"So What Does It Mean To Move Backward?","text":"<p>If we look back at the original requirements for moving backward, we find this single line with regards to moving backward</p> <p>When the rover is told to move backward, then it will move rover unit away from the direction it\u2019s facing</p> <p>Hmm, based on our previous experience with implementing <code>MoveForward</code>, I have a good idea of the requirements and after double-checking with our Subject Matter Expert, we confirm that our assumption is correct and derive the following requirements.</p> <ul> <li>Given the <code>Rover</code> is facing North, when it moves forward, then its <code>Y</code> value decreases by 1</li> <li>Given the <code>Rover</code> is facing South, when it moves forward, then its <code>Y</code> value increases by 1</li> <li>Given the <code>Rover</code> is facing East, when it moves forward, then its <code>X</code> value decreases by 1</li> <li>Given the <code>Rover</code> is facing East, when it moves forward, then its <code>X</code> value increases by 1</li> </ul> <p>Great, we have enough information to get started so we can start demonstrating the software and get quicker feedback!</p>"},{"location":"articles/2020/06/16/mars-rover--implementing-rover--moving-backward/#redgreenrefactor-for-rover-facing-north","title":"Red/Green/Refactor For Rover Facing North","text":"<p>Given the lessons learned when writing tests for when implementing move forward, we can write the following test.</p> <pre><code>[Test]\npublic void AndFacingNorthThenYDecreasesByOne()\n{\n  var rover = new Rover { Orientation = Direction.North };\n  var initialLocation = rover.Location;\n\n  rover.MoveBackward();\n\n  var expectedLocation = new Coordinate(initialLocation.X, initialLocation.Y - 1);\n  Assert.AreEqual(expectedLocation, rover.Location);\n  Assert.AreEqual(Direction.North, rover.Orientation);\n}\n</code></pre> <p>Now let\u2019s write just enough code to pass.</p> <pre><code>public void MoveBackward()\n{\n  Location=Location.AdjustYBy(-1);\n}\n</code></pre> <p>Not too shabby, we\u2019ve got enough code in place to make everything pass so let\u2019s take a look at refactoring. From the business rules, <code>MoveBackward</code> is too simple to need a refactor, though I have a suspicion about what the final look of the method will look like. From the test code, the test is pretty straightforward and looks a ton like the tests we wrote for <code>MoveForward</code>.</p> <p>With all of that in mind, let\u2019s go ahead and commit these changes and go to the next requirement!</p>"},{"location":"articles/2020/06/16/mars-rover--implementing-rover--moving-backward/#redgreenrefactor-for-rover-facing-south","title":"Red/Green/Refactor For Rover Facing South","text":"<p>Once again, we can write a failing test for when the <code>Rover</code> faces <code>South</code></p> <pre><code>[Test]\npublic void AndFacingSouthThenYIncreasesByOne()\n{\n  var rover = new Rover { Orientation = Direction.South };\n  var initialLocation = rover.Location;\n\n  rover.MoveBackward();\n\n  var expectedLocation = new Coordinate(initialLocation.X, initialLocation.Y + 1);\n  Assert.AreEqual(expectedLocation, rover.Location);\n  Assert.AreEqual(Direction.South, rover.Orientation);\n}\n</code></pre> <p>And now enough code to make it pass</p> <pre><code>public void MoveBackward()\n{\n  if (Orientation == Direction.North) {\n    Location=Location.AdjustYBy(-1);\n  }\n  Location = Location.AdjustYBy(1);\n}\n</code></pre> <p>Once again, now that we have passing tests, is there anything we want to refactor? The business rules look to be simple enough so I don\u2019t feel the need to refactor those. When we look at the test code, the test seems straightforward and I\u2019m not sure what I\u2019d simplify.</p> <p>Time to commit these changes and on to the next requirement.</p>"},{"location":"articles/2020/06/16/mars-rover--implementing-rover--moving-backward/#redgreenrefactor-for-rover-facing-east","title":"Red/Green/Refactor For Rover Facing East","text":"<p>Third verse same as the first, we can write a failing test for when the Rover faces East</p> <pre><code>[Test]\npublic void AndFacingEastThenXDecreasesByOne()\n{\n  var rover = new Rover { Orientation = Direction.East };\n  var initialLocation = rover.Location;\n\n  rover.MoveBackward();\n\n  var expectedLocation = new Coordinate(initialLocation.X - 1, initialLocation.Y);\n  Assert.AreEqual(expectedLocation, rover.Location);\n  Assert.AreEqual(Direction.East, rover.Orientation);\n}\n</code></pre> <p>And now enough code to make it pass</p> <pre><code>public void MoveBackward()\n{\n  if (Orientation == Direction.North) {\n    Location=Location.AdjustYBy(-1);\n  }\n  if (Orientation == Direction.South) {\n    Location = Location.AdjustYBy(1);\n  }\n  Location = Location.AdjustXBy(-1);\n}\n</code></pre> <p>With everything passing again, we can pause to rethink about refactoring but so far so good from my perspective. There are maybe some superficial changes that could be made, but I can\u2019t make a strong enough argument to implement them.</p> <p>Time to commit and tackle the last requirement!</p>"},{"location":"articles/2020/06/16/mars-rover--implementing-rover--moving-backward/#redgreenrefactor-for-rover-facing-west","title":"Red/Green/Refactor for Rover Facing West","text":"<p>Let\u2019s go ahead and write the final test for when the <code>Rover</code> faces West</p> <pre><code>[Test]\npublic void AndFacingWestThenXIncreasesByOne()\n{\n  var rover = new Rover { Orientation = Direction.West };\n  var initialLocation = rover.Location;\n\n  rover.MoveBackward();\n\n  var expectedLocation = new Coordinate(initialLocation.X + 1, initialLocation.Y);\n  Assert.AreEqual(expectedLocation, rover.Location);\n  Assert.AreEqual(Direction.West, rover.Orientation);\n}\n</code></pre> <p>And now enough code to make it pass</p> <pre><code>public void MoveBackward()\n{\n  if (Orientation == Direction.North) {\n    Location=Location.AdjustYBy(-1);\n  }\n  if (Orientation == Direction.South) {\n    Location = Location.AdjustYBy(1);\n  }\n  if (Orientation == Direction.East) {\n    Location = Location.AdjustXBy(-1);\n  }\n  if (Orientation == Direction.West) {\n    Location = Location.AdjustXBy(1);\n  }\n}\n</code></pre>"},{"location":"articles/2020/06/16/mars-rover--implementing-rover--moving-backward/#how-does-rover-look","title":"How Does Rover Look?","text":"<p>With the final test in place, let\u2019s take a look at <code>Rover</code> looks like!</p> <pre><code>public class Rover\n{\n  public Direction Orientation {get; set;}\n  public Coordinate Location {get; set;}\n\n  public Rover()\n  {\n    Orientation = Direction.North;\n    Location = new Coordinate(){X=0, Y=0};\n  }\n\n  public void MoveForward()\n  {\n    if (Orientation == Direction.North) {\n      Location = Location.AdjustYBy(1);\n    }\n    if (Orientation == Direction.South) {\n      Location = Location.AdjustYBy(-1);\n    }\n    if (Direciton == Direction.East) {\n      Location = Location.AdjustXBy(1);\n    }\n    if (Orientation == Direction.West) {\n      Location = Location.AdjustXBy(-1);\n    }\n  }\n\n  public void MoveBackward()\n  {\n    if (Orientation == Direction.North) {\n      Location=Location.AdjustYBy(-1);\n    }\n    if (Orientation == Direction.South) {\n      Location = Location.AdjustYBy(1);\n    }\n    if (Orientation == Direction.East) {\n      Location = Location.AdjustXBy(-1);\n    }\n    if (Orientation == Direction.West) {\n      Location = Location.AdjustXBy(1);\n    }\n  }\n}\n</code></pre> <p>Overall, <code>Rover</code> is looking to be in a good spot, however, one thing that stands out is that <code>MoveForward</code> and <code>MoveBackward</code> look similar due to their matching <code>if</code> statements. When I see duplication like this, I start thinking about how possible refactoring techniques to reduce the duplication. However, it can be tough to see a pattern before it establishes.</p>"},{"location":"articles/2020/06/16/mars-rover--implementing-rover--moving-backward/#when-to-refactor","title":"When to Refactor","text":"<p>When it comes to refactoring to a pattern, I like to have three or more examples so I have a better idea of what use cases need to be supported. A common mistake I see developers make is that they start implementing a pattern before really understanding the problem. My approach is to refactor to a pattern when it makes sense and not before.</p> <p>Circling back to this duplicated ifs, I\u2019ve got a hunch that <code>TurnLeft</code> and <code>TurnRight</code> will follow a similar approach but I\u2019m curious to know what they\u2019re going to look like and I don\u2019t want to refactor too early. So taking my own advice, I\u2019m going to go ahead and skip refactoring <code>MoveForward</code> and <code>MoveBackward</code> until I at implement <code>TurnLeft</code> as that may change my approach.</p>"},{"location":"articles/2020/06/16/mars-rover--implementing-rover--moving-backward/#wrapping-up","title":"Wrapping Up","text":"<p>Just like that, we now have a <code>Rover</code> that knows how to both <code>MoveForward</code> and <code>MoveBackward</code>! Like before, we first started by examining the requirements and coming up with our various test cases. From there, we were able to drive out the functionality by using red-green-refactor and building our software with tests. In the next post, we\u2019ll take a look at implementing the logic for turning left!</p>"},{"location":"articles/2020/07/14/mars-rover--implementing-logger--design/","title":"Mars Rover \u2013 Implementing Logger \u2013 Design","text":"<p>Welcome to the tenth installment of Learning Through Example \u2013 Mars Rover! At this point, we\u2019ve wrapped up all the functionality for the <code>Rover</code>, so now it\u2019s time to start implementing the logging requirements. Like always, we\u2019ll first take at the requirements to ensure we\u2019ve got a good idea of what\u2019s needed. From there, we\u2019ll talk about the different approaches for implementation we could take. Finally, we\u2019ll decide on a rough approach for implementation</p>"},{"location":"articles/2020/07/14/mars-rover--implementing-logger--design/#what-do-we-need","title":"What Do We Need?","text":"<p>If we look back at the original requirements for logging, we find the following</p> <p>In order to help troubleshoot failures with the emulation, every time a command is received, both the command received, the rover\u2019s location, and the rover\u2019s orientation should be logged.</p>"},{"location":"articles/2020/07/14/mars-rover--implementing-logger--design/#determining-intent-from-vague-requirements","title":"Determining Intent From Vague Requirements","text":"<p>Yep, these requirements are clearly defined, but what\u2019s the problem we\u2019re trying to solve? Why do we care if this happens or not? After talking more with the Subject Matter Expert (SME), it seems like there are two main reasons for the logging requirement. First, they want a way to troubleshoot if something goes wrong in the emulation, and having this trace will be helpful in reproducing the error. Second, they want to have different runs be separated in the logging in the event they need to troubleshoot a specific run.</p>"},{"location":"articles/2020/07/14/mars-rover--implementing-logger--design/#going-from-vague-to-concrete-requirements","title":"Going from Vague to Concrete Requirements","text":"<p>Given the above intent, it sounds like an easy approach we can take is to log all of this information to a file with a specific name. In real production code, you would likely use a logging solution instead of implementing your own, but this will provide a good opportunity to learn some of the System.IO namespace in .NET.</p> <p>With a rough implementation in mind, we have another discussion with our SME, and come up with the following requirements:</p> <ul> <li>When the application starts, the user will need to specify where to log the information to (known as the path)</li> <li>If the path doesn\u2019t exist, the user is informed of such and the application doesn\u2019t continue</li> <li>If the path isn\u2019t accessible (maybe due to a permissions issue), then the user is informed of such and the application doesn\u2019t continue</li> <li>If the path is valid, then a text file will be created that contains the information generated by the emulator</li> <li>Every time the <code>Rover</code> receives a command, its <code>Location</code> and <code>Orientation</code> should be logged</li> </ul>"},{"location":"articles/2020/07/14/mars-rover--implementing-logger--design/#software-design-guidelines","title":"Software Design Guidelines","text":"<p>Now that we have requirements, how do we want to begin implementation? Do we want to add this logic to the Rover class? Or should we add a new component to handle logging?</p>"},{"location":"articles/2020/07/14/mars-rover--implementing-logger--design/#adding-logging-to-rover","title":"Adding Logging to Rover","text":"<p>So one approach to this problem is to widen the responsibilities for the <code>Rover</code>, add a <code>Log</code> method and update the existing methods to use this new method. The advantage of this approach is that the logging is in one place so it\u2019s easy to make changes. Another advantage is that since the <code>Rover</code> component already exists, we can go ahead and add some new tests pretty quick.</p> <p>Unfortunately, this approach has two major downsides. First, by widening the responsibilities, we\u2019ll need to update all existing tests to deal with logging (which may not be a trivial change to make). Second, since the <code>Rover</code> handles the logging, we\u2019ve muddled our easy-to-test business rules with a cross-cutting concern and now the <code>Rover</code> would have a couple of different reasons to change. Bummer \ud83d\ude41</p>"},{"location":"articles/2020/07/14/mars-rover--implementing-logger--design/#writing-a-new-component","title":"Writing a New Component","text":"<p>The advantage of writing a new component is that we can focus on making a <code>Logger</code> component that just knows how to log any message to storage, regardless of what uses this component. By taking this approach, we can have a \u201cdumb\u201d component (almost not worth testing) and keep the things that we care about (i.e. the business rules) easier to test.</p> <p>The main downside of this approach is that we have yet another component to manage. In our particular case, that\u2019s alright, but as the codebase becomes larger, this is something to keep in mind.</p>"},{"location":"articles/2020/07/14/mars-rover--implementing-logger--design/#looking-at-dependencies","title":"Looking at Dependencies","text":"<p>With our rough approach in mind (creating a new component to handle logging), let\u2019s take a look at what this new component is going to need to work as expected. Based on the requirements, the <code>Logger</code> will need to know where to log the information to (the path) and what to log (the message). These necessary pieces of information are known as dependencies because the <code>Logger</code> won\u2019t work if these are not provided.</p> <p>When it comes to injecting in a dependency, there are three main approaches, each providing benefits and drawbacks. Let\u2019s take a closer look at these approaches.</p>"},{"location":"articles/2020/07/14/mars-rover--implementing-logger--design/#constructor-injection","title":"Constructor Injection","text":"<p>First and foremost, constructor injection makes a lot of sense if the dependency doesn\u2019t change throughout the lifetime of the object. Let\u2019s say that we have an object that knows how to retrieve records from a database. As part of its work, it will need access to a connection string to connect to the database in question.</p> <pre><code>public class ItemRepository\n{\n  private readonly string _connectionString;\n\n  // By adding the dependency here, there's no way you that\n  // an ItemRepository could be created without specifying\n  // a connection string.\n  public ItemRepository(string connectionString)\n  {\n    _connectionString = connectionString;\n  }\n}\n\n// Example usage\n// If the connectionString isn't provided, then\n// the code doesn't compile!\nvar itemRepository = new ItemRepository(\"Data Source=localhost;Initial Catalog=AdventureWorks;Integrated Security=True\")\n</code></pre> <p>Generally speaking, an application doesn\u2019t speak to multiple databases, so the odds of the connection string needing to change during the lifetime of this class is slim. Given that, it makes sense to inject this dependency at the constructor level.</p> <p>One of the benefits of injecting dependencies at the constructor level is that you\u2019re revealing your intent to other developers. In this case, you\u2019re signaling others \u201cHey, if you\u2019re going to use this class, you need to specify a connection string, otherwise this isn\u2019t going to work!\u201d</p> <p>The main downside of this approach is that by adding more parameters to the constructor, you may end up with classes that have a ton of dependencies. However, if you find yourself in that situation, you might have another problem going on.</p>"},{"location":"articles/2020/07/14/mars-rover--implementing-logger--design/#method-injection","title":"Method Injection","text":"<p>For this approach, method injection makes sense if the dependency can change during the lifetime of the object. Let\u2019s say that we have a component that needs a way to send notifications and that the notification mechanism is based on user preferences. For example, if I\u2019m making a purchase, then I want the order confirmation to be emailed to me whereas other customers might want their receipt texted to them.</p> <pre><code>public class PurchaseWorkflow\n{\n  // By adding the NotificationStrategy here,\n  // there's no way that a developer can call \n  // the Complete method without specifying \n  // a way to notify the customer.\n  public void Complete(Order order, INotificationStrategy strategy)\n  {\n    string receiptDetails = CreateReceipt(order);\n    strategy.SendNotification(order.Customer, receiptDetails);\n  }\n}\n\n// Example Usage\nvar order = new Order(\"Cameron\", 22.50);\nvar workflow = new PurchaseWorkflow();\n// If a NotificationStrategy isn't passed in, this code won't compile.\nworkflow.Complete(order, new TextNotificationStrategy()); \n</code></pre> <p>If we were to use constructor injection, that means that we would need to instantiate a whole new object just because the notification mechanism needed to change which seems a bit excessive for our use case.</p> <p>Otherwise, method injection has the same main advantage of constructor injection (i.e. revealing design intent) and disadvantage (can lead to a bunch of necessary parameters to pass in)</p>"},{"location":"articles/2020/07/14/mars-rover--implementing-logger--design/#property-injection","title":"Property Injection","text":"<p>Unlike the other two approaches, property injection allows us to set the dependency as a property on the object and then start using the object. Unlike the other two approaches where you have to always specify the dependency, this approach allows you to set the dependency, use the class, then switch out the dependency without having to create a new instance (like constructor injection would force you to) and without having to specify it every time a method was called.</p> <p>However, the main downside is that you as the developer need to remember to set this property otherwise the code will compile, but you\u2019ll run into a runtime exception. It\u2019s because of this limitation that I\u2019ll avoid this injection technique and stick with either constructor or method level injections.</p> <pre><code>public class PurchaseWorkflow\n{\n  // By having the notification be set as a property, \n  // developer can set this dependency once and then\n  // only change it when needed.\n  // However, there's nothing forcing a developer to \n  // set this property.\n  public INotificationStrategy NotificationStrategy {get; set;}\n\n  public void Complete(Order order)\n  {\n    string receiptDetails = CreateReceipt(order);\n    NotificationStrategy.SendNotification(order.Customer, receiptDetails);\n  }\n}\n\n// The Mistake\nvar order = new Order(\"Cameron\", 22.50);\nvar workflow = new PurchaseWorkflow();\nworkflow.Complete(order); // This will cause a NullReferenceException to be thrown\n\n// Proper Usage\nvar order = new Order(\"Cameron\", 22.50);\nvar workflow = new PurchaseWorkflow();\n// As long as you remember to set this property, things should work!\nworkflow.NotificationStrategy = new TextNotificationStrategy();\nworkflow.Complete(order);\n</code></pre>"},{"location":"articles/2020/07/14/mars-rover--implementing-logger--design/#designing-the-logger","title":"Designing the Logger","text":"<p>Now that we\u2019ve looked at the various ways we can inject dependencies, we can start coming up with the rough design for <code>Logger</code>. Looking back at the requirements, we have two dependencies to worry about (the path and the message to log).</p> <p>Given that the path is not going to change during a run of the emulator, we should leverage constructor injection for this dependency. For the message, since that will be based on the <code>Rover</code>'s <code>Location</code> and <code>Orientation</code> which will change during the emulation, we should leverage method injection for this dependency.</p> <p>With all of this in place, we now have a much clearer idea of how the <code>Logger</code> component is going to look!</p>"},{"location":"articles/2020/07/14/mars-rover--implementing-logger--design/#wrapping-up","title":"Wrapping Up","text":"<p>In this post, we explored the requirements for logging and why the logging logic should be a separate component due it being a cross-cutting concern. From there, we explored at the three main ways to inject dependencies (constructor, method, and property). With this knowledge, we were able to make some smart decisions on how to inject where to log the information and what to log. In the next pose, we\u2019ll build upon this design by creating the <code>Logger</code> component. From there, we were able to make decisions on how to inject the path of where to log and the message to log. In the next post, we\u2019ll start implementing the logic for creating the <code>Logger</code> component.</p>"},{"location":"articles/2020/06/03/mars-rover--implementing-rover--creation/","title":"Mars Rover \u2013 Implementing Rover : Creation","text":"<p>In this installment of the Mars Rover kata, we\u2019re going to start implementing the <code>Rover</code> type! First, we\u2019re going to review the models that we derived in Part 2 of the series. From there, we\u2019ll take a look at the various requirements and see which piece to start implementing. From there, we\u2019ll write our first tests, driving the new functionality and running into a snag or two on the way. By the end of this post, we\u2019ll be one requirement down for the kata and have a better understanding of the <code>Rover</code> type!</p>"},{"location":"articles/2020/06/03/mars-rover--implementing-rover--creation/#model-review","title":"Model Review","text":"<p>As a recap, we\u2019ve derived the following models and implementations for the kata so far. For this post, we\u2019re going to be spending the majority of our time working with <code>Rover</code>, but it\u2019s good to know what our base looks like.</p> <pre><code>public enum Command\n{\n  MoveForward, MoveBackward,\n  TurnLeft, TurnRight,\n  Quit\n}\n</code></pre> <pre><code>public enum Direction\n{\n  North, South, East, West\n}\n</code></pre> <pre><code>public class Coordinate\n{\n  public int X {get; set;}\n  public int Y {get; set;}\n}\n</code></pre> <pre><code>public class Rover\n{\n  public Direction Orientation {get; set;}\n  public Coordinate Location {get; set;}\n}\n</code></pre> <p>With this in mind, let\u2019s start working on adding some new functionality to <code>Rover</code>!</p>"},{"location":"articles/2020/06/03/mars-rover--implementing-rover--creation/#in-the-beginning","title":"In the beginning","text":""},{"location":"articles/2020/06/03/mars-rover--implementing-rover--creation/#picking-an-approach","title":"Picking an Approach","text":"<p>When I begin a new feature, one thing that I\u2019m always thinking about is how do I break down the work ahead of me in such a way that I can start delivering value much faster. This doesn\u2019t mean that I\u2019m skipping on quality, but it does mean that I value quicker feedback than 100% test coverage. With that being said, when I look at the requirements, something that I can implement pretty quickly is that when the <code>Rover</code> starts, it should be at (0, 0) facing North.</p> <p>Like everything in development, there are multiple ways we could implement this functionality</p> <ol> <li>Provide a default constructor for <code>Rover</code> that sets those values explicitly</li> <li>Update the Program.cs file to set those values for <code>Rover</code></li> </ol> <p>With the first approach, we\u2019re encoding this business rule into the <code>Rover</code> type and forcing that when anyone creates an instance of <code>Rover</code>, it will always be at (0, 0) facing North which is a nice way of putting the business rule into the right component.</p> <p>With the second approach, we\u2019re going to let <code>Rover</code> be a dumb component and have some other component decide these values. The downside to this approach is that if there\u2019s an error with <code>Rover</code>, we won\u2019t know if it\u2019s because of how <code>Rover</code> is behaving or how it was created.</p> <p>Both approaches are valid, so I\u2019m just going to pick one and use that for now. If later down the road we need to make a change, we\u2019ll update as needed. With that being said, I\u2019m leaning towards the first approach, so let\u2019s go ahead and write our first unit test on making sure that <code>Rover</code> is facing <code>North</code>.</p>"},{"location":"articles/2020/06/03/mars-rover--implementing-rover--creation/#gaining-our-bearings","title":"Gaining Our Bearings","text":"<p>Creating a unit test for <code>Rover</code> where the <code>Orientation</code> should be <code>North</code> would look like the following:</p> <pre><code>[Test]\npublic void ThenTheRoverIsFacingNorth()\n{\n  // Arrange and Act\n  var rover = new Rover();\n\n  // Assert\n  Assert.AreEqual(Direction.North, rover.Orientation);\n}\n</code></pre> <p>What\u2019s interesting here is that when we run this test, it passes! But why does it pass? If we look back at <code>Rover</code>, here\u2019s how we\u2019ve defined it.</p> <pre><code>public class Rover\n{\n  public Direction Orientation {get; set;}\n  public Coordinate Location {get; set;}\n}\n</code></pre> <p>We don\u2019t have any constructors defined nor are we setting values for the two properties, so how does it know that <code>Orientation</code> is <code>North</code>?</p> <p>Magic?</p> <p>Not quite! Since we\u2019re not setting any value for <code>Orientation</code>, it will be whatever the default value for <code>Direction</code> is. We can determine that by using the default operator in C#.</p> <pre><code>default(Direction); // North\n</code></pre> <p>But why is <code>North</code> the default value for <code>Direction</code>?</p> <p>If you remember how <code>Direction</code> was defined, we decided to define it as an enum which is really a number in disguise.</p> <pre><code>public enum Direction\n{\n  North, South, East, West\n}\n</code></pre> <p>So what\u2019s the default value for a number?</p> <pre><code>default(int); // 0\n</code></pre> <p>So what\u2019s happening here is that when the <code>Rover</code> is created, it looks for any logic in the default constructor. Since there isn\u2019t one, it will set default values to both properties. For <code>Orientation</code>, that value will be 0 and by default, it will be the first value listed in the <code>Direction</code> enum.</p> <p>So for fun, if we change <code>North</code> to be the second choice in <code>Direction</code></p> <pre><code>public Direction\n{\n  South, North, East, West\n}\n</code></pre> <p>then our test fails with our expected error.</p> <p> </p> Failing unit test for Mars Rover as the expected value was North, but the value was South. <p>I don\u2019t know about you, but someone changing the ordering of an enum shouldn\u2019t be causing a failing test. Luckily, resolving this issue is as simple as explicitly stating the <code>Orientation</code> in <code>Rover</code>\u2018s default constructor.</p> <pre><code>public class Rover\n{\n  public Direction Orientation {get; set;}\n  public Coordinate Location {get; set;}\n\n  public Rover()\n  {\n    Orientation = Direction.North;\n  }\n}\n</code></pre> <p>Now, our test passes even if we change the ordering of values in <code>Direction</code>!</p>"},{"location":"articles/2020/06/03/mars-rover--implementing-rover--creation/#getting-to-ground-zero","title":"Getting to Ground Zero","text":"<p>Now that we have our <code>Orientation</code> figured out, let\u2019s go ahead and write our next test to make sure that the <code>Location</code> is correct.</p> <pre><code>[Test]\npublic void ThenTheRoverIsAt00()\n{\n  // Arrange and Act\n  var rover = new Rover();\n\n  var expectedLocation = new Coordinate{X=0, Y=0};\n  Assert.AreEqual(expectedLocation, rover.Location);\n}\n</code></pre> <p>When we run the test, we get the following error:</p> <p> </p> Failed test for Rover since it was expecting a Coordinate, but the Location was null. <p>Ah, yeah, that makes sense, <code>Location</code> is a <code>Coordinate</code> which is an object. Because we\u2019ve not explicitly set <code>Location</code>, the default value is <code>null</code>.</p> <pre><code>default(Coordinate); // null\n</code></pre> <p>So let\u2019s go ahead and update the <code>Rover</code> constructor</p> <pre><code>public class Rover\n{\n  public Direction Orientation {get; set;}\n  public Coordinate Location {get; set;}\n\n  public Rover()\n  {\n    Orientation = Direction.North;\n    Location = new Coordinate(){X=0, Y=0};\n  }\n}\n````\n\nSo let\u2019s re-run our test now.\n\n```csharp\n[Test]\npublic void ThenTheRoverIsAt00()\n{\n  // Arrange and Act\n  var rover = new Rover();\n\n  var expectedLocation = new Coordinate{X=0, Y=0};\n  Assert.AreEqual(expectedLocation, rover.Location);\n}\n</code></pre> <p> </p> Failed unit test since the expected coordinate is not the same as the actual coordinate.. <p>What\u2019s going on here?</p>"},{"location":"articles/2020/06/03/mars-rover--implementing-rover--creation/#not-all-locations-are-created-equally","title":"Not All Locations Are Created Equally","text":"<p>When we leverage <code>Assert.AreEqual</code>, under the hood, it\u2019s leveraging the built-in <code>Equals</code> method for the values being passed in. For primitive types, this will do a comparison by values, but if we\u2019re comparing objects, then it will do comparison by reference.</p> <p>Given this, the problem we\u2019re running into now is that even though I have two <code>Coordinate</code>s that have the same value, since they are two different objects, then <code>Assert.AreEqual</code> will fail. We\u2019ve got a couple of different ways to solve this problem.</p>"},{"location":"articles/2020/06/03/mars-rover--implementing-rover--creation/#overriding-the-equals-method","title":"Overriding the <code>Equals</code> Method","text":"<p>One way to solve the problem is by overriding the <code>Equals</code> method on the <code>Coordinate</code> class and override the logic so that two <code>Coordinate</code>s are the same if all of their properties are the same. This is a pretty solid approach to take if that\u2019s how equality should work everywhere. With that being said, here are some things to keep in mind when using this technique.</p> <p>First, if the class gains a new property, you will need to remember to update the <code>Equals</code> method, otherwise, you\u2019ll get interesting behavior when two objects that have a single difference are being treated as the same.</p> <p>Next, if you override <code>Equals</code>, then you must override GetHashCode as well. If you fail to do this, this will generate a warning during compilation time, but the bigger problem is that for two objects that are the same based on the definition of <code>Equals</code> but hash differently, then you will fail to find the item correctly in <code>Dictionary</code> and <code>HashSet</code> structures. When implementing <code>GetHashCode</code>, you should use the same properties for hashing as you would for equality checking.</p> <p>Overall, I will use this approach if equality for this type needs to be by value for everywhere in the application but this isn\u2019t my favorite approach because developers need to remember to update both <code>Equals</code> and <code>GetHashCode</code> when new properties are added.</p>"},{"location":"articles/2020/06/03/mars-rover--implementing-rover--creation/#going-from-class-to-struct","title":"Going from <code>class</code> to <code>struct</code>","text":"<p>The second approach we can take to have equality by value is by changing our type for <code>Coordinate</code> from a <code>class</code> to a <code>struct</code>. The cool thing about <code>structs</code> is that they by default handle equality by value so if you do need to add another property, things will work as expected. However, like all things in software, there are a couple of things to be aware of.</p> <p>First, <code>structs</code> have to have an empty, default constructor. So if there are some validation rules that need to be checked in the constructor, there\u2019s no way to force that to happen. Another effect of this drawback is that if a <code>struct</code> needs two things to exist or it shouldn\u2019t be created, you can\u2019t force callers to pass them in which can play havoc with making illegal states unrepresentable.</p> <p>Second, when working with <code>struct</code>s, and you change one of the properties, you\u2019re modifying a copy of the struct and not the original version.</p> <pre><code>public struct Date\n{\n  public int Month {get; set;}\n  public int Day {get; set;}\n  public int Year {get; set;}\n}\n\npublic class Order\n{\n  public Date TransactionDate {get; set;}\n  public decimal Total {get; set;}\n}\n\nvar order = new Order{Total=9.99m}; // at this point, Order will have a Date of 0/00/0000\norder.TransactionDate = new Date{Month=6, Day=22, Year=2020}; // Order has a new date!\norder.TransactionDate.Day = 23 // Fail to compile =&gt; Cannot modify the return value of `TransactionDate`\n                              // because it is not a variable\n</code></pre> <p>It\u2019s not a problem if you replace the whole struct value, but if you only want to change a part of it, you\u2019ll need to replace the whole value. If you\u2019ve ever worked with the <code>DateTime</code> type, you\u2019ll notice that you can\u2019t change values, but you can create a new <code>DateTime</code> value with updated properties.</p> <p>Overall, I prefer this approach when modeling values where dependencies aren\u2019t required and there\u2019s not a need for validation logic.</p>"},{"location":"articles/2020/06/03/mars-rover--implementing-rover--creation/#creating-an-equals-method-for-testing","title":"Creating an <code>Equals</code> Method for Testing","text":"<p>The third approach you can take when checking if things are equal is by defining a custom <code>Equals</code> method that lives in my test suite and is only used for testing. The benefit of this approach is that I can now verify my objects by value, without the need of overriding two different methods. In addition, since this type of equality is only needed for testing, I can now have this logic live in the test project. In addition, I can still have validation logic and non-default constructors for my classes.</p> <p>The main downside to this approach is that if there\u2019s a new property added to the class that you\u2019ve written custom equality logic for, you\u2019ll need to update this method as well.</p> <p>Overall, I prefer this approach when I need equality by value for testing purposes only and I don\u2019t want to make wide-sweeping changes by changing my type from <code>class</code> to <code>struct</code>.</p>"},{"location":"articles/2020/06/03/mars-rover--implementing-rover--creation/#making-locations-equal","title":"Making Locations Equal","text":"<p>Now that we have some more information about why the test failed and a few different approaches, let\u2019s take a look at what approach makes sense here. First off, <code>Coordinate</code>s really don\u2019t have any validation logic. Second, in our system, if we have two <code>Coordinate</code>s with the same values, then they should be considered the same. Given these assumptions, I\u2019m going to go ahead and change the type definition from <code>class</code> to <code>struct</code>.</p> <pre><code>public struct Coordinate\n{\n  public int X {get; set;}\n  public int Y {get; set;}\n}\n</code></pre> <p>Now that we\u2019ve made that change, if we re-run our test, it passes!</p>"},{"location":"articles/2020/06/03/mars-rover--implementing-rover--creation/#wrapping-up","title":"Wrapping Up","text":"<p>In this post, we started writing tests on <code>Rover</code>. We started off by adding a default constructor which sets the <code>Orientation</code> to <code>Direction.North</code> and explored about how setting defaults explicitly can protect us from changes in the future. From there, we wrote a test on <code>Location</code> and learned about how <code>Assert.AreEquals</code> leverages default equality and a few different approaches to solving this problem. Now that we have a way to assert against <code>Orientation</code> and <code>Location</code>, we can start writing tests for when the rover moves forward!</p>"},{"location":"articles/2020/05/20/mars-rover---defining-the-problem/","title":"Mars Rover - Defining the Problem","text":"<p>In this installment, we\u2019ll be looking at the problem description for Mars Rover. After becoming more familiar with the problem, we\u2019ll start by identifying the terminology that we should be using when talking about the problem by defining a ubiquitous language. From there, I\u2019ll show you how to break down the problem into various concepts and how to find the relationships between the various pieces. By the end of this post, you should feel comfortable exploring a new domain, understanding the terminology used, and defining relationships.</p>"},{"location":"articles/2020/05/20/mars-rover---defining-the-problem/#problem-description","title":"Problem Description","text":"<p>Congratulations and welcome to the S.P.A.C.E\u00b9 Institute, good to have you aboard! Our big focus for the year is to develop a rover that can navigate the surface of Mars! While the engineers are working on the design and building of the rover, we can focus on building the navigation module and start iterating on its design. With that in mind, here are a couple of assumptions we\u2019re going to make for this version.</p> <ol> <li>The rover will be traveling on a two-dimensional plane that should be modeled as a coordinate (X, Y)</li> <li>The rover is guaranteed to be able to travel in a chosen direction (no worries about obstacles or other landmarks)</li> </ol> <p>Given the above assumptions, here are the business rules that the emulation will need to follow</p> <ul> <li>When the emulation starts, the rover will always be at (0, 0) and facing North</li> <li>There are a series of commands that the rover can receive that can change its location or direction<ul> <li>When the rover is told to move forward, then it will move one rover unit in the direction it\u2019s facing</li> <li>When the rover is told to move backward, then it will move rover unit away from the direction it\u2019s facing</li> <li>When the rover is told to turn left, it will rotate 90 degrees to the left, but not change its location</li> <li>When the rover is told to turn right, it will rotate 90 degrees to the right, but not change its location</li> <li>When the emulation is told to quit, the rover will stop receiving commands</li> </ul> </li> <li>For the emulation, valid directions include North, East, South, and West</li> <li>In order to help troubleshoot failures with the emulation, every time a command is received, both the command received, the rover\u2019s location, and the rover\u2019s orientation should be logged.</li> </ul> <p>\u00b9 Simple Programming Application Checks Expertise</p>"},{"location":"articles/2020/05/20/mars-rover---defining-the-problem/#identifying-the-domain","title":"Identifying the Domain","text":"<p>When building software, I want to understand the various business terms that are being used to describe the problem so that when I\u2019m talking to subject matter experts (SMEs), I\u2019m using the same terminology as they are. For those who are familiar with Domain-Driven Design, this practice of using the same terminology is known as defining a ubiquitous language and the goal is to make sure that when someone says Command, then we are all referring to the same concept. If you\u2019ve ever worked in a codebase where something was called one thing, but the business referred to it as something different, then you are familiar with the pain of having to map between the two concepts.</p>"},{"location":"articles/2020/05/20/mars-rover---defining-the-problem/#find-the-nouns","title":"Find The Nouns","text":"<p>When working to define the ubiquitous language, a common approach is to find the nouns that are being used in the description as this can create the foundation of your classes (if following Object-Oriented principles) or your types (if following Functional Programming principles).</p> <p>Looking over the description again, these nouns stood out to me:</p> <p> </p> Domain models: Rover, Command, Location, Direction, and Orientation"},{"location":"articles/2020/05/20/mars-rover---defining-the-problem/#find-the-relationships","title":"Find The Relationships","text":"<p>Once the nouns have been found, I\u2019ll pivot to finding out how these different concepts are related to each other. One approach to finding these relationships is using \u201chas-a\u201d and \u201cis-a\u201d relationships. At a high level, if two things are related through \u201chas-a\u201d, then those concepts should be composed together. If two concepts have an \u201cis-a\u201d relationship, then I know that the concepts should be interchangeable for one another.</p> <p>To help identify these relationships, I would work with the SME to understand what each of these concepts means and how they relate to each other. Since it\u2019ll be a bit hard to simulate a conversation, here\u2019s the information that we would learn from our SME.</p> <ul> <li>A Rover has a Location and an Orientation</li> <li>Orientation is the Direction that a Rover is facing</li> <li>Location is the coordinates that the Rover is located at</li> <li>A Command is something that a Rover receives from the User</li> <li>A Direction can be North, East, South, or West</li> <li>A Command can be Move Forward, Move Backward, Turn Left,  Turn Right, or Quit</li> </ul> <p>With this new understanding, our concepts and relationships would look something like this:</p> <p> </p> Domain model relationships where Rover has a Location and an Orientation. Orientation is a Direction and Command is not related to anything."},{"location":"articles/2020/05/20/mars-rover---defining-the-problem/#wrapping-up","title":"Wrapping Up","text":"<p>In this post, we explored the problem description for the Mars Rover kata and built up our understanding of the various concepts by exploring the nouns. After finding the nouns, we leveraged \u201chas-a\u201d and \u201cis-a\u201d thinking to come up with a rough idea of how the various concepts related to one another. In the next post, we\u2019ll be focusing on how to model these concepts in code!</p>"},{"location":"articles/2020/06/09/mars-rover--implementing-rover--moving-forward/","title":"Mars Rover \u2013 Implementing Rover : Moving Forward","text":"<p>Welcome to the fifth installment of Learning Through Example \u2013 Mars Rover! In this post, we\u2019ll pick up from where we left on with <code>Rover</code> and start digging into how to make it move forward! We\u2019ll first examine what it means for the <code>Rover</code> to move forward by looking over the requirements in deeper detail. Once we have a better understanding, we\u2019ll start driving out the functionality by focusing on a simple case and building more complexity. By the end of this post, we\u2019ll have a <code>Rover</code> that will know how to move forward when facing any <code>Direction</code>!</p>"},{"location":"articles/2020/06/09/mars-rover--implementing-rover--moving-forward/#so-what-does-it-mean-to-move-forward","title":"So What Does It Mean To Move Forward?","text":"<p>If we look back at the original requirements for moving forward, we find this single line with regards to moving forward</p> <p>When the rover is told to move forward, then it will move one rover unit in the direction it\u2019s facing</p> <p>Super helpful, right? I don\u2019t know about you, but this not nearly enough information for us to start our work because I\u2019m not sure what that actually means!</p> <p>In this case, we will have a more in-depth conversation with our Subject Matter Expert and we\u2019ll find out that depending on the <code>Orientation</code> of the <code>Rover</code> the <code>Rover</code>\u2018s <code>Location</code> will change. Through additional conversations, we end up figuring out some more concrete business rules for when the <code>Rover</code> moves forward.</p> <ul> <li>Given the <code>Rover</code> is facing North, when it moves forward, then its <code>Y</code> value increases by 1</li> <li>Given the <code>Rover</code> is facing South, when it moves forward, then its <code>Y</code> value decreases by 1</li> <li>Given the <code>Rover</code> is facing East, when it moves forward, then its <code>X</code> value increases by 1</li> <li>Given the <code>Rover</code> is facing East, when it moves forward, then its <code>X</code> value decreases by 1</li> </ul> <p>Great, we have enough information to get started so we can start demonstrating the software and get quicker feedback!</p>"},{"location":"articles/2020/06/09/mars-rover--implementing-rover--moving-forward/#writing-the-first-test","title":"Writing The First Test","text":"<p>Let\u2019s begin writing our first test for the <code>Rover</code> moving forward! We\u2019ll be leveraging the same naming guidelines mentioned in part three to help make the use cases standout in our tests</p> <pre><code>[Test]\npublic void AndFacingNorthThenYIncreasesByOne()\n{\n  // Arrange\n  var rover = new Rover() { Orientation = Direction.North};\n\n  // Act\n  rover.MoveForward();\n\n  // Assert\n  Assert.AreEqual(1, rover.Coordinate.Y);\n}\n</code></pre> <p>So far, so good! The test matches the intent behind the name and a new developer can see that we\u2019ve created a <code>Rover</code> facing <code>North</code>, called its <code>MoveForward</code> method and making sure that the <code>Y</code> property is 1.</p> <p>If we try running the test, it will fail because <code>Rover</code> doesn\u2019t have a <code>MoveForward</code> method, so let\u2019s go ahead and write a simple implementation.</p> <pre><code>public void MoveForward()\n{\n}\n</code></pre> <p>Even though there\u2019s no business logic implemented, it\u2019s enough code for our test to compile and if we run the test, the test fails because <code>Y</code> is not 1.</p> <p>With that in mind, let\u2019s go ahead and write the simplest thing that could work just to check our approach.</p> <pre><code>public void MoveForward()\n{\n  Location.Y+=1;\n}\n</code></pre> <p>Hmm, when we try to compile this code though, we get the following error</p> <p> </p> Compiler error when trying to update the Location's Y Property <p>What\u2019s going on here?</p>"},{"location":"articles/2020/06/09/mars-rover--implementing-rover--moving-forward/#when-things-go-off-track","title":"When Things Go Off Track","text":"<p>The error is caused due to the interaction of <code>struct</code> and the <code>Location</code> property implementation. If you recall, <code>struct</code>s are value types which means when you assign them to a variable, the variable has its own copy of the <code>struct</code>, not the reference.</p> <pre><code>// Let's create a location\nvar location = new Coordinate {X = 0, Y = 0};\n\n// And now let's have newLocation have the same value, NOTE: This isn't a reference to location!\nvar newLocation = location;\n\n// And if we check if both are equal, they are!\nConsole.WriteLine(location.Equals(newLocation)); // True\n\n// Now let's change location's X value\nlocation.X = 200;\n\n// That works just fine, but if we check what newLocation is, we see that it's stil (0, 0)\nConsole.WriteLine($\"newLocation = ({newLocation.X}, {newLocation.Y})\");\n\n// Which means when we compare, they're not the same!\nConsole.WriteLine(location.Equals(newLocation)); // False\n</code></pre> <p>So what does that have to do with the <code>Location</code> property? Well, we\u2019ve defined it as an auto-property which is syntactic sugar for telling the compiler to generate a backing field for the property and to implement default <code>get</code> and <code>set</code> logic.</p> <pre><code>// Given this definition of Rover\npublic class Rover\n{\n  public Coordinate Location {get; set}\n}\n\n// This is syntactic sugar for the following\npublic class Rover\n{\n  private Coordinate _location;\n  public Coordinate Location\n  {\n    get =&gt; _location;\n    set =&gt; _location = value;\n  }\n}\n</code></pre> <p>So the problem arises from how <code>get</code> is working. It\u2019s returning the backing field which is going to be stored as a variable for use. Recall from above that when we do that type of assignment, we\u2019re working on a copy of the value. So if we try to make changes to the copy, the changes won\u2019t make it back to the backing field which in turn won\u2019t ever update the <code>Location</code> property!</p>"},{"location":"articles/2020/06/09/mars-rover--implementing-rover--moving-forward/#getting-back-on-track","title":"Getting Back On Track","text":"<p>So good job on the compiler letting us know that there\u2019s a problem even if the message is a bit obscure! But how do we fix the problem? Well, to get the code to compile, instead of updating the <code>Y</code> property of <code>Location</code>, let\u2019s go ahead and update the entire <code>Location</code> property instead.</p> <pre><code>public void MoveForward()\n{\n  Location = new Coordinate { X = Location.X, Y = Location.Y + 1 };\n}\n</code></pre> <p>And if we try to run our test, we find that it now passes, hooray!</p>"},{"location":"articles/2020/06/09/mars-rover--implementing-rover--moving-forward/#refactoring-the-code","title":"Refactoring The Code","text":"<p>For those keeping track at home, we\u2019re doing a pretty good job of following Test Drive Development (TDD) principles in that we first wrote a failing test, then wrote enough code to make it pass. The third step is to refactor our code (both production and test) to make it easier to work with or to make it more robust.</p> <p>If we take a look at the <code>MoveForward</code> method, it\u2019s pretty simple and there\u2019s not much we can refactor there for now.</p> <pre><code>public void MoveForward()\n{\n  Location = new Coordinate { X = Location.X, Y = Location.Y + 1 };\n}\n</code></pre>"},{"location":"articles/2020/06/09/mars-rover--implementing-rover--moving-forward/#is-our-state-correct","title":"Is Our State Correct?","text":"<p>So if our business code is pretty good, let\u2019s take a look at our test and see what can be done.</p> <pre><code>[Test]\npublic void AndFacingNorthThenYIncreasesByOne()\n{\n  // Arrange\n  var rover = new Rover() { Orientation = Direction.North};\n\n  // Act\n  rover.MoveForward();\n\n  // Assert\n  Assert.AreEqual(1, rover.Coordinate.Y);\n}\n</code></pre> <p>Looking at this code, one thing that stands out is that we\u2019re checking that <code>Y</code> got updated, but we\u2019re not verifying that <code>X</code> nor the <code>Orientation</code> didn\u2019t change. In fact, if we change the implementation of <code>MoveForward</code> to set the X value to be 200 and change the <code>Orientation</code> to be <code>South</code>, the test would still pass and it clearly shouldn\u2019t!</p> <p>Thankfully, we can mediate this oversight by creating an <code>expectedLocation</code> which will have the expected <code>X</code> and <code>Y</code> values for the Rover. In addition, we\u2019ll update one <code>Assert</code> to use this new value and add another <code>Assert</code> to verify the <code>Orientation</code></p> <pre><code>[Test]\npublic void AndFacingNorthThenYIncreasesByOne()\n{\n  // Arrange\n  var rover = new Rover {Orientation = Direction.North};\n\n  // Act\n  rover.MoveForward();\n\n  // Assert\n  var expectedLocation = new Coordinate { X = 0, Y = 1};\n  Assert.AreEqual(expectedLocation, rover.Location);\n  Assert.AreEqual(Direction.North, rover.Orientation);\n}\n</code></pre> <p>Nice! We\u2019re now much more explicit about our expectations of <code>Rover</code> should be at this exact <code>Location</code> and should have this exact <code>Orientation</code>, otherwise, fail the test.</p>"},{"location":"articles/2020/06/09/mars-rover--implementing-rover--moving-forward/#are-there-hidden-assumptions","title":"Are There Hidden Assumptions?","text":"<p>While looking at this test, there\u2019s one more subtle issue with code, can you spot it?</p> <p>We\u2019re making an assumption about what the initial <code>Location</code> for the <code>Rover</code>! What if the <code>Rover</code> started off at (5, 5) instead of (0, 0)? This test would fail, but not for the right reason (an error in the production code), but due to fragility in the way the test was written.</p> <p>If we wanted to harden this test, we have two approaches</p>"},{"location":"articles/2020/06/09/mars-rover--implementing-rover--moving-forward/#setting-the-location","title":"Setting the Location","text":"<p>We could change our <code>Arrange</code> step to explicitly set the initial location of <code>Rover</code> to be (0, 0). This would guarantee the initial setup and if the default <code>Location</code> were to ever change, our test would still pass.</p> <pre><code>[Test]\npublic void AndFacingNorthThenYIncreasesByOne()\n{\n  // Arrange\n  var rover = new Rover\n  {\n    Orientation = Direction.North,\n    Location = new Coordinate {X = 0, Y = 0},\n  };\n\n  // Act\n  rover.MoveForward();\n\n  // Assert\n  var expectedLocation = new Coordinate { X = 0, Y = 1};\n  Assert.AreEqual(expectedLocation, rover.Location);\n  Assert.AreEqual(Direction.North, rover.Orientation);\n}\n</code></pre>"},{"location":"articles/2020/06/09/mars-rover--implementing-rover--moving-forward/#capturing-the-initial-location","title":"Capturing the Initial Location","text":"<p>When we look at this test and the code we\u2019re testing, the key thing that we\u2019re wanting to test is that the right value was modified correctly (in this case either by +1 or -1). Given that, we could update our <code>Arrange</code> step to capture what the initial <code>Location</code> was and then update our <code>Assert</code> step to know about the location.</p> <pre><code>[Test]\npublic void AndFacingNorthThenYIncreasesByOne()\n{\n  // Arrange\n  var rover = new Rover {Orientation = Direction.North};\n  var initialLocation = rover.Location; // capturing the initial location\n\n  // Act\n  rover.MoveForward();\n\n  // Assert\n  var expectedLocation = new Coordinate { X = initialLocation.X, Y = initialLocation.Y+1};\n  Assert.AreEqual(expectedLocation, rover.Location);\n  Assert.AreEqual(Direction.North, rover.Orientation);\n}\n</code></pre> <p>Given the two approaches, I like the idea of capturing the initial location, so that\u2019s what I\u2019m going to go with.</p>"},{"location":"articles/2020/06/09/mars-rover--implementing-rover--moving-forward/#writing-additional-tests","title":"Writing Additional Tests","text":"<p>Now that we have a passing test for <code>Rover</code> and moving forward, let\u2019s go ahead and implement another piece of functionality by writing a test for when the <code>Rover</code> is facing <code>South</code></p>"},{"location":"articles/2020/06/09/mars-rover--implementing-rover--moving-forward/#redgreenrefactor-for-rover-facing-south","title":"Red/Green/Refactor for Rover Facing South","text":"<pre><code>[Test]\npublic void AndFacingSouthThenYDecreasesByOne()\n{\n  // Arrange\n  var rover = new Rover {Orientation = Direction.South};\n  var initialLocation = rover.Location; // capturing the inital location\n\n  // Act\n  rover.MoveForward();\n\n  // Assert\n  var expectedLocation = new Coordinate { X = initialLocation.X, Y = initialLocation.Y-1};\n  Assert.AreEqual(expectedLocation, rover.Location);\n  Assert.AreEqual(Direction.South, rover.Orientation);\n}\n</code></pre> <p>And write enough code to make it pass!</p> <pre><code>public void MoveForward()\n{\n  if (Orientation == Direction.North) {\n    Location = new Coordinate { X = Location.X, Y = Location.Y + 1 };\n  }\n  else {\n    Location = new Coordinate { X = Location.X, Y = Location.Y - 1};\n  }\n}\n</code></pre> <p>Now that we have a passing test suite again, is there anything we want to refactor? Are there any patterns starting to emerge?</p> <p>From the business code, <code>MoveForward</code> seems pretty straightforward and I\u2019m not sure what refactor I could do there that would make a lot of sense right now.</p> <p>If we take a look at the test code, I\u2019m noticing that our two tests so far look almost like carbon copies of each other. In fact, if we take a closer look, it seems like the only differences between the two tests are the <code>Rover</code>\u2018s <code>Orientation</code> and the <code>expectedLocation</code>. I\u2019m really tempted to refactor this code to be a bit more DRY and remove some duplication. However, I\u2019ve only seen two examples so far and before I refactor to a pattern, I actually want the pattern to manifest first so I know what the pattern is.</p> <p>Let\u2019s keep writing some more tests and see what pattern emerges!</p>"},{"location":"articles/2020/06/09/mars-rover--implementing-rover--moving-forward/#redgreenrefactor-for-rover-facing-east","title":"Red/Green/Refactor for Rover Facing East","text":"<p>Now that the <code>Rover</code> can move forward when facing <code>North</code> or <code>South</code>, let\u2019s go ahead and write a test for when the <code>Rover</code> faces East.</p> <pre><code>  [Test]\n  public void AndFacingEastThenXIncreasesByOne()\n  {\n    // Arrange\n    var rover = new Rover {Orientation = Direction.East};\n    var initialLocation = rover.Location;\n\n    // Act\n    rover.MoveForward();\n\n    // Assert\n    var expectedLocation = new Coordinate { X = initialLocation.X+1, Y = initialLocation.Y};\n    Assert.AreEqual(expectedLocation, rover.Location);\n    Assert.AreEqual(Direction.East, rover.Orientation);\n  }\n</code></pre> <p>With the test in place, let\u2019s write enough code to make it pass.</p> <pre><code>public void MoveForward()\n{\n  if (Orientation == Direction.North) {\n    Location = new Coordinate { X = Location.X, Y = Location.Y + 1 };\n  }\n  if (Orientation == Direction.South) {\n    Location = new Coordinate { X = Location.X, Y = Location.Y - 1};\n  }\n  if (Direction == Direction.East) {\n    Location = new Coordinate {X = Location.X + 1, Y = Location.Y};\n  }\n}\n</code></pre> <p>With this passing test, let\u2019s take a look at possible refactoring opportunities.</p>"},{"location":"articles/2020/06/09/mars-rover--implementing-rover--moving-forward/#refactoring-coordinate","title":"Refactoring Coordinate","text":"<p>If we look at the production code, I\u2019m getting really tired of having to write <code>Location = new Coordinate {X=Location.X..., Y=Location.Y...}</code> because I know I\u2019m going to have to write this similar logic for the last remaining test for moving forward and probably something similar for moving backward.</p> <p>Looking at the way we\u2019ve been modifying <code>Coordinate</code>, it seems like we\u2019re every modifying <code>X</code> or <code>Y</code> by a set amount, so what if we wrote some methods that could adjust either <code>X</code> or <code>Y</code>?</p> <p>If we take a look at <code>Coordinate</code>, it seems like we have a struct with the two properties in mention, so let\u2019s add a method called <code>AdjustXBy</code> that will return a new <code>Coordinate</code> with <code>X</code> adjusted by that value and keep <code>Y</code> the same</p> <pre><code>public struct Coordinate\n{\n  public int X {get; set;}\n  public int Y {get; set;}\n\n  public Coordinate AdjustXBy(int adjustment)\n  {\n    return new Coordinate {X = X+adjustment, Y=Y};\n  }\n</code></pre> <p>With this change in place, let\u2019s go ahead and update our <code>MoveForward</code> method to use this new code!</p> <pre><code>public void MoveForward()\n{\n  if (Orientation == Direction.North) {\n    Location = new Coordinate { X = Location.X, Y = Location.Y + 1 };\n  }\n  if (Orientation == Direction.South) {\n    Location = new Coordinate { X = Location.X, Y = Location.Y - 1};\n  }\n  if (Direction == Direction.East) {\n    Location = Location.AdjustXBy(1);\n  }\n}\n</code></pre> <p>Even in this small example, this addition is already more concise of our intent than the other two cases. After doing a quick verification that the test still passes (otherwise the refactor isn\u2019t a refactor), let\u2019s go ahead and add a new method to <code>Coordinate</code> called <code>AdjustYBy</code> that is similar to <code>AdjustXBy</code></p> <pre><code>  public Coordinate AdjustYBy(int adjustment)\n  {\n    return new Coordinate {X=X, Y=Y+adjustment};\n  }\n</code></pre> <p>And let\u2019s go ahead and update <code>MoveForward</code> to take advantage of this new functionality</p> <pre><code>public void MoveForward()\n{\n  if (Orientation == Direction.North) {\n    Location = Location.AdjustYBy(1);\n  }\n  if (Orientation == Direction.South) {\n    Location = Location.AdjustYBy(-1);\n  }\n  if (Direciton == Direction.East) {\n    Location = Location.AdjustXBy(1);\n  }\n}\n</code></pre> <p>After making that much change to the production code, we\u2019ll go ahead and run our test suite again and it seems like the change is working as expected, nice!</p>"},{"location":"articles/2020/06/09/mars-rover--implementing-rover--moving-forward/#refactoring-test-code","title":"Refactoring Test Code","text":"<p>Now that we\u2019ve refactored the business rules and our test suite is passing correctly, we can take a look at refactoring our test code. With the addition of the <code>East</code> test, the tests are definitely following a pattern and I should be able to extract out that logic to a single test and then pass in different parameters (even though the link is to NUnit, most test frameworks support this concept).</p> <pre><code>  [Test]\n  public void AndFacingNorthThenYIncreasesByOne()\n  {\n    // Arrange\n    var rover = new Rover {Orientation = Direction.North};\n    var initialLocation = rover.Location; // capturing the inital location\n\n    // Act\n    rover.MoveForward();\n\n    // Assert\n    var expectedLocation = new Coordinate { X = initialLocation.X, Y = initialLocation.Y+1};\n    Assert.AreEqual(expectedLocation, rover.Location);\n    Assert.AreEqual(Direction.North, rover.Orientation);\n  }\n</code></pre> <pre><code>[Test]\npublic void AndFacingSouthThenYDecreasesByOne()\n{\n  // Arrange\n  var rover = new Rover {Orientation = Direction.South};\n  var initialLocation = rover.Location; // capturing the inital location\n\n  // Act\n  rover.MoveForward();\n\n  // Assert\n  var expectedLocation = new Coordinate { X = initialLocation.X, Y = initialLocation.Y-1};\n  Assert.AreEqual(expectedLocation, rover.Location);\n  Assert.AreEqual(Direction.South, rover.Orientation);\n}\n</code></pre> <pre><code>[Test]\npublic void AndFacingEastThenXIncreasesByOne()\n{\n  // Arrange\n  var rover = new Rover {Orientation = Direction.East};\n  var initialLocation = rover.Location;\n\n  // Act\n  rover.MoveForward();\n\n  // Assert\n  var expectedLocation = new Coordinate { X = initialLocation.X+1, Y = initialLocation.Y};\n  Assert.AreEqual(expectedLocation, rover.Location);\n  Assert.AreEqual(Direction.East, rover.Orientation);\n}\n</code></pre> <p>Given the differences between the tests, we would need to extract the starting <code>Direction</code> and the <code>expectedLocation</code> to be parameters. However, the <code>expectedLocation</code> is based on the initialLocation which is currently based on whatever the <code>Rover</code> defaults to.</p> <p>Based on that chain, if we wanted to do this refactor, we would have to pass in a <code>Rover</code> as the parameter and I really don\u2019t like that idea because if <code>Rover</code> grows to be bigger, then creating a <code>Rover</code> becomes more involved and I don\u2019t want to inflict that onto my test. In addition, one thing that is nice about our tests is that they\u2019re easy to read and to follow their logic which has a ton of value given that developers spend more time reading code than writing code.</p> <p>All of that to say, that even though the tests look similar, I\u2019m going to pass on refactoring to a single unified test because I\u2019d be trading readability for removing duplication and these tests are small enough that I don\u2019t think it\u2019s that much technical debt to take on.</p>"},{"location":"articles/2020/06/09/mars-rover--implementing-rover--moving-forward/#redgreenrefactor-for-rover-facing-west","title":"Red/Green/Refactor for Rover Facing West","text":"<p>With the latest test, we\u2019re 3/4 of the way through implementing <code>MoveForward</code>, so let\u2019s go ahead and write another failing test for when the <code>Rover</code> faces <code>West</code>.</p> <pre><code>[Test]\npublic void AndFacingWestThenXDecreasesByOne()\n{\n  // Arrange\n  var rover = new Rover {Orientation = Direction.West};\n  var initialLocation = rover.Location;\n\n  // Act\n  rover.MoveForward();\n\n  // Assert\n  var expectedLocation = new Coordinate { X = initialLocation.X-1, Y = initialLocation.Y};\n  Assert.AreEqual(expectedLocation, rover.Location);\n  Assert.AreEqual(Direction.West, rover.Orientation);\n}\n</code></pre> <p>With the test in places, let\u2019s write enough code to make the test pass by taking advantage of <code>Coordinate.AdjustXBy</code></p> <pre><code>public void MoveForward()\n{\n  if (Orientation == Direction.North) {\n    Location = Location.AdjustYBy(1);\n  }\n  if (Orientation == Direction.South) {\n    Location = Location.AdjustYBy(-1);\n  }\n  if (Direciton == Direction.East) {\n    Location = Location.AdjustXBy(1);\n  }\n  if (Orientation == Direction.West) {\n    Location = Location.AdjustXBy(-1);\n  }\n}\n</code></pre> <p>And with this latest addition, not only do we have a passing test suite, but we\u2019ve also covered the business rules for when the <code>Rover</code> moves forward, completing this part of the kata, nice!</p> <p>As a recap, here\u2019s what <code>Rover</code> and <code>WhenMovingForward</code> looks like</p> <pre><code>public class Rover\n{\n  public Direction Orientation {get; set;}\n  public Coordinate Location {get; set;}\n\n  public Rover()\n  {\n    Orientation = Direction.North;\n    Location = new Coordinate(){X=0, Y=0};\n  }\n\n  public void MoveForward()\n  {\n    if (Orientation == Direction.North) {\n      Location = Location.AdjustYBy(1);\n    }\n    if (Orientation == Direction.South) {\n      Location = Location.AdjustYBy(-1);\n    }\n    if (Direciton == Direction.East) {\n      Location = Location.AdjustXBy(1);\n    }\n    if (Orientation == Direction.West) {\n      Location = Location.AdjustXBy(-1);\n    }\n  }\n}\n</code></pre> <pre><code>[TestFixture]\npublic class WhenMovingForward()\n{\n  [Test]\n  public void AndFacingNorthThenYIncreasesByOne()\n  {\n    // Arrange\n    var rover = new Rover {Orientation = Direction.North};\n    var initialLocation = rover.Location;\n\n    // Act\n    rover.MoveForward();\n\n    // Assert\n    var expectedLocation = new Coordinate { X = initialLocation.X, Y = initialLocation.Y+1};\n    Assert.AreEqual(expectedLocation, rover.Location);\n    Assert.AreEqual(Direction.North, rover.Orientation);\n  }\n\n  [Test]\n  public void AndFacingSouthThenYDecreasesByOne()\n  {\n    // Arrange\n    var rover = new Rover {Orientation = Direction.South};\n    var initialLocation = rover.Location;\n\n    // Act\n    rover.MoveForward();\n\n    // Assert\n    var expectedLocation = new Coordinate { X = initialLocation.X, Y = initialLocation.Y-1};\n    Assert.AreEqual(expectedLocation, rover.Location);\n    Assert.AreEqual(Direction.South, rover.Orientation);\n  }\n\n  [Test]\n  public void AndFacingEastThenXIncreasesByOne()\n  {\n    // Arrange\n    var rover = new Rover {Orientation = Direction.East};\n    var initialLocation = rover.Location;\n\n    // Act\n    rover.MoveForward();\n\n    // Assert\n    var expectedLocation = new Coordinate { X = initialLocation.X+1, Y = initialLocation.Y};\n    Assert.AreEqual(expectedLocation, rover.Location);\n    Assert.AreEqual(Direction.East, rover.Orientation);\n  }\n\n\n  [Test]\n  public void AndFacingWestThenXDecreasesByOne()\n  {\n    // Arrange\n    var rover = new Rover {Orientation = Direction.West};\n    var initialLocation = rover.Location;\n\n    // Act\n    rover.MoveForward();\n\n    // Assert\n    var expectedLocation = new Coordinate { X = initialLocation.X-1, Y = initialLocation.Y};\n    Assert.AreEqual(expectedLocation, rover.Location);\n    Assert.AreEqual(Direction.West, rover.Orientation);\n  }\n}\n</code></pre>"},{"location":"articles/2020/06/09/mars-rover--implementing-rover--moving-forward/#wrapping-up","title":"Wrapping Up","text":"<p>With this final test in place, we have the core functionality for when the <code>Rover</code> moves forward. In addition, we\u2019ve written enough tests and functionality now that if requirements were to change, we have a pretty good guess on what the work involved would be. In the next part of the kata, we\u2019ll start implementing a new piece of functionality!</p>"},{"location":"articles/2020/05/20/learning-through-example--mars-rover-kata/","title":"Learning Through Example \u2013 Mars Rover Kata","text":"<p>Over my career, I\u2019ve spent a lot of time bringing new engineers up to speed on how to break down problems, how to write better code, and how to write automated tests. What I\u2019ve come to find out is that there are a ton of resources on how to do each of these things individually, but not very many that brings all of these concepts together in one place. The purpose of this series is to bring all of these ideas together as we solve the Mars Rover kata.</p>"},{"location":"articles/2020/05/20/learning-through-example--mars-rover-kata/#purpose","title":"Purpose","text":"<p>For new engineers, this kata has about the right amount of complexity to explore these various concepts and help identify things to improve on. As a team lead at SentryOne, I\u2019ve onboard interns and our associate engineers using this kata as a launch point to teach them the tooling and processes we use. In addition, this kata serves as a method to evaluate their current skills so we can help round out a solid foundation for their career.</p> <p>By the end of this series, you will have a better understanding of how to break down a problem into small, deliverable pieces while being able to write tests around new functionality. Even though there will be a lot of code to look at, none of these concepts are technology-specific so I challenge you to follow along with your tech stack of choice and see how you would implement some of these concepts in that stack. </p>"},{"location":"articles/2020/05/20/learning-through-example--mars-rover-kata/#technologies-used","title":"Technologies Used","text":"<p>On the topic of technologies, I\u2019ll be using the following for my implementation of this kata. As long as you find the relevant tool for your technology stack, you should be able to follow along!</p> <ul> <li>Source Control: GitHub</li> <li>Language/Framework: C# on .NET Core 3.1</li> <li>Unit Testing Framework: NUnit</li> <li>Mocking Framework: NSubstitute</li> <li>Editor: Visual Studio Code</li> </ul>"},{"location":"articles/2020/05/20/learning-through-example--mars-rover-kata/#series","title":"Series","text":"<ul> <li>Part 0 \u2013 Introduction</li> <li>Part 1 \u2013 Defining the Problem</li> <li>Part 2 \u2013 Modeling Concepts</li> <li>Part 3 \u2013 Intro to Testing</li> <li>Part 4 \u2013 Creating a Rover</li> <li>Part 5 \u2013 Rover Moving Forward</li> <li>Part 6 \u2013 Rover Moving Backward</li> <li>Part 7 \u2013 Rover Turning Left</li> <li>Part 8 \u2013 Refactoring Rover</li> <li>Part 9 \u2013 Rover Turning Right</li> <li>Part 10 \u2013 Creating a Logger</li> <li>Part 11 \u2013 Logging to a File</li> <li>Part 12 \u2013 Combining Rover and Logger </li> <li>Part 13 \u2013 Implementing the User Interface</li> <li>Part 14 \u2013 Reflection</li> </ul>"},{"location":"articles/2020/06/23/mars-rover--implementing-rover--turning-left/","title":"Mars Rover \u2013 Implementing Rover : Turning Left","text":"<p>Welcome to the seventh installment of Learning Through Example \u2013 Mars Rover! In this post, we\u2019re going to start driving out the functionality for the <code>Rover</code> and the business rules for turning left! First, we\u2019ll take a look at the requirements to make sure we have an idea of what\u2019s needed. From there, we\u2019ll start implementing the requirements. By the end of this post, we\u2019ll have a <code>Rover</code> that can do 3/4 of the possible commands!</p>"},{"location":"articles/2020/06/23/mars-rover--implementing-rover--turning-left/#turning-in-place-by-turning-left","title":"Turning in Place By Turning Left","text":"<p>If we look back at the original requirements for turning left, we find this single line as the requirement</p> <p>When the rover is told to turn left, it will rotate 90 degrees to the left, but not change its location</p> <p>Given this requirement, we\u2019re able to double check with our Subject Matter Expert that the <code>Rover</code> is essentially rotating in place which yields the following requirements.</p> <ul> <li>Given the <code>Rover</code> is facing <code>North</code>, when it turns left, then the <code>Rover</code> should be facing <code>West</code> at the same Coordinate</li> <li>Given the <code>Rover</code> is facing <code>West</code>, when it turns left, then the <code>Rover</code> should be facing <code>South</code> at the same Coordinate</li> <li>Given the <code>Rover</code> is facing <code>South</code>, when it turns left, then the <code>Rover</code> should be facing <code>East</code> at the same Coordinate</li> <li>Given the <code>Rover</code> is facing <code>East</code>, when it turns left, then the <code>Rover</code> should be facing <code>North</code> at the same Coordinate</li> </ul> <p>Even though we\u2019ve not written tests that have this exact setup, it looks like we should be able to use that as a start while we\u2019re implementing. So let\u2019s go ahead tackle the first requirement!</p>"},{"location":"articles/2020/06/23/mars-rover--implementing-rover--turning-left/#redgreenrefactor-for-rover-facing-north","title":"Red/Green/Refactor For Rover Facing North","text":"<p>Leaning on lessons learned previously, we can write the following test for when the <code>Rover</code> is facing <code>North</code>.</p> <pre><code>[Test]\npublic void AndFacingNorthThenTheRoverFacesWest()\n{\n  var rover = new Rover {Orientation=Direction.North};\n  var initialLocation = rover.Location;\n\n  rover.TurnLeft();\n\n  Assert.AreEqual(initialLocation, rover.Location);\n  Assert.AreEqual(Direction.West, rover.Orientation);\n}\n</code></pre> <p>Not too bad of a setup! First, we create the <code>Rover</code> and capture its <code>initialLocation</code>. From there, we call our new method, <code>TurnLeft</code> which doesn\u2019t exist yet, but will soon. After <code>TurnLeft</code> has been called, we check to make sure that our <code>Location</code> is the same and that our <code>Orientation</code> has been updated accordingly.</p> <p>Now let\u2019s write just enough code to pass.</p> <pre><code>public void TurnLeft()\n{\n  if (Orientation == Direction.North) {\n    Orientation = Direction.West;\n  }\n}\n</code></pre> <p>Pretty straightforward implementation and is good enough to make the test pass. Not much to refactor at this point so let\u2019s go ahead and work on the next test case.</p>"},{"location":"articles/2020/06/23/mars-rover--implementing-rover--turning-left/#redgreenrefactor-for-rover-facing-west","title":"Red/Green/Refactor For Rover Facing West","text":"<p>For the next test, let\u2019s go ahead and write one for when the <code>Rover</code> faces <code>West</code></p> <pre><code>[Test]\npublic void AndFacingWestThenTheRoverFacesSouth()\n{\n  var rover = new Rover {Orientation=Direction.West};\n  var initialLocation = rover.Location;\n\n  rover.TurnLeft();\n\n  Assert.AreEqual(initialLocation, rover.Location);\n  Assert.AreEqual(Direction.South, rover.Orientation);\n}\n</code></pre> <p>And now enough code to make it pass</p> <pre><code>public void TurnLeft()\n{\n  if (Orientation == Direction.North) {\n    Orientation = Direction.West;\n  }\n  else if (Orientation == Direction.West) {\n    Orientation = Direction.South;\n  }\n}\n</code></pre> <p>Once again, now that we have passing tests, is there anything we want to refactor? The business rules look to be simple enough so I don\u2019t feel the need to refactor those. When we look at the test code, the test seems straightforward and I\u2019m not sure what I\u2019d simplify.</p> <p>Time to commit these changes and on to the next requirement.</p>"},{"location":"articles/2020/06/23/mars-rover--implementing-rover--turning-left/#redgreenrefactor-for-rover-facing-south","title":"Red/Green/Refactor For Rover Facing South","text":"<p>Third verse same as the first, we can write a failing test for when the <code>Rover</code> faces <code>South</code>.</p> <pre><code>[Test]\npublic void AndFacingSouthThenTheRoverFacesEast()\n{\n  var rover = new Rover {Orientation=Direction.South};\n  var initialLocation = rover.Location;\n\n  rover.TurnLeft();\n\n  Assert.AreEqual(initialLocation, rover.Location);\n  Assert.AreEqual(Direction.East, rover.Orientation);\n}\n</code></pre> <p>And now enough code to make it pass</p> <pre><code>public void TurnLeft()\n{\n  if (Orientation == Direction.North) {\n    Orientation = Direction.West;\n  }\n  else if (Orientation == Direction.West) {\n    Orientation = Direction.South;\n  }\n  else if (Orientation == Direction.South) {\n    Orientation = Direction.East;\n  }\n}\n</code></pre> <p>With everything passing again, we can pause to rethink about refactoring but so far so good from my perspective. There are maybe some superficial changes that could be made, but I can\u2019t make a strong enough argument to implement them.</p> <p>Time to commit and tackle the last requirement!</p>"},{"location":"articles/2020/06/23/mars-rover--implementing-rover--turning-left/#redgreenrefactor-for-rover-facing-east","title":"Red/Green/Refactor for Rover Facing East","text":"<p>Let\u2019s go ahead and write the final test for when the <code>Rover</code> faces <code>East</code></p> <pre><code>[Test]\npublic void AndFacingEastThenTheRoverFacesNorth()\n{\n  var rover = new Rover {Orientation=Direction.East};\n  var initialLocation = rover.Location;\n\n  rover.TurnLeft();\n\n  Assert.AreEqual(initialLocation, rover.Location);\n  Assert.AreEqual(Direction.North, rover.Orientation);\n}\n</code></pre> <p>And now enough code to make it pass</p> <pre><code>public void TurnLeft()\n{\n  if (Orientation == Direction.North) {\n    Orientation = Direction.West;\n  }\n  else if (Orientation == Direction.West) {\n    Orientation = Direction.South;\n  }\n  else if (Orientation == Direction.South) {\n    Orientation = Direction.East;\n  }\n  else if (Orientation == Direction.East) {\n    Orientation = Direction.North;\n  }\n}\n</code></pre>"},{"location":"articles/2020/06/23/mars-rover--implementing-rover--turning-left/#there-seems-to-be-a-pattern","title":"There Seems To Be a Pattern","text":"<p>Now that we have a few tests in place, it seems like all of the tests are following a pretty straightforward pattern:</p> <ol> <li>Create a <code>Rover</code> with a particular <code>Orientation</code></li> <li>Get the starting <code>Location</code></li> <li>Call <code>TurnLeft</code></li> <li>Verify that the <code>Location</code> didn\u2019t change</li> <li>Verify that the <code>Orientation</code> is correct</li> </ol>"},{"location":"articles/2020/06/23/mars-rover--implementing-rover--turning-left/#parameterized-testing","title":"Parameterized Testing","text":"<p>When I find myself writing tests where the tests look exactly the same and the only difference is the initial data and expected output, then I start thinking about how to parameterize the test so that the test is run multiple times, but with different parameters. By making this change, we reduce the amount of test code written, without sacrificing readability or maintainability.</p>"},{"location":"articles/2020/06/23/mars-rover--implementing-rover--turning-left/#when-to-not-parameterize","title":"When To Not Parameterize","text":"<p>It seems like it would be easy to write tests in this fashion, so why don\u2019t we use this technique all the time?</p> <p>The primary reason why we wouldn\u2019t want to use this approach is if we would need to pass in a ton of parameters in order to make this test generic enough. A general guideline I use is that if the total parameters are three or less, then parameterization is a good fit. However, if I find out that I need to pass in more than three parameters, it makes me wonder if the tests are really the same test at all.</p> <p>A secondary reason why this approach may not be a good fit is if it clouds our readability and debug-ability of the tests. Recall that our tests help us drive out our requirements and helps us talk in a ubiquitous language. If we can no longer do that easily, then we should not parameterize our tests.</p>"},{"location":"articles/2020/06/23/mars-rover--implementing-rover--turning-left/#refactoring","title":"Refactoring","text":"<p>With all of that said, let\u2019s take a look at our tests for <code>WhenTurningLeft</code> and see what each of the tests have in common.</p> <pre><code>[TestFixture]\npublic class WhenTurningLeft\n{\n  [Test]\n  public void AndFacingNorthThenTheRoverFacesWest()\n  {\n    var rover = new Rover {Orientation=Direction.North};\n    var initialLocation = rover.Location;\n\n    rover.TurnLeft();\n\n    Assert.AreEqual(initialLocation, rover.Location);\n    Assert.AreEqual(Direction.West, rover.Orientation);\n  }\n\n  [Test]\n  public void AndFacingWestThenTheRoverFacesSouth()\n  {\n    var rover = new Rover {Orientation=Direction.West};\n    var initialLocation = rover.Location;\n\n    rover.TurnLeft();\n\n    Assert.AreEqual(initialLocation, rover.Location);\n    Assert.AreEqual(Direction.South, rover.Orientation);\n  }\n\n  [Test]\n  public void AndFacingSouthThenTheRoverFacesEast()\n  {\n    var rover = new Rover {Orientation=Direction.South};\n    var initialLocation = rover.Location;\n\n    rover.TurnLeft();\n\n    Assert.AreEqual(initialLocation, rover.Location);\n    Assert.AreEqual(Direction.East, rover.Orientation);\n  }\n\n  [Test]\n  public void AndFacingEastThenTheRoverFacesNorth()\n  {\n    var rover = new Rover {Orientation=Direction.East};\n    var initialLocation = rover.Location;\n\n    rover.TurnLeft();\n\n    Assert.AreEqual(initialLocation, rover.Location);\n    Assert.AreEqual(Direction.North, rover.Orientation);\n  }\n}\n</code></pre> <p>If we examine a bit closer, we\u2019ll notice that the only differences are the starting <code>Orientation</code> and the expected <code>Orientation</code>. Since those are the only values that we would need to parameterize, this is a great candidate for parameterization!</p> <p>So starting off slow, we\u2019re going to refactor the <code>AndFacingEastThenTheRoverFacesNorth</code> test to use NUnit\u2019s TestCase attribute to pass in the parameters. Even though we\u2019re using NUnit for this functionality, most test frameworks support this concept, just with different syntax.</p> <pre><code>[Test]\n[TestCase(Direction.East, Direction.North, TestName = \"AndFacingEastThenTheRoverFacesNorth\")]\npublic void RoverTurningLeft(Direction start, Direction expected)\n{\n  var rover = new Rover { Orientation = start };\n  var initialLocation = rover.Location;\n\n  rover.TurnLeft();\n\n  Assert.AreEqual(expected, rover.Orientation);\n  Assert.AreEqual(initialLocation, rover.Location);\n}\n</code></pre> <p>With these changes in place, let\u2019s break down what our approach was.</p> <p>First, we added the <code>TestCase</code> attribute to the test as this allows us to specify parameters. In this case, we\u2019re passing in two parameters (<code>Direction.East</code> and <code>Direction.North</code>). In addition, we\u2019re also giving this <code>TestCase</code> a unique test name by setting the <code>TestName</code> property to <code>AndFacingEastThenTheRoverFacesNorth</code>. By setting this property, we\u2019re controlling what this test will show up in the test runner.</p> <p>Second, we changed the signature of the test method to take in two <code>Direction</code>s, one for the <code>start</code> and the other for the <code>expected</code> direction for <code>Rover</code>. These new parameters line up with the ordering of parameters in the <code>TestCase</code>. In addition, since this test is going to be a bit more generic, I renamed this method to <code>RoverTurningLeft</code> because the <code>TestName</code> is going to have my requirement and this method is the generic shell.</p> <p>Finally, we updated how we initialized the <code>Rover</code> by setting it\u2019s <code>Orientation</code> to be <code>start</code> and we changed our <code>Assert</code> to be on <code>expected</code>.</p>"},{"location":"articles/2020/06/23/mars-rover--implementing-rover--turning-left/#verifying-the-changes","title":"Verifying The Changes","text":"<p>With this new test in place, let\u2019s run it and verify that our changes worked</p> <p> </p> Single test passing for when Rover faces North <p>Nice! Let\u2019s go ahead and add additional TestCase attributes for the other tests</p> <pre><code>[Test]\n[TestCase(Direction.North, Direction.West, TestName = \"AndFacingNorthThenTheRoverFacesWest\")]\n[TestCase(Direction.West, Direction.South, TestName = \"AndFacingWestThenTheRoverFacesSouth\")]\n[TestCase(Direction.South, Direction.East, TestName = \"AndFacingSouthThenTheRoverFacesEast\")]\n[TestCase(Direction.East, Direction.North, TestName = \"AndFacingEastThenTheRoverFacesNorth\")]\npublic void RoverTurningLeft(Direction start, Direction expected)\n{\n  var rover = new Rover { Orientation = start };\n  var initialLocation = rover.Location;\n\n  rover.TurnLeft();\n\n  Assert.AreEqual(expected, rover.Orientation);\n  Assert.AreEqual(initialLocation, rover.Location);\n}\n</code></pre> <p>And if we run our test suite, we verify that everything passes!</p> <p> </p> All four tests passing <p>With this in place, we can remove the other tests in this file, which yields the following test class.</p> <pre><code>[TestFixture]\npublic class WhenTurningLeft\n{\n  [Test]\n  [TestCase(Direction.North, Direction.West, TestName = \"AndFacingNorthThenTheRoverFacesWest\")]\n  [TestCase(Direction.West, Direction.South, TestName = \"AndFacingWestThenTheRoverFacesSouth\")]\n  [TestCase(Direction.South, Direction.East, TestName = \"AndFacingSouthThenTheRoverFacesEast\")]\n  [TestCase(Direction.East, Direction.North, TestName = \"AndFacingEastThenTheRoverFacesNorth\")]\n  public void RoverTurningLeft(Direction start, Direction expected)\n  {\n    var rover = new Rover { Orientation = start };\n    var initialLocation = rover.Location;\n\n    rover.TurnLeft();\n\n    Assert.AreEqual(expected, rover.Orientation);\n    Assert.AreEqual(initialLocation, rover.Location);\n  }\n}\n</code></pre> <p>Already we can see how easy it would be to extend this test suite if a new <code>Direction</code> were to be added by adding the new <code>TestCase</code> and updating the other ones as needed.</p>"},{"location":"articles/2020/06/23/mars-rover--implementing-rover--turning-left/#wrapping-up","title":"Wrapping Up","text":"<p>Goodness, just like that we have a <code>Rover</code> that knows how to <code>MoveForward</code>, <code>MoveBackward</code>, and <code>TurnLeft</code>! In this post, we added some new functionality to <code>Rover</code> by first examining the requirements and implementing them one at a time. From there, we noticed during our refactor step that our test code looked similar, just differing on inputs. This, in turn, inspired us to look at parameterized testing which allowed us to drastically reduce the amount of code needed for the various use cases and allows us to add additional cases easier in the future. In the next post, we start taking a look at some interesting patterns that <code>Rover</code> is exhibiting in its three methods.</p>"},{"location":"articles/2020/05/19/mars-rover---modeling-concepts/","title":"Mars Rover - Modeling Concepts","text":"<p>In the last post, we took a look at the problem description for Mars Rover and developed a set of concepts for the problem. From these concepts, we were able to develop common terminology and determine the relationships between the concepts. In this post, I\u2019m going to show how I think about software design in general and how to apply them when modeling in code.</p> <p>As a note, I\u2019ll be showing code in both C# (for Object-Oriented approaches) and F# (for Functional Programming approaches). Once again, these concepts are fundamentals, but depending on your technology stack, the implementations will vary.</p>"},{"location":"articles/2020/05/19/mars-rover---modeling-concepts/#design-guidelines","title":"Design Guidelines","text":"<p>When I\u2019m designing my models, my end goal is to produce software that captures the problem at hand using the same terms that the business uses. By striving for this goal, I can have richer conversations with my stakeholders when I run into interesting interactions of various business rules and can speak to them using the right terminology. In addition to capturing the problem, I will focus on designing my models in such a way that a developer can\u2019t violate a business rule because the code won\u2019t compile. At this point, I would have made illegal states unrepresentable in my code.</p> <p>I first came across this term while reading Scott Wlashcin\u2018s work on the terrific F# for Fun and Profit website and it immediately resonated with me. I\u2019ve definitely been bitten before working in a codebase where I wrote some code that compiled but blew up in my face during runtime because the parameter I passed in wasn\u2019t valid for the method I was calling. Wouldn\u2019t it be nice if the compiler told me while I was writing the code that what I was doing wouldn\u2019t work? By thinking a bit more about the models being used and what some of their properties are, we can make this goal achievable.</p>"},{"location":"articles/2020/05/19/mars-rover---modeling-concepts/#modeling-types","title":"Modeling Types","text":"<p>With these goals in mind, when it comes to modeling concepts, I naturally gravitate to types and find that types will fall in one of three categories.</p>"},{"location":"articles/2020/05/19/mars-rover---modeling-concepts/#the-type-has-a-finite-number-of-values","title":"The Type Has a Finite Number of Values","text":"<p>If the type has a finite number of valid values, then we can remove error conditions by defining the type to only be one of those possible options. For those from an Object-Oriented background, enums are a great example of modeling these types as you can explicitly set a label for the different values. For those from a Functional background, sum types are a great way to model these choices.</p> <p>Some examples of such a type include the states in the U.S., the suits for a deck of playing cards, or the months in a year.</p> <pre><code>public enum State\n{\n  Alabama, Alaska, California, Delaware,\n  Florida, Georgia, Tennessee, Wyoming\n}\n\npublic enum Suit\n{\n  Hearts, Clubs, Spades, Diamonds\n}\n\npublic enum Months\n{\n  January, February, March, April, May, June,\n  July, August, September, October, November, December\n}\n</code></pre> <pre><code>type State = Alabama  | Alaska  | California\n           | Delaware | Florida | Georgia\n           | Tennesee | Wyoming\n</code></pre>"},{"location":"articles/2020/05/19/mars-rover---modeling-concepts/#the-type-has-an-infinite-number-of-values","title":"The Type Has an Infinite Number of Values","text":"<p>For other types, however, there are so many possible valid values that it\u2019s impossible to list all of them. For example, if we were looking at valid house numbers for an address, any positive integer would be valid so good luck on defining every positive number as an enum or sum type.</p> <p>In these cases, I will leverage built-in primitives to model the concept at first. So in the case of HouseNumber, an integer might be a good enough spot to start. However, if I then find myself writing code that can work on integers, but shouldn\u2019t work on HouseNumbers, then I might wrap a stronger type around the integer (see below).</p> <pre><code>// If the value isn't a major component of the design, we can use a primitive type\nint houseNumber;\n\n// However, if the type is a major concept to the domain at hand,\n// it makes sense to lift it to its own type\npublic class HouseNumber\n{\n  public int Value {get;}\n  public StreetNumber(int input)\n  {\n    // validation logic\n    Value = input;\n  }\n}\n\n// The difference between the two approaches is that in the first case, this would work\nint houseNumber = 400;\nMath.Sqrt(houseNumber);\n\n// But this wouldn't\nvar houseNumber = new HouseNumber(400);\nMath.Sqrt(houseNumber); // fails to compile with \"cannot convert from HouseNumber to double\"\n</code></pre> <pre><code>// If the value isn't a major component of the design, we can use a primitive type\nlet houseNumber:int;\n\n// However, if the type is a major concept to the domain at hand,\n// it makes sense to lift it to its own type (single case sum type)\ntype HouseNumber = HouseNumber of int\n\n// The difference between the two approaches is that in the first case, this would work\nlet houseNumber = 400;\nMath.Sqrt(houseNumber);\n\n// But this wouldn't\nlet houseNumber = HouseNumber 400\nMath.Sqrt(houseNumber); // fails to compile with\n                        // \"This expression was expected to have type 'float' but here has type 'HouseNumber'\"\n</code></pre>"},{"location":"articles/2020/05/19/mars-rover---modeling-concepts/#the-type-is-a-composition-of-other-types","title":"The Type Is a Composition of Other Types","text":"<p>As the saying goes, large programs are built by composing a bunch of smaller programs, and types are no different. As we begin to model more complicated types, it\u2019s natural to start thinking about types being composed of other types. For these types, we\u2019ll leverage either objects (if following OO) or records (if following FP).</p> <p>One way you can determine if you\u2019re needing a composite type like this is if you find yourself using the word and or has when describing the type, then it\u2019s a composition. For example:</p> <p>An Address has a HouseNumber, it has a StreetName, it has a State.</p> <p>An Address consists of a HouseNumber and a StreetName and a State</p> <pre><code>public class Address\n{\n  public int HouseNumber {get; set;}\n  public string StreetName {get; set;}\n  public State State {get; set;}\n}\n</code></pre> <pre><code>type Address = {\n  houseNumber:int,\n  streetName:string,\n  state:State\n}\n</code></pre>"},{"location":"articles/2020/05/19/mars-rover---modeling-concepts/#modeling-types_1","title":"Modeling Types","text":"<p>Now that we\u2019ve talked about some different modeling techniques, let\u2019s see how we can apply those rules as we start to model Mars Rover. From the previous post, we were able to derive the following concepts and relationships:</p> <ul> <li>A Rover has a Location and an Orientation</li> <li>Orientation is the Direction that a Rover is facing</li> <li>Location is the coordinates that the Rover is located at</li> <li>A Command is something that a Rover receives from the User</li> <li>A Direction can be North, East, South, or West</li> <li>A Command can be Move Forward, Move Backward, Turn Left,  Turn Right, or Quit</li> </ul> <p>Yielding the following graph</p> <p> </p> Domain model relationships where Rover has a Location and an Orientation. Orientation is a Direction and Command is not related to anything. <p>Given the above rules, we can start taking a look at how to model these in code! We\u2019ll first start with the models that don\u2019t have a dependency, and then build up from there</p>"},{"location":"articles/2020/05/19/mars-rover---modeling-concepts/#modeling-direction","title":"Modeling Direction","text":"<p>From the above requirements, Direction can only be one of four possible values (North, East, South, West). So based on that, it looks like we can leverage the first rule and model Direction like so:</p> <pre><code>public enum Direction\n{\n  North, South, East, West\n}\n</code></pre> <pre><code>type Direction = North | East | South | West\n</code></pre>"},{"location":"articles/2020/05/19/mars-rover---modeling-concepts/#modeling-command","title":"Modeling Command","text":"<p>From the above requirements, Command can only be one of five possible values (MoveForward, MoveBackward, TurnLeft, TurnRight, and Quit). Based on that, we can once again leverage the first rule and model Command like so:</p> <pre><code>public enum Command\n{\n  MoveForward, MoveBackward,\n  TurnLeft, TurnRight,\n  Quit\n}\n</code></pre> <pre><code>type Command = MoveForward | MoveBackward\n             | TurnLeft | TurnRight | Quit\n</code></pre>"},{"location":"articles/2020/05/19/mars-rover---modeling-concepts/#modeling-location","title":"Modeling Location","text":"<p>After talking more with our Subject Matter Expert, a Location is the Coordinate where the Rover is located.</p> <p>Aha! A new concept!</p> <p>When we ask additional questions, we find out that a Coordinate refers to the Cartesian Coordinate System and for the problem we\u2019re solving, we can assume that a Coordinate represents two numbers where the first number represents the location from the x-axis and the second number represents the location from the y-axis.</p> <p>With this new information, our mental model has changed to be the following</p> <p> </p> Domain model relationships where Rover has a Location and an Orientation. Location is a Coordinate where Coordinate has an X and Y value. Orientation is a Direction and Command is not related to anything. <p>Going into further discussion, we find out that both X and Y will be whole numbers for our emulation and that they can be negative. Based on these properties, it sounds like X and Y can be modeled as integers and therefore fall under the second rule.</p> <p>Given that a Coordinate has to have both an X and Y value, it sounds like Coordinate falls under the third rule and that this concept is a composition of X and Y.</p> <p><pre><code>public class Coordinate\n{\n  public int X {get; set;}\n  public int Y {get; set;}\n}\n</code></pre> <pre><code>type Coordinate = {x:int; y:int}\n</code></pre></p>"},{"location":"articles/2020/05/19/mars-rover---modeling-concepts/#modeling-orientation","title":"Modeling Orientation","text":"<p>From the above requirements, it seems like Orientation is what we call the Direction that the Rover is facing. Based on that, this sounds like a property that Rover would have.</p>"},{"location":"articles/2020/05/19/mars-rover---modeling-concepts/#modeling-rover","title":"Modeling Rover","text":"<p>Now that we have both the Direction and Coordinate concepts designed, we can start designing Rover. From the requirements, it looks like Rover is a combination of Direction (known as Orientation) and a Coordinate (known as a Location). Based on that, Rover falls under the third rule and looks like the following.</p> <pre><code>public class Rover\n{\n  public Direction Orientation {get; set;}\n  public Coordinate Location {get; set;}\n}\n</code></pre> <pre><code>type Rover = {\n  orientation:Direction;\n  location:Coordinate;\n}\n</code></pre>"},{"location":"articles/2020/05/19/mars-rover---modeling-concepts/#wrapping-up","title":"Wrapping Up","text":"<p>In this post, we implemented the basic types needed to solve the Mars Rover kata! We first started by taking a look at the concepts identified earlier and thought about the characteristics of the type which helped guide us to build software that both uses the terms of the problem domain and also prevents us from creating errors by making illegal states unrepresentable. In the next post, we\u2019ll start adding functionality to our application.</p>"},{"location":"articles/2020/05/19/mars-rover---modeling-concepts/#additional-reading","title":"Additional Reading","text":"<ul> <li>Designing with types: Making illegal states unrepresentable by Scott Wlaschin</li> <li>Domain Model by Martin Fowler</li> </ul>"},{"location":"articles/2020/06/30/mars-rover--implementing-rover--refactoring-rover/","title":"Mars Rover \u2013 Implementing Rover \u2013 Refactoring Rover","text":"<p>Welcome to the eighth installment of Learning Through Example \u2013 Mars Rover! In this post, we\u2019re going to examine the <code>Rover</code> class and see what refactoring we can do based on some patterns we\u2019re seeing with <code>MoveForward</code>, <code>MoveBackward</code>, and <code>TurnLeft</code>. After looking at the characteristics, we\u2019ll explore a couple of different approaches with their pros and cons. Finally, we\u2019ll make the refactor, using our test suite to make sure we didn\u2019t regress in functionality.</p>"},{"location":"articles/2020/06/30/mars-rover--implementing-rover--refactoring-rover/#whats-the-problem","title":"What\u2019s The Problem","text":"<p>If we look at the definition for <code>Rover</code>, it becomes clear that we have some major code duplication going on with regards to its <code>MoveForward</code>, <code>MoveBackward</code>, and <code>TurnLeft</code> methods.</p> <pre><code>public class Rover\n{\n  public Direction Orientation {get; set;}\n  public Coordinate Location {get; set;}\n\n  public Rover()\n  {\n    Orientation = Direction.North;\n    Location = new Coordinate {X=0, Y=0};\n  }\n\n  public void MoveForward()\n  {\n    if (Orientation == Direction.North) {\n      Location = Location.AdjustYBy(1);\n    }\n    if (Orientation == Direction.South) {\n      Location = Location.AdjustYBy(-1);\n    }\n    if (Orientation == Direction.East) {\n      Location = Location.AdjustXBy(1);\n    }\n    if (Orientation == Direction.West) {\n      Location = Location.AdustXBy(-1);\n    }\n  }\n\n  public void MoveBackward()\n  {\n    if (Orientation == Direction.North) {\n      Location = Location.AdjustYBy(-1);\n    }\n    if (Orientation == Direction.South) {\n      Location = Location.AdjustYBy(1);\n    }\n    if (Orientation == Direction.East) {\n      Location = Location.AdjustXBy(-1);\n    }\n    if (Orientation == Direction.West) {\n      Location = Location.AdjustXBy(1);\n    }\n  }\n\n  public void TurnLeft()\n  {\n    if (Orientation == Direction.North) {\n      Orientation = Direction.West;\n    }\n    else if (Orientation == Direction.West) {\n      Orientation = Direction.South;\n    }\n    else if (Orientation == Direction.South) {\n      Orientation = Direction.East;\n    }\n    else if (Orientation == Direction.East) {\n      Orientation = Direction.North;\n    }\n  }\n}\n</code></pre> <p>Based on the implementations, it seems like knowing what <code>Direction</code> the <code>Rover</code> is facing is a key rule to determine what update the <code>Rover</code> needs to do. The big pain point with this block of statements is that if we added a new <code>Direction</code> (like NorthEast), then we\u2019d have to update these statements in multiple places even though all of these instances are referring to the same concept (i.e is the <code>Rover</code> facing this <code>Direction</code>). The other, more nuanced issue is that if we added a new <code>Direction</code>, there\u2019s nothing forcing us to update all of these places because of our use of <code>if/else</code>.</p>"},{"location":"articles/2020/06/30/mars-rover--implementing-rover--refactoring-rover/#how-to-resolve","title":"How To Resolve?","text":"<p>From what we\u2019re seeing, the primary issue are the duplicated <code>if/else</code> statements, so one of our primary design goals should be to have that logic in one place (instead of three different places). From there, we\u2019d also like to have a way for the compiler to force us to handle the different <code>Directions</code> that the <code>Rover</code> is facing.</p>"},{"location":"articles/2020/06/30/mars-rover--implementing-rover--refactoring-rover/#isolating-the-ifelse","title":"Isolating the if/else","text":"<p>In order to isolate the <code>if/else</code>, let\u2019s go ahead and extract the logic to a new private method</p> <pre><code>private xxx Execute(xxxx)\n{\n  if (Orientation == Direction.North) {\n    //\n  }\n  else if (Orientation == Direction.South) {\n    // \n  }\n  else if (Orientation == Direction.East) {\n    //\n  }\n  else if (Orientation == Direction.West) {\n    //\n  }\n}\n</code></pre> <p>We\u2019ve now got the <code>if/else</code> isolated, but there are some questions on what the return type of this method should be or what parameters it will take. In order to answer those questions, let\u2019s take a look at a couple of different approaches.</p>"},{"location":"articles/2020/06/30/mars-rover--implementing-rover--refactoring-rover/#using-a-functional-approach","title":"Using a Functional Approach","text":"<p>Now that we have a method that can operate given the <code>Rover</code>'s <code>Orientation</code>, one approach we could do is to modify this method to take in an Action for every possible <code>Orientation</code>. Then based on the <code>Orientation</code>, the appropriate <code>Action</code> is called.</p> <pre><code>private void Execute(Action ifNorth, Action ifSouth, Action ifEast, Action ifWest)\n{\n  if (Orientation == Direction.North) {\n    ifNorth();\n  }\n  else if (Orientation == Direction.South) {\n    ifSouth();\n  }\n  else if (Orientation == Direction.East) {\n    ifEast();\n  }\n  else if (Orientation == Direction.West) {\n    ifWest();\n  }\n}\n</code></pre> <p>With this implementation, <code>TurnLeft</code> would look like</p> <pre><code>public void TurnLeft()\n{\n  Action ifNorth = () =&gt; Orientation = Direction.West;\n  Action ifWest = () =&gt; Orientation = Direction.South;\n  Action ifSouth = () =&gt; Orientation = Direction.East;\n  Action ifEast = () =&gt; Orientation = Direction.North;\n\n  Execute(ifNorth, ifSouth, ifEast, ifWest);\n}\n</code></pre> <p>And <code>MoveForward</code> would look like</p> <pre><code>public void MoveForward()\n{\n  Action ifNorth = () =&gt; Location=Location.AdjustYBy(1);\n  Action ifSouth = () =&gt; Location=Location.AdjustYBy(-1);\n  Action ifEast = () =&gt; Location=Location.AdjustXBy(1);\n  Action ifWest = () =&gt; Location=Location.AdjustXBy(-1);\n\n  Execute(ifNorth, ifSouth, ifEast, ifWest);\n}\n</code></pre>"},{"location":"articles/2020/06/30/mars-rover--implementing-rover--refactoring-rover/#advantages","title":"Advantages","text":"<p>The primary advantage of this approach is that in order to call <code>Execute</code>, you have to pass a parameter for the different directions. If you fail to do so, the code will fail to compile, which forces developers to handle the various use cases.</p> <p>Furthermore, if there\u2019s a bug in any of the <code>Actions</code>, then for troubleshooting, we\u2019d need to know what method caused the problem and what the rover\u2019s <code>Orientation</code> was and we can quickly figure out what the problem is.</p>"},{"location":"articles/2020/06/30/mars-rover--implementing-rover--refactoring-rover/#drawbacks","title":"Drawbacks","text":"<p>When using this approach, one thing to keep in mind is if we need to support additional <code>Direction</code>s in the future. The <code>Execute</code> method is already taking in four parameters and keeping them in the right order is difficult enough. What about five, six, or ten directions to support? The function signature would quickly become unwieldy.</p> <p>In addition, the other downside to this approach is that if any of the <code>Action</code>s were to become much more complicated, it would start cluttering up the respective <code>Move</code> or <code>Turn</code> methods.</p>"},{"location":"articles/2020/06/30/mars-rover--implementing-rover--refactoring-rover/#using-an-object-oriented-approach","title":"Using an Object-Oriented Approach","text":"<p>Now that we have a method that can operate given the Rover\u2018s Orientation, another approach is to introduce the Strategy pattern where we would need to create the following types</p> <ul> <li><code>IMovementStrategy</code> \u2013 an interface that lists the different kinds of movements that can be done (currently <code>MoveForward</code>, <code>MoveBackward</code>, and <code>TurnLeft</code>)</li> <li><code>NorthMovementStrategy</code> \u2013 a new class that implements the <code>IMovementStrategy</code> and is responsible for the various business rules when moving and facing <code>North</code></li> <li><code>GetMovementStrategy</code> \u2013 a method in <code>Rover</code> that for the current <code>Orientation</code>, returns the right implementation of <code>IMovementStrategy</code></li> <li>Update the existing move methods to use <code>GetMovementStrategy</code></li> </ul> <p>First, let\u2019s take a look at the <code>IMovementStrategy</code> definition</p> <pre><code>internal IMovementStrategy()\n{\n  Coordinate MoveForward(Coordinate coordinate);\n  Coordinate MoveBackward(Coordinate coordinate);\n  Direction TurnLeft();\n}\n</code></pre> <p>The key takeaway for this type is that if we need to add an additional movement (like <code>TurnRight</code>), we\u2019ll need to update this interface. Taking a closer look, we\u2019ve defined the signatures for all of the methods to return a value (either a <code>Location</code> or a <code>Direction</code>). We have to make this change because no <code>Strategy</code> will have access to the <code>Rover</code> itself, so it\u2019ll need to return the correct value for <code>Rover</code> to hold onto.</p> <p>Next, let\u2019s take a look at an example implementation for when the <code>Orientation</code> is <code>North</code></p> <pre><code>internal class NorthMovementStrategy : IMovementStrategy\n{\n  public Coordinate MoveForward(Coordinate coordinate)\n  {\n    return coordinate.AdjustYBy(1);\n  }\n  public Coordinate MoveBackward(Coordinate coordinate)\n  {\n    return coordinate.AdjustYBy(-1);\n  }\n  public Direction TurnLeft()\n  {\n    return Direction.West;\n  }\n}\n</code></pre> <p>If you look closer, we\u2019ve essentially moved the business rules for the three methods from the existing business rules. We would repeat this process for the other <code>Direction</code>s (yielding a <code>SouthMovementStrategy</code>, an <code>EastMovementStrategy</code>, and a <code>WestMovementStrategy</code>)</p> <p>Now that we have the various strategies in place, we can create the <code>GetMovementStrategy</code> method</p> <pre><code>private IMovementStrategy GetStrategy()\n{\n  if (Orientation == Direction.North) {\n   return new NorthMovementStrategy();\n  }\n  else if (Orientation == Direction.South) {\n    return new SouthMovementStrategy();\n  }\n  else if (Orientation == Direction.East) {\n    return new EastMovementStrategy();\n  }\n  else if (Orientation == Direction.West) {\n    return new WestMovementStrategy();\n  }\n}\n</code></pre> <p>Which then would allow us to refactor <code>TurnLeft</code> as the following</p> <pre><code>public void TurnLeft()\n{\n  var movementStrategy = GetMovementStrategy();\n  Orientation = movementStrategy.TurnLeft();\n}\n</code></pre> <p>And <code>MoveForward</code> as</p> <pre><code>public void MoveForward()\n{\n  var movementStrategy = GetMovementStrategy();\n  Location = movementStrategy.MoveForward(Location);\n}\n</code></pre>"},{"location":"articles/2020/06/30/mars-rover--implementing-rover--refactoring-rover/#advantages_1","title":"Advantages","text":"<p>When using this approach, we can easily extend functionality when new <code>Direction</code>s are added. For example, if we added <code>NorthEast</code>, then we would create a new <code>MovementStrategy</code>, have it implement the interface and fill in business rules (also allowing us to easily add tests as we go). From there, we would update the <code>GetMovementStrategy</code> method to return the new strategy. By adding new functionality by primarily writing new code and making very little modifications to existing code, we can adhere to the Open/Closed Principle from SOLID design.</p>"},{"location":"articles/2020/06/30/mars-rover--implementing-rover--refactoring-rover/#drawbacks_1","title":"Drawbacks","text":"<p>The major drawback of this approach is that we had to create a lot of boilerplate code to make everything hang. One interface, four implementations, and a factory method for creating the appropriate strategy for a given <code>Orientation</code>.</p> <p>The other, more subtle, drawback is that one must be aware of the pivot point of change. For example, we originally made this refactor based on strategies of what to do when faced in a given <code>Direction</code>. This made sense as an easy refactor. However, we\u2019ve now made it easy to support a new <code>Direction</code> when added, but if we were to add a new <code>Command</code> instead, we would update the <code>IMovementStrategy</code> with the new method which would break all implementations. Once that happens, we have no choice but to implement the whole feature at once.</p> <p>Personally, I\u2019m a proponent of delivering value in small chunks and verify that we\u2019re building the right thing, but in this case, this approach doesn\u2019t make that easy.</p>"},{"location":"articles/2020/06/30/mars-rover--implementing-rover--refactoring-rover/#start-the-refactor","title":"Start the Refactor","text":"<p>Given the above options, I\u2019m going to choose the more functional approach mostly because the business rules in my case are one-liners. If they were more complex, then I\u2019d lean heavily more towards the <code>Strategy</code> pattern.</p> <p>With that in mind, I\u2019m going to go ahead and define the general <code>Execute</code> method as such</p> <pre><code>private void Execute(Action ifNorth, Action ifSouth, Action ifEast, Action ifWest)\n{\n  if (Orientation == Direction.North) {\n    ifNorth();\n  }\n  else if (Orientation == Direction.South) {\n    ifSouth();\n  }\n  else if (Orientation == Direction.East) {\n    ifEast();\n  }\n  else if (Orientation == Direction.West) {\n    ifWest();\n  }\n}\n</code></pre> <p>Once that\u2019s in place, I can go ahead and refactor <code>TurnLeft</code> to use the <code>Execute</code> method.</p> <pre><code>public void TurnLeft()\n{\n  Action ifNorth = () =&gt; Orienation = Direction.West;\n  Action ifWest = () =&gt; Orientation = Direction.South;\n  Action ifSouth = () =&gt; Orientation = Direction.East;\n  Action ifEast = () =&gt; Orientation = Direction.North;\n\n  Execute(ifNorth, ifSouth, ifEast, ifWest);\n}\n</code></pre> <p>Now that I\u2019ve updated <code>TurnLeft</code>, I can run the tests to make sure I didn\u2019t break functionality. This is where the power of automated tests come to play. I can have higher confidence that my refactor didn\u2019t break anything because of the test suite.</p> <p>After verifying  that the tests pass for <code>TurnLeft</code>, I\u2019ll continue refactoring <code>MoveForward</code></p> <pre><code>public void MoveForward()\n{\n  Action ifNorth = () =&gt; Location=Location.AdjustYBy(1);\n  Action ifSouth = () =&gt; Location=Location.AdjustYBy(-1);\n  Action ifEast = () =&gt; Location=Location.AdjustXBy(1);\n  Action ifWest = () =&gt; Location=Location.AdjustXBy(-1);\n\n  Execute(ifNorth, ifSouth, ifEast, ifWest);\n}\n</code></pre> <p>and <code>MoveBackward</code> using the same technique</p> <pre><code>public void MoveForward()\n{\n  Action ifNorth = () =&gt; Location=Location.AdjustYBy(1);\n  Action ifSouth = () =&gt; Location=Location.AdjustYBy(-1);\n  Action ifEast = () =&gt; Location=Location.AdjustXBy(1);\n  Action ifWest = () =&gt; Location=Location.AdjustXBy(-1);\n\n  Execute(ifNorth, ifSouth, ifEast, ifWest);\n}\n\npublic void MoveBackward()\n{\n  Action ifNorth = () =&gt; Location=Location.AdjustYBy(-1);\n  Action ifSouth = () =&gt; Location=Location.AdjustYBy(1);\n  Action ifEast = () =&gt; Location=Location.AdjustXBy(-1);\n  Action ifWest = () =&gt; Location=Location.AdjustXBy(1);\n\n  Execute(ifNorth, ifSouth, ifEast, ifWest);\n}\n</code></pre>"},{"location":"articles/2020/06/30/mars-rover--implementing-rover--refactoring-rover/#one-final-touch","title":"One Final Touch","text":"<p>So at this point, we\u2019ve successfully refactored <code>MoveForward</code>, <code>MoveBackward</code>, and <code>TurnLeft</code> to use <code>Execute</code> instead of their own logic, but the more I look at <code>Execute</code>, the more I want to convert the <code>if/else</code> into a <code>switch</code> statement because a <code>switch</code> is going to force us to implement the different <code>Direction</code>s whereas the <code>if/else</code> pattern did not. So let\u2019s go ahead and make that change.</p> <pre><code>public void Execute(Action ifNorth, Action ifSouth, Action ifEast, Action ifWest)\n{\n  switch(Orientation)\n  {\n    case Direction.North: ifNorth(); break;\n    case Direction.South: ifSouth(); break;\n    case Direction.East: ifEast(); break;\n    case Direction.West: ifWest(); break;\n  }\n}\n</code></pre> <p>And we can verify that our refactored worked like a top!</p>"},{"location":"articles/2020/06/30/mars-rover--implementing-rover--refactoring-rover/#wrapping-up","title":"Wrapping Up","text":"<p>At this point, we\u2019ve made some good changes to the <code>Rover</code> with how the various methods work by examining the duplication (the <code>if/else</code>), extracting the duplication to a single area, and the compared the functional approach and the OOP approach via the Strategy pattern. In the next post, we\u2019ll take on implementing <code>TurnRight</code> and see how our new refactor works in practice when adding new functionality!</p>"},{"location":"articles/2020/07/07/mars-rover--implementing-rover--turn-right/","title":"Mars Rover \u2013 Implementing Rover \u2013 Turn Right","text":"<p>Welcome to the ninth installment of Learning Through Example \u2013 Mars Rover! In this post, we\u2019re going to start driving out the functionality for the Rover and the business rules for turning right! First, we\u2019ll take a look at the requirements to make sure we have an idea of what\u2019s needed. From there, we\u2019ll start implementing the requirements. By the end of this post, we\u2019ll have a Rover that can do perform all of the commands!</p>"},{"location":"articles/2020/07/07/mars-rover--implementing-rover--turn-right/#turning-in-place-by-turning-right","title":"Turning in Place By Turning Right","text":"<p>If we look back at the original requirements for turning right, we find this single line as the requirement</p> <p>When the rover is told to turn right, it will rotate 90 degrees to the right, but not change its location</p> <p>Given this requirement, we\u2019re able to double-check with our Subject Matter Expert that the <code>Rover</code> is essentially rotating in place which yields the following requirements.</p> <ul> <li>Given the <code>Rover</code> is facing <code>North</code>, when it turns right, then the <code>Rover</code> should be facing <code>East</code> at the same <code>Coordinate</code></li> <li>Given the <code>Rover</code> is facing <code>East</code>, when it turns right, then the <code>Rover</code> should be facing <code>South</code> at the same <code>Coordinate</code></li> <li>Given the <code>Rover</code> is facing <code>South</code>, when it turns right, then the <code>Rover</code> should be facing <code>West</code> at the same <code>Coordinate</code></li> <li>Given the <code>Rover</code> is facing <code>West</code>, when it turns right, then the <code>Rover</code> should be facing <code>North</code> at the same <code>Coordinate</code></li> </ul> <p>So far, the requirements are very similar to when we were implementing <code>TurnLeft</code>, so let\u2019s see if we can leverage the same setup!</p>"},{"location":"articles/2020/07/07/mars-rover--implementing-rover--turn-right/#redgreenrefactor-for-rover-facing-north","title":"Red/Green/Refactor For Rover Facing North","text":"<p>Given what we learned when we were implementing the rules for <code>TurnLeft</code>, I\u2019m going to go ahead and copy the test and add the single case for when <code>Rover</code> faces <code>North</code></p> <pre><code>[Test]\n[TestCase(Direction.North, Direction.East, TestName = \"AndFacingNorthThenTheRoverFacesEast\")]\npublic void RoverTurningRight(Direction start, Direction expected)\n{\n  var rover = new Rover { Orientation = start };\n  var initialLocation = rover.Location;\n\n  rover.TurnRight();\n\n  Assert.AreEqual(expected, rover.Orientation);\n  Assert.AreEqual(initialLocation, rover.Location);\n}\n</code></pre> <p>Since <code>TurnRight</code> doesn\u2019t exist, this test will fail so let\u2019s go ahead and write enough code to make it pass. For the implementation, I\u2019m going to go ahead and take advantage of the pattern that we found last time, yielding the following</p> <pre><code>public void TurnRight()\n{\n  Action ifNorth = () =&gt; Orientation = Direction.East;\n  Action ifSouth = () =&gt; {};\n  Action ifEast = () =&gt; {};\n  Action ifWest = () =&gt; {};\n\n  Execute(ifNorth, ifSouth, ifEast, ifWest);\n}\n</code></pre> <p>One thing to note is to take a look at how we defined <code>ifSouth</code>, <code>ifEast</code>, and <code>ifWest</code>. Since we\u2019re still wanting to implement one requirement at a time, I\u2019ve defined them to do nothing so that if <code>Execute</code> is called, nothing will happen. Alternatively, I could have defined them to throw an exception, but I believe that\u2019s a bit much since we\u2019ll be implementing the functionality soon!</p> <p>With that implementation, our test passes, so it\u2019s time to go on to the next requirement!</p>"},{"location":"articles/2020/07/07/mars-rover--implementing-rover--turn-right/#rapid-test-creation-with-testcase","title":"Rapid Test Creation with TestCase","text":"<p>With a parameterized unit test in place, adding a new test is as simple as adding a new <code>TestCase</code> attribute.</p> <pre><code>[Test]\n[TestCase(Direction.North, Direction.East, TestName = \"AndFacingNorthThenTheRoverFacesEast\")]\n[TestCase(Direction.East, Direction.South, TestName = \"AndFacingEastThenTheRoverFacesSouth\")]\npublic void RoverTurningRight(Direction start, Direction expected)\n{\n  var rover = new Rover { Orientation = start };\n  var initialLocation = rover.Location;\n\n  rover.TurnRight();\n\n  Assert.AreEqual(expected, rover.Orientation);\n  Assert.AreEqual(initialLocation, rover.Location);\n}\n</code></pre> <p>And updating <code>ifEast</code> action in the <code>TurnRight</code> method</p> <pre><code>public void TurnRight()\n{\n  Action ifNorth = () =&gt; Orientation = Direction.East;\n  Action ifSouth = () =&gt; {};\n  Action ifEast = () =&gt; Orientation = Direction.South;\n  Action ifWest = () =&gt; {};\n\n  Execute(ifNorth, ifSouth, ifEast, ifWest);\n}\n</code></pre> <p>Now that we\u2019ve proven how easy it is to make this change, let\u2019s go ahead and knock out the final two test cases</p> <pre><code>[Test]\n[TestCase(Direction.North, Direction.East, TestName = \"AndFacingNorthThenTheRoverFacesEast\")]\n[TestCase(Direction.East, Direction.South, TestName = \"AndFacingEastThenTheRoverFacesSouth\")]\n[TestCase(Direction.South, Direction.West, TestName = \"AndFacingSouthThenTheRoverFacesWest\")]\n[TestCase(Direction.West, Direction.North, TestName = \"AndFacingWestThenTheRoverFacesNorth\")]\npublic void RoverTurningRight(Direction start, Direction expected)\n{\n  var rover = new Rover { Orientation = start };\n  var initialLocation = rover.Location;\n\n  rover.TurnRight();\n\n  Assert.AreEqual(expected, rover.Orientation);\n  Assert.AreEqual(initialLocation, rover.Location);\n}\n</code></pre> <p>And implement the code to make those cases pass</p> <pre><code>public void TurnRight()\n{\n  Action ifNorth = () =&gt; Orientation = Direction.East;\n  Action ifSouth = () =&gt; Orientation = Direction.West;\n  Action ifEast = () =&gt; Orientation = Direction.South;\n  Action ifWest = () =&gt; Orientation = Direction.North;\n\n  Execute(ifNorth, ifSouth, ifEast, ifWest);\n}\n</code></pre> <p>Goodness, that was a pretty quick implementation of <code>TurnRight</code>! Thanks to our previous refactoring in <code>Rover</code> and leveraging parameterized testing, we were able to implement this new feature with a minimal amount of code and the code we did write was focused on the new functionality, not boilerplate.</p> <p>With that being said, here\u2019s our final version of <code>Rover</code>!</p> <pre><code>public class Rover\n{\n  public Direction Orientation { get; set; }\n  public Coordinate Location { get; set; }\n\n  public Rover()\n  {\n    Orientation = Direction.North;\n    Location = new Coordinate();\n  }\n\n  public void MoveForward()\n  {\n    Action ifNorth = () =&gt; Location=Location.AdjustYBy(1);\n    Action ifSouth = () =&gt; Location=Location.AdjustYBy(-1);\n    Action ifEast = () =&gt; Location=Location.AdjustXBy(1);\n    Action ifWest = () =&gt; Location=Location.AdjustXBy(-1);\n\n    Execute(ifNorth, ifSouth, ifEast, ifWest);\n  }\n\n  public void MoveBackward()\n  {\n    Action ifNorth = () =&gt; Location=Location.AdjustYBy(-1);\n    Action ifSouth = () =&gt; Location=Location.AdjustYBy(1);\n    Action ifEast = () =&gt; Location=Location.AdjustXBy(-1);\n    Action ifWest = () =&gt; Location=Location.AdjustXBy(1);\n\n    Execute(ifNorth, ifSouth, ifEast, ifWest);\n  }\n\n  public void TurnLeft()\n  {\n    Action ifNorth = () =&gt; Orientation = Direction.West;\n    Action ifWest = () =&gt; Orientation = Direction.South;\n    Action ifSouth = () =&gt; Orientation = Direction.East;\n    Action ifEast = () =&gt; Orientation = Direction.North;\n\n    Execute(ifNorth, ifSouth, ifEast, ifWest);\n  }\n\n  public void TurnRight()\n  {\n    Action ifNorth = () =&gt; Orientation = Direction.East;\n    Action ifEast = () =&gt; Orientation = Direction.South;\n    Action ifSouth = () =&gt; Orientation = Direction.West;\n    Action ifWest = () =&gt; Orientation = Direction.North;\n\n    Execute(ifNorth, ifSouth, ifEast, ifWest);\n  }\n\n  private void Execute(Action ifNorth, Action ifSouth, Action ifEast, Action ifWest)\n  {\n    switch(Orientation)\n    {\n      case Direction.North: ifNorth(); break;\n      case Direction.South: ifSouth(); break;\n      case Direction.East: ifEast(); break;\n      case Direction.West: ifWest(); break;\n    }\n  }\n}\n</code></pre>"},{"location":"articles/2020/07/07/mars-rover--implementing-rover--turn-right/#wrapping-up","title":"Wrapping Up","text":"<p>With the completion of <code>TurnRight</code>, the Rover has finally been implemented with all required pieces of functionality. Through this process, we learned how to write good unit tests, how to refactor unit tests to use parameterized unit testing, and reducing repetition through a SOLID refactor of <code>if/else</code> all of which culminating in <code>TurnRight</code> being a simple piece of functionality to include. In the next post, we will start working on implementing our logger!</p>"},{"location":"articles/2020/05/26/mars-rover---intro-to-testing/","title":"Mars Rover - Intro To Testing","text":""},{"location":"articles/2020/05/26/mars-rover---intro-to-testing/#what-this-is-and-what-this-isnt","title":"What This Is and What This Isn\u2019t","text":"<p>The goal of this post isn\u2019t to try to convince you that you should be unit testing your code, but to give you enough information that if you need to unit test your code or if you\u2019re in a codebase that expects tests, you will have the knowledge to hold your own. If you are looking for reasons why you should be testing, there are some great resources in the Additional Resources section at the end of the post!</p>"},{"location":"articles/2020/05/26/mars-rover---intro-to-testing/#unit-testing-101","title":"Unit Testing 101","text":"<p>At its core, unit testing is the practice of writing code that tests that your code is working correctly. Confused? It\u2019ll make sense in a minute, I promise! If you\u2019ve ever made changes to a project, how confident were you that your changes worked? If I had to guess, there\u2019s a high level of confidence that your changes solved the problem. However, how confident were you that your changes didn\u2019t break some other piece of functionality? I imagine that your confidence is a bit lower. For me, I\u2019m reasonably confident that my changes are solid, but I\u2019m not always sure that I didn\u2019t break some other features elsewhere. The most straightforward approach to verify everything is working correctly is to run the application and try it out, right?</p> <p>For small applications, that\u2019s a pretty reasonable approach to take and wouldn\u2019t take too much time. However, what if you\u2019re working on a more complicated application? How much time is it taking for you to compile the application, launch it, and start navigating through the UI? The premise of unit testing is that we can exercise the same logic that\u2019s running in the application but without having to go through the interface. This type of testing is generally faster and less error-prone since your test code will do the same tests over and over again. The downside is that you\u2019ll spend more time writing code, but for me, I feel much more confident in my work when I can run the full test suite and know pretty quickly if I\u2019ve broken something.</p>"},{"location":"articles/2020/05/26/mars-rover---intro-to-testing/#naming-again","title":"Naming Again?!","text":"<p>If you\u2019ve been following along with the Mars Rover kata series then you know I\u2019m a huge fan of using the same language as my Subject Matter Experts (SMEs) when it comes to the problem at hand as it prevents confusion when different terminology is used for the same concept.</p> <p>When it comes to naming my tests, I take this approach one step further and name my tests in such a manner that if you read the class name followed by the method, then you\u2019ll have a clear idea of what the test\u2019s intent is. The main reason I name my tests this way is that if my test fails, I want to know what requirement or workflow is not working so I can see if it makes sense for that workflow to be impacted.</p> <p>Sometimes, a test will start to fail because requirements have changed, so the test needs to be updated or removed. In other cases, the test failure reveals a contradiction in rules so it helps me if I can clearly see the use case and ask the right questions to my SMEs.</p>"},{"location":"articles/2020/05/26/mars-rover---intro-to-testing/#framing-context-with-givenwhenthen","title":"Framing Context with Given/When/Then","text":"<p>When it comes to naming my test classes and method, I borrow inspiration from the Given/When/Then naming methodology. I came across this convention when learning about Behavior Driven Development (BDD) early on in my career but I was familiar with the naming convent from working with user stories. The intent of Given/When/Then is that it provides the context, an action, and a result. Going to Mars Rover, an example user story would be</p> <p>Given that the rover is at (0, 0) facing North, when it receives the move forward command, then it should be at (0, 1) facing North</p> <p>Let\u2019s break this user story down and examine the various parts:</p>"},{"location":"articles/2020/05/26/mars-rover---intro-to-testing/#given","title":"Given","text":"<p>The Given portion of the user story sets up the context of the application. It can be as simple as Given that the user is logged into the system or as complex as Given that today is a holiday and that a user wants to make a reservation. The goal here is that someone can read this portion of the story and understand what is being accomplished. For this user story, the context is that we have a Rover that\u2019s located at (0, 0) facing North.</p>"},{"location":"articles/2020/05/26/mars-rover---intro-to-testing/#when","title":"When","text":"<p>The When portion of the user story gives what action is being taken in the application. At this level, we\u2019d typically stay away from technical jargon (i.e. the user clicks the reservation button) and focus more on the workflow being accomplished. For this user story, the action is that the Rover received the Move Forward command.</p>"},{"location":"articles/2020/05/26/mars-rover---intro-to-testing/#then","title":"Then","text":"<p>The Then portion of the user story tells us what the expected result or behavior is. It can be as simple as then the total is $42.55 or as complex as then the sale is declined and the user is informed that there was an issue processing the payment. For this user story, we\u2019re expecting that after the Rover receives the Move Forward command, then the rover is at (0, 1) facing North.</p>"},{"location":"articles/2020/05/26/mars-rover---intro-to-testing/#test-structure-and-naming-conventions","title":"Test Structure and Naming Conventions","text":"<p>Now that we\u2019ve learned a bit more about Given/When/Then, let\u2019s talk about how this can influence your test structure. When I\u2019m writing tests, I\u2019ll typically place them in a unit test project that\u2019s separate from production code so that we don\u2019t deploy the unit tests with the production code.</p> <p>In the case of Mars Rover, if I have a project called MarsRover, then I\u2019d have a project called MarsRover.UnitTests. Once I have this new project created, I\u2019ll create a folder called xyzTests where xyz is the name of the class I\u2019m writing tests against. So if I\u2019m writing tests against the Rover class in the MarsRover project, then I would have a folder called RoverTests in the MarsRover.UnitTests project.</p> <p> </p> Project naming conventions outlined in red. The test class and folder naming conventions outlined in blue <p>From here, I\u2019ll generally create a file per method or workflow that I want to test for the class. So based on our user story above, I would have a file called WhenMovingForward and this file will contain all the different tests for this functionality. Once a file is in place, we can start writing the various test methods for different behaviors. When naming methods, I will include the context for the setup and what the expectations were. By combining the test name and the method name, it will sound like a user story.</p> <p> </p> VS Code Test Runner showing test classes with test names"},{"location":"articles/2020/05/26/mars-rover---intro-to-testing/#organizing-your-test-with-arrangeactassert","title":"Organizing Your Test With Arrange/Act/Assert","text":"<p>At this point, we have the infrastructure in place for our tests, so how do we write a good test? Every good test will have three parts, Arrange, Act, and Assert. In the Arrange step, we\u2019re going to focus on creating dependencies and getting the initial state setup for the test. For simple tests, this step may only consist of creating the class that we\u2019re testing. For more complicated tests, there may be more dependencies to create, some methods to call, and possibly modifying the local environment. The Act step is where the method or property that we\u2019re testing is called. This step should be the simplest portion of the test since it should only be a line or two of code. The third and final step is the Assert step where we check that the result we observed was correct. For simple tests, this could be a single line where check a value whereas more complicated tests may need to check various properties.</p> <p>Using WhenMovingForward as an example, here\u2019s what an example test might look like.</p> <pre><code>using NUnit;\nusing System;\n\nnamespace MarsRover.UnitTests.RoverTests\n{\n  [TestFixture]\n  public class WhenMovingForward()\n  {\n    [Test]\n    public void AndFacingNorthThenYIncreasesByOne()\n    {\n      // Arrange\n      var rover = new Rover { Orientation = Direction.North };\n      var initialLocation = rover.Location;\n\n      // Act\n      rover.MoveForward();\n\n      // Assert\n      var expectedLocation = new Coordinate {X=initialLocation.X, Y=initialLocation.Y+1};\n      Assert.AreEqual(expectedLocation, initialLocation);\n    }\n  }\n}\n</code></pre> <p>I like to think of Arrange/Act/Assert (AAA) as a science experiment because we have to first find a hypothesis, design some test to prove or disprove the hypothesis, get all the necessary ingredients together for the test, run the test, and see if we have evidence to support our hypothesis.</p>"},{"location":"articles/2020/05/26/mars-rover---intro-to-testing/#wrapping-up","title":"Wrapping Up","text":"<p>In this post, we took a brief break from the Mars Rover kata to get a quick understanding of unit testing and the naming conventions we\u2019ll be leveraging for the rest of the series! We first talked about the importance of naming and how if we follow Given, When, Then syntax, we can turn our user stories into readable test names for future engineers. From there, I showed you Arrange, Act, Assert notation for tests, and an example test using the convention. Next time, we\u2019ll start implementing Rover!</p>"},{"location":"articles/2020/05/26/mars-rover---intro-to-testing/#additional-resources","title":"Additional Resources","text":"<ul> <li>Unit Testing Makes Me Faster (presentation) by Jeremy Clark</li> <li>Given, When, Then by Martin Fowler</li> <li>Starting to Unit Test: Not as Hard as You Think by Erik Dietrich</li> <li>The Art of Unit Testing: with examples in C# 2nd Edition by Roy Osherove</li> <li>_Everything I Needed To Know About Debugging I Learned In Elementary Physics (presentation) by Nate Taylor</li> </ul>"},{"location":"articles/2025/12/01/reducing-bugs-by-using-the-model-view-update-pattern/","title":"Reducing Bugs by Using the Model View Update Pattern","text":"<p>Note: This article is part of the 2025 C# Advent Calendar, so after you're done reading this, keep an eye out for the other cool articles coming out this month!</p> <p>For those who've followed me for a bit, you know that I'm a big believer that functional programming is a great way of approaching problems.</p> <p>One thing I've mentioned in my recent presentations is that we can take introductory concepts (pure functions, immutability, composition) and build real world software with them.</p> <p>In this post, I'm going to show you a pattern that I've recently ported to C#, the Model View Update, also known as The Elm Architecture.</p>"},{"location":"articles/2025/12/01/reducing-bugs-by-using-the-model-view-update-pattern/#inspiration-of-the-pattern","title":"Inspiration of the Pattern","text":"<p>Back in 2017, I came across a new functional language called Elm that took a different approach for web development. At a high level, Elm argues that you can think of an application as four pieces.</p> <ol> <li>The Model - What data are we presenting to the user?</li> <li>The View - How should we format the model?</li> </ol> <p>At this point, this seems very similar to other MV* patterns (Model View Controller, Model View Presenter, or Model View ViewModel). </p> <p>The next two parts is what sets this pattern apart from the others.</p> <ol> <li>The Command - What things can the user do on the screen (button clicks, entering text, etc...)</li> <li>The Update function - Given a Model and a Command, what does the new model look like?</li> </ol> <p>To me, this is an interesting concept because when the user makes changes, the model is being directly manipulated (generally through two-way binding) and then you had to make sure that you didn't put your business rules directly in the UI code. (For those who come from WinForms, how many times did you find business code in the code-behind?).</p> <p>With this approach, however, we've narrowed down what the UI can do (it can render a model and return a command, but can't directly manipulate the model). </p> <p>If you think that this approach isn't solid, you might be surprised to know that Elm inspired the creation of the following libraries in the JavaScript ecosystem:</p> <ul> <li>ngrx (Angular state management system)</li> <li>redux (React state management system)</li> </ul> <p>I've recently been using this pattern for console applications and have been pleasantly surprised how well it's working out.</p> <p>In this post, I'll walk you through how we can use this pattern to build out the \"Hello World\" equivalent application, manipulating a counter.</p>"},{"location":"articles/2025/12/01/reducing-bugs-by-using-the-model-view-update-pattern/#implementing-the-pattern","title":"Implementing the Pattern","text":""},{"location":"articles/2025/12/01/reducing-bugs-by-using-the-model-view-update-pattern/#defining-the-command","title":"Defining the Command","text":"<p>Before we can model the <code>Command</code>, we need to think about what commands we want to support. In our example, let's say that we want the user to be able to do the following:</p> <ul> <li>Increment the counter by 2</li> <li>Decrement the counter by 1</li> <li>Reset the counter to 0</li> <li>Quit the application</li> </ul> <p>In the Elm version (and it's equivalent TypeScript definition), a <code>Command</code> looks something like this:</p> <pre><code>type Command = {tag:'increment', value:2} | {tag:'decrement', value:1} | {tag:'reset'} | {tag:'quit'};\n</code></pre> <p>This takes advantage of algebraic data type  known as a sum type, where the <code>Command</code> type has one of three different constructors (one called <code>increment</code>, another called <code>decrement</code>, one called <code>reset</code>, and finally, <code>quit</code>). </p> <p>Even though C# doesn't have sum types (at least not yet), we can mimic this behavior using an abstract class.</p> <pre><code>// Abstract command that uses a string to\n// denote which command it is\n// (useful for casting later)\npublic abstract class Command&lt;T&gt; where T:Enum\n{\n  public abstract T Tag {get; }\n}\n</code></pre>"},{"location":"articles/2025/12/01/reducing-bugs-by-using-the-model-view-update-pattern/#defining-commands-for-counter","title":"Defining Commands for Counter","text":"<p>With the <code>Command</code> class defined, let's start implementing the various commands our program can have.</p> <p>First, we'll define an <code>enum</code> to keep track of the types of <code>Commands</code>. We could omit this and just use strings, but the value of the <code>enum</code> is that we can have C# generate the cases (though we still have to have a default case given the nature of enums).</p> <pre><code>// Enum to help with exhaustive matching later\n// on\n\npublic enum CommandType\n{\n  Increment,\n  Decrement,\n  Reset,\n  Quit,\n  Unknown\n}\n</code></pre> <p>With the enum defined, we can start define the commands, some of which will have more information included (see the IncrementCommand and DecrementCommand).</p> <pre><code>public class IncrementCommand : Command&lt;CommandType&gt;\n{\n  public override CommandType Tag =&gt; CommandType.Increment;\n\n  // Since some of the commands will have custom values and others not, \n  // we can inject those values through the constructor\n  public int Value {get; init;}\n\n  public IncrementCommand(int value)\n  {\n    Value = value;\n  }\n}\n\npublic class DecrementCommand : Command&lt;CommandType&gt;\n{\n  public override CommandType Tag =&gt; CommandType.Decrement;\n  public int Value {get; init;}\n\n  public DecrementCommand(int value)\n  {\n    Value = value;\n  }\n}\n\n// In other cases, we don't need\n// any other info, so we can inherit and implement the Tag property\npublic class ResetCommand : Command&lt;CommandType&gt;\n{\n  public override CommandType Tag =&gt; CommandType.Reset;\n}\n\npublic class QuitCommand : Command&lt;CommandType&gt;\n{\n  public override CommandType Tag =&gt; CommandType.Quit;\n}\n\npublic class UnknownCommand : Command&lt;CommandType&gt;\n{\n  public override CommandType Tag =&gt; CommandType.Unknown;\n}\n</code></pre>"},{"location":"articles/2025/12/01/reducing-bugs-by-using-the-model-view-update-pattern/#implementing-the-update-function","title":"Implementing the Update Function","text":"<p>Now that we have our various commands created, we can start building out the <code>Update</code> function. </p> <p>From our earlier description, we know that our <code>Update</code> function has to take in our model (a number) and a <code>Command</code> and then has to return a new model (a number).</p> <pre><code>// This example is a static method, but let's say that the rules were more complicated, \n// we could inject those into a class and make this method non-static.\n\npublic static int Update(int number, Command&lt;CommandType&gt; c)\n{\n  // I'm leveraging the new switch syntax, \n  // but you can use the original syntax\n  // without issues\n\n  return c.tag switch =&gt; {\n    CommandType.Increment =&gt; number + ((IncrementCommand)c).Value,\n    CommandType.Decrement =&gt; number - ((DecrementCommand)c).Value,\n    CommandType.Reset =&gt; 0,\n    // In the case we're told to quit or we\n    // get an unknown command, we'll return\n    // the model back\n    CommandType.Quit =&gt; number,\n    CommandType.Unknown =&gt; number,\n    // Since C# doesn't have exhaustive matching, we still require the default case here\n    _ =&gt; number\n  };\n}\n</code></pre>"},{"location":"articles/2025/12/01/reducing-bugs-by-using-the-model-view-update-pattern/#implementing-the-view-function","title":"Implementing the View Function","text":"<p>At this point, we could go ahead and start writing tests to verify that our model is being updated given the command, but our users are going to be interacting with the application, so let's build that out next.</p> <p>From before, we know that the <code>View</code> function takes in the model (a number) and it will return a <code>Command</code>. Given that we need to interact with the user, this is an impure function by design, so we shouldn't put our business rules in here.</p> <pre><code>public static Command&lt;CommandType&gt; View(int model) {\n  Console.WriteLine($\"Counter: {model}\");\n  Console.WriteLine(\"(I)ncrement, (D)ecrement, (R)eset, or (Q)uit\");\n\n  return ConvertStringToCommand(Console.ReadLine());\n}\n\n// Even though this is called by the View\n// function, this is a Pure function\n// because it only depends upon a string\n// for its logic\nprivate static Command&lt;CommandType&gt; ConvertStringToCommand(string s) {\n  return (s ?? \"\").Trim().ToLower() switch {\n    \"i\" =&gt; new IncrementCommand(2), // will increment by 2\n    \"d\" =&gt; new DecrementCommand(1), // will decrement by 1\n    \"r\" =&gt; new ResetCommand(),\n    \"q\" =&gt; new QuitCommand(),\n    _ =&gt; new UnknownCommand()\n  };\n}\n</code></pre>"},{"location":"articles/2025/12/01/reducing-bugs-by-using-the-model-view-update-pattern/#wiring-everything-together","title":"Wiring Everything Together","text":"<p>Now that we have our Model (a number), the <code>View</code> function, an <code>Update</code> function, and our list of <code>Command</code>s, we can wire everything together.</p> <pre><code>public static class Framework\n{\n  public static void Run&lt;TModel, TCommandType&gt;(\n      Func&lt;TModel, Command&lt;TCommandType&gt;&gt; view,\n      Func&lt;TModel, Command&lt;TCommandType&gt;, TModel&gt; update,\n      TModel model)\n  where TCommandType : Enum\n  {\n    // We need the Enum to have a Quit option\n    // defined, otherwise, we won't know when\n    // to quit the application.\n    if (!Enum.IsDefined(typeof(TCommandType), \"Quit\"))\n    {\n      throw new InvalidOperationException(\"Command must have a Quit option\");\n    }\n    var quitCommand = Enum.Parse(typeof(TCommandType), \"Quit\");\n\n    // Getting our initial state\n    var currentModel = model;\n    Command&lt;TCommandType&gt; command;\n\n    // While command isn't Quit\n    do\n    {\n      // Clear the screen\n      Console.Clear();\n      // Get the command from when we render the view\n      command = view(currentModel);\n      // Get the new model from update\n      currentModel = update(currentModel, command);\n    } while (!command.Tag.Equals(quitCommand));\n  }\n}\n</code></pre>"},{"location":"articles/2025/12/01/reducing-bugs-by-using-the-model-view-update-pattern/#final-version","title":"Final Version","text":"<p>With the <code>Framework.Run</code> function defined, we can invoke it via our <code>Program.cs</code> file.</p> <p>You can find the working version below (or you can clone a copy from GitHub)</p> Program.cs<pre><code>internal class Program\n{\n  private static void Main(string[] args)\n  {\n    var startCounter = 0;\n    Framework.Run(View, CounterRules.Update, startCounter);\n  }\n\n  public static Command&lt;CommandType&gt; View(int model)\n  {\n    Console.WriteLine(\"Counter: \" + model);\n    Console.WriteLine(\"Please enter a command:\");\n    Console.WriteLine(\"(I)ncrement, (D)ecrement, (R)eset, (Q)uit\");\n    var input = Console.ReadLine() ?? \"\";\n    return ConvertStringToCommand(input);\n  }\n\n  private static Command&lt;CommandType&gt; ConvertStringToCommand(string s) =&gt; (s ?? \"\").ToLower().Trim() switch\n  {\n    \"i\" =&gt; new IncrementCommand(2),\n    \"d\" =&gt; new DecrementCommand(1),\n    \"r\" =&gt; new ResetCommand(),\n    \"q\" =&gt; new QuitCommand(),\n    _ =&gt; new UnknownCommand(),\n  };\n}\n</code></pre> CounterRules.cs<pre><code>public static class CounterRules\n{\n  public static int Update(int model, Command&lt;CommandType&gt; c)\n  {\n    return c.Tag switch\n    {\n      CommandType.Increment =&gt; model + (c as IncrementCommand)!.Value,\n      CommandType.Decrement =&gt; model - (c as DecrementCommand)!.Value,\n      CommandType.Reset =&gt; 0,\n      CommandType.Quit =&gt; model,\n      CommandType.Unknown =&gt; model,\n      _ =&gt; model\n    };\n  }\n}\n</code></pre> Commands.cs<pre><code>public enum CommandType\n{\n  Increment,\n  Decrement,\n  Reset,\n  Quit,\n  Unknown\n}\n\npublic class IncrementCommand : Command&lt;CommandType&gt;\n{\n  public override CommandType Tag =&gt; CommandType.Increment;\n  public int Value { get; init; }\n  public IncrementCommand(int value)\n  {\n    Value = value;\n  }\n}\n\npublic class DecrementCommand : Command&lt;CommandType&gt;\n{\n  public override CommandType Tag =&gt; CommandType.Decrement;\n  public int Value { get; init; }\n  public DecrementCommand(int value)\n  {\n    Value = value;\n  }\n}\n\npublic class ResetCommand : Command&lt;CommandType&gt;\n{\n  public override CommandType Tag =&gt; CommandType.Reset;\n}\n\npublic sealed class QuitCommand : Command&lt;CommandType&gt;\n{\n  public override CommandType Tag =&gt; CommandType.Quit;\n}\n\npublic sealed class UnknownCommand : Command&lt;CommandType&gt;\n{\n  public override CommandType Tag =&gt; CommandType.Unknown;\n}\n</code></pre> Framework.cs<pre><code>public abstract class Command&lt;T&gt; where T : Enum\n{\n  public abstract T Tag { get; }\n}\n\npublic static class Framework\n{\n  public static void Run&lt;TModel, TCommandType&gt;(\n      Func&lt;TModel, Command&lt;TCommandType&gt;&gt; view,\n      Func&lt;TModel, Command&lt;TCommandType&gt;, TModel&gt; update,\n      TModel model)\n  where TCommandType : Enum\n  {\n    if (!Enum.IsDefined(typeof(TCommandType), \"Quit\"))\n    {\n      throw new InvalidOperationException(\"Command must have a Quit option\");\n    }\n    var quitCommand = Enum.Parse(typeof(TCommandType), \"Quit\");\n    var currentModel = model;\n    Console.Clear();\n    var command = view(currentModel);\n    do\n    {\n      currentModel = update(currentModel, command);\n      Console.Clear();\n      command = view(currentModel);\n    } while (!command.Tag.Equals(quitCommand));\n  }\n}\n</code></pre>"},{"location":"articles/2025/12/01/reducing-bugs-by-using-the-model-view-update-pattern/#conclusion","title":"Conclusion","text":"<p>In this post, we built out a basic application using the Model View Update pattern that was first introduced by the Elm language. We also implemented a basic sum type, <code>Command</code>, using an abstract class that was then constrained to particular <code>CommandTypes</code>.</p>"},{"location":"articles/2024/03/11/five-tips-to-have-more-effective-meetings/","title":"Five Tips to Have More Effective Meetings","text":"<p>As a leader it's inevitable that you will have to organize a meeting. Whether it's for updates, 1-1s, or making decisions, the team is looking towards you to lead the conversation and have it be a good use of time.</p> <p>But how do you have a good meeting? That's not something that's covered in leadership training. Is it the perfect invite? A well honed pitch? Throw something out there and see if it sticks?</p> <p>Like anything else, a good meeting needs some preparation, however, if you follow these five tips, I guarantee your meetings will be better than before.</p>"},{"location":"articles/2024/03/11/five-tips-to-have-more-effective-meetings/#step-1-does-it-even-need-to-be-a-meeting","title":"Step 1: Does It Even Need to Be a Meeting?","text":"<p>The best kind of meeting is the one that didn't have to happen. Have you ever sat through a meeting where everyone did a bunch of talking, you halfway listened and thought to yourself, \"this could have been an email?\"</p>  Photo by BRUNO CERVERA on Unsplash <p>Been there, done that have, and have the t-shirt.</p> <p>When I think about why we need meetings, it's because we're trying to accomplish something that one person alone couldn't get done. With this assumption in mind, I find that meetings take one of two shapes: sharing information (e.g., stand-ups, retrospectives, all-hands) or to make a decision (e.g., technical approach, ironing the business rules).</p> <p>Depending on what you're trying to accomplish, then next thought is determine if the communication needs to be synchronous (get everyone together) or asynchronous (let people get involved at their own pace).</p> <p>For example, if the team has been struggling in getting work done, then it makes sense to have a meeting to figure out what's happening and ensure that everyone is hearing the exact wording/tone of the messaging.</p> <p>On the other hand, if your intent is to let the team know that Friday is a holiday, then that can be done through email or message in your chat tool.</p> <p>One way you can figure out if the meeting could have been an email is to pretend it was a meeting and you canceled it. Is there anything that can't proceed? If not, then maybe you don't need that meeting.</p>"},{"location":"articles/2024/03/11/five-tips-to-have-more-effective-meetings/#step-2-how-do-we-even-know-if-were-successful","title":"Step 2: How Do We Even Know If We're Successful?","text":"<p>Have you ever attended a meeting and didn't know what it was about or why you met? These types of meetings typically suffer from not having a goal or purpose behind the meeting.</p> <p>Recall from Step #1, we're meeting because there's something that we need from the group that we couldn't do as individuals. So what is it?</p> <p>When scheduling the meeting, include the purpose (here's why we're meeting) and the goal (here's how we know if we're done) to the description. Not only is this a great way to focus the meeting, it can also serve as a way for people to know if they need to attend or not.</p>  Photo by Afif Ramdhasuma on Unsplash <p>This is also a good litmus test to see if you know why there should be a meeting as this forces you to think about the problem being solved and how it should happen. If you're struggling to determine the purpose and the goal, then you're attendees will also struggle.</p>"},{"location":"articles/2024/03/11/five-tips-to-have-more-effective-meetings/#step-3-do-you-have-the-right-people","title":"Step 3: Do You Have The Right People?","text":"<p>A common mistake I see people make is that they invite everyone who has a stake or passing interest in the topic which can make for a large (10+ people) meeting.</p> <p>Even though the intent is good (give everyone visibility), this is a waste because the more people you have in a meeting, the less effective it will be. A meeting with four people will have a better conversation and get things done more than a meeting with nine people.</p> <p>Let's pretend that you're at a large party and you see a group that you know, so you walk up to the group, hoping to break into the conversation.</p> <p>As more people join in the group, they're going to naturally split up into smaller groups, each with their own conversations. The main reason is that the large the group, the less likely you have a chance to participate and get involved. So you might start a conversation with 1 or 2, split off and then start a new group.</p> <p>Meetings have the same problem. The large the group, the more likely that side conversations will happen and it makes it harder for you to facilitate and keep everyone on track.</p> <p>To keep meetings effective, be sure to only include the necessary people. For example, instead of inviting an entire team, invite only 1 or 2 people.</p> <p>At a high level, you need the these three roles filled to have a successful meeting</p> <ol> <li>The Shot Caller - This is the main stakeholder and can approve our decisions. Without their buy-in, no real decision can be made.</li> <li>The Brain Trust - These are the people who have the details and can drive the conversation. You want to keep this group as tightly focused as possible.</li> <li>The Facilitator - Generally the organizer, this is the person who ensures that the goal is achieved and keeps the meeting running.</li> </ol> <p>One way to narrow down the invite list is to answer this question:</p>  If this person can't make the meeting, then we can't meet.  <p>If you can't accomplish the goal without them, then they need to be there. I'm such a believer in this advice that if it's the day of the meeting and we don't have the Shot Caller or the Brain Trust, then I'll reschedule the meting as I'd rather move it than waste everyone's time.</p>  Photo by Jason Goodman on Unsplash"},{"location":"articles/2024/03/11/five-tips-to-have-more-effective-meetings/#step-4-running-the-meeting","title":"Step 4: Running the Meeting","text":"<p>It's the big day and you've got everyone in the room, now what?</p> <p>In Step #2, we talked about having a purpose and goal for the meeting. Now is when we vocalize these two things to kick the meeting off. From there, we can seed the conversation with one of these strategies:</p> <ul> <li>Asking an opening question to prime the Brain Trust.</li> <li>Throwing to the Shot Caller to frame any restrictions the attendees need to be aware of.</li> <li>Start with a specific person to kick the conversation off.</li> </ul> <p>Once the conversation starts flowing, your job is to keep the meeting on track. For those who've played games like Dungeons and Dragons, you're acting like a Game Master where you know the direction the meeting needs to go to (The Goal), but the attendees are responsible for getting there.</p> <p>It can be challenging to keep the meeting on track if you're also driving the conversation, so pace yourself, take notes, and get others involved to keep the conversation going.</p> <p>When leading longer meetings (more than 60 minutes), make sure to take a 10 minute break.</p> <p>For attendees, this allows them to stretch their legs, take a bathroom break, and to stew on the conversation that's happened so far. For those who are \"thinkers\" than \"reacters\", this gives them time to compose their thoughts and have better conversations after the break.</p> <p>As a facilitator, this gives you a way to think about the meeting so far, identify areas that the group needs to dig into, and if needed, it can break the conversation out of a rut.</p>"},{"location":"articles/2024/03/11/five-tips-to-have-more-effective-meetings/#step-5-wrapping-up-how-do-things-get-done","title":"Step 5: Wrapping Up - How Do Things Get Done?","text":"<p>As the meeting comes to a close, we need to make sure that action follows next. A meeting with no follow-up is a lot like a rocking chair. Plenty of motion, but no progress being made.</p> <p>In order to make sure that next steps happen, make sure to define action items with attendees owning getting them done. Action items don't have to be complex, it could be as simple as:</p> <ul> <li>Defining stories for the team</li> <li>Sending summary notes to other stakeholder</li> <li>Following up with Person about X.</li> </ul> <p>When defining action items, be wary of items that are scheduling another meeting (e.g. let's schedule a meeting with Team Y to get their perspective). This implies that you didn't have the right people in the room (see Step 3). Also, remember, meetings are to get alignment or to come up with a solution, so what purpose does this follow up meeting have?</p> <p>As the meeting wraps up, take a few moments to summarize the outcome, verbally ensure that actions items have been assigned and thank everyone for their attention and time.</p>"},{"location":"articles/2024/03/11/five-tips-to-have-more-effective-meetings/#congratulations-youre-an-expert-with-meetings-now-right","title":"Congratulations, You're an Expert With Meetings Now, Right?","text":"<p>Running effective meetings can be made easier if you take the time to do the necessary preparation. Even those these steps may seem heavy on the documentation, you'll find that it'll help you focus on the core problem at hand, which helps focus the group, which makes everyone that much better.</p> <p>By following these five steps, you'll increase your chances of having a great meeting and as you gain more experience, you'll become more comfortable running them.</p>"},{"location":"articles/2024/07/18/my-experience-preparing-for-the-azure-administrator-associate-az-104-exam/","title":"My Experience Preparing for the Azure Administrator Associate (AZ-104) Exam","text":"<p>There's been a bit of a lull the past couple of weeks on the blog as I've been focusing my time on studying and preparing for the AZ-104 exam. This was a particularly challenging certificate for me as I don't have a traditional IT Admin background so I had to not only shore up the gaps in that knowledge, but then also had to learn how to model similar concepts in Azure.</p> <p>That being said, I was able to pass the exam on my first take and wanted to share some advice for those who are looking to take this or other Azure exams.</p>"},{"location":"articles/2024/07/18/my-experience-preparing-for-the-azure-administrator-associate-az-104-exam/#expand-your-studying-outside-of-the-microsoft-learn-documentation","title":"Expand your studying outside of the Microsoft Learn documentation","text":"<p>The Microsoft Learn docs are fine for doing a deep dive into a subject, but if it's the first time learning a concept, then they can be a bit rough as they assume you have knowledge that you might not. To help round out your learning, I recommend finding other resources like videos, books, or articles.</p>"},{"location":"articles/2024/07/18/my-experience-preparing-for-the-azure-administrator-associate-az-104-exam/#build-and-experiment-in-azure","title":"Build and Experiment in Azure","text":"<p>Given that most of these concepts are pretty abstract, I found that they stuck with me much more when I build out the resources. For example, when working with a Virtual Machine, all of its components need to be in the same region. You can either remember that text OR you know that has to be true because if you try building out the VM in Azure and try to change components, it's going to fail.</p>"},{"location":"articles/2024/07/18/my-experience-preparing-for-the-azure-administrator-associate-az-104-exam/#dont-rely-solely-on-practice-exams","title":"Don't Rely Solely on Practice Exams","text":"<p>Back in 2019, I was studying/preparing for the 483 (exam on C#) and the advice at the time was to go over the practice exams over and over again until things stuck. Following the same advice, I took tons of practice exams (through MS Learn and MeasureUp) and though they might have had the same format as the exam (multiple choice, drag-and-drop, etc...), none of them were a good stand-in for the real exam.</p> <p>The reason being is that the exam questions likely won't ask you to define a term, but are more likely to be along the lines of how you'd solve a problem (which expects you to know the terminology inherently). So if you don't have the underlying knowledge, you're going to have a bad time trying to answer the questions.</p> <p>Where the practice exams shone was helping me identify areas that I needed to focus more on. For example, if I struggled in the Networking section, then I know I needed to revisit concepts there. This helped me make the most of my studying time.</p>"},{"location":"articles/2024/07/18/my-experience-preparing-for-the-azure-administrator-associate-az-104-exam/#using-generative-ai-to-help-understand-concepts","title":"Using Generative AI To Help Understand Concepts","text":"<p>Given that I don't come from a networking/IT background, there were some concepts that were quite confusing to me. For example, I was trying to understand why I would need System routes if we already had Network Security Groups and Copilot was able to give me the following:</p> <p></p> <p>To help make sure I didn't fall victim to hallucinations, I followed up on the links that Copilot provided to make sure that I understood the concepts, but given that I learn best by asking questions, this was a major win for me since you can't ask questions to books/videos.</p>"},{"location":"articles/2024/07/18/my-experience-preparing-for-the-azure-administrator-associate-az-104-exam/#resources-that-helped-me","title":"Resources That Helped Me","text":"<p>For those looking to study up on this exam, I had success using these resources. Note: I do not receive compensation for these recommendations.</p> <ul> <li>(Paid) AZ-104 Microsoft Azure Administrator Exam Prep by Scott Duffy - I found this to be a great introduction to concepts and a good companion before going through the MS Learn or other resources</li> <li>(Free) AZ-104 Administrator Associate Study Cram v2 by John Savill - This resource is a good refresher and it helped me see concepts from a different perspective. </li> </ul>"},{"location":"articles/2018/07/18/my-interviewing-strategy/","title":"My Interviewing Strategy","text":"<p>I\u2019ve found that interviewing for a new job can be super stressful and it reminds me of speed dating, \u201cLet\u2019s get to know each other over the next few hours to see if this relationship can work\u201d. With such a short time window, there\u2019s not much time to ask \u201cfluff\u201d questions like \u201cif you were a tree, what kind of tree would you be\u201d. Instead, I\u2019m more likely to ask some of the following questions to get a better understanding of what I\u2019m about to step into. If the company can\u2019t answer some of these questions, it\u2019s not a deal breaker, but can be a red flag about this place.</p> <p>With that being said, I hope these questions help you during your interviewing process!</p>"},{"location":"articles/2018/07/18/my-interviewing-strategy/#business-strategy","title":"Business Strategy","text":"<ul> <li>What is your biggest concern for ? <li>What was a major success for  over the past year? <li>What was a stumbling block for  over the past year? <li>What does your ideal customer look like?</li> <li>What is your business model (i.e. how does  make its money)? <li>Any thoughts of expanding to other markets (such as, if the company sells a particular type of medical device, are there any thoughts of making other devices or add-ons for the main device)? If not, why?</li> <li>Who would you say are your biggest competitors? What differentiates you from them?</li>"},{"location":"articles/2018/07/18/my-interviewing-strategy/#software-development","title":"Software Development","text":"<ul> <li>What is the current architecture of the software?</li> <li>What is the direction that  is moving to? <li>What is your current tech stack?</li> <li>What development methodologies (TDD, Pairing, Mobbing, XP, etc\u2026) are you using?</li> <li>How do you maintain quality?</li> <li>What is one quality you appreciate it a teammate?</li> <li>What is one quality that gets on your nerves?  How much work is new (greenfield) vs maintenance (brownfield)?</li>"},{"location":"articles/2018/07/18/my-interviewing-strategy/#general","title":"General","text":"<ul> <li>What would you consider to be a big success over the past year?</li> <li>What would you consider a failure or stumbling block over the past year?</li> <li>What is the best thing about working at ? <li>What is one thing that be improved about working at ? <li>What determines \u201csuccess\u201d for this role? How would you measure / know it?</li> <li>What does a typical day look like for this role?</li>"},{"location":"articles/2018/07/18/my-interviewing-strategy/#benefits","title":"Benefits","text":"<ul> <li>How many vacation days? How many sick days? Are they from the same bucket (i.e PTO) or different buckets?</li> <li>Is there a 401(K)? If so, what\u2019s the vesting period (i.e. how long until the employer contributions become yours)? What\u2019s the employer contribution?</li> <li>Is there a training budget? If so, is it per person, per team, per department? What do you typical training expenses look like (books, videos, conferences, or in person training)?</li> <li>When does open enrollment begin for insurance? Am I covered on day one or is there a waiting period?</li> </ul>"},{"location":"articles/2023/04/16/telling-the-story-the-pitfall-of-a-single-data-point/","title":"Telling the Story: The Pitfall of a Single Data Point","text":"Let's say that you're sitting down to read a new book, and you come across the following:  <p> The King's Knave Inn was but a short distance from the Alverston train depot, just outside the town proper. (excerpt from The Infernal Machine by John Lutz) </p>  After reading this, a friend interrupts your reading and asks your thoughts on the book so far.   What would you say?  <p>Most likely, you'd respond that you need to read more, and it's still too early to decide if the book is good or not.</p> <p>To honestly answer this question, you would need to read more of the book (ideally all of it) to get a full picture of the story.</p> <p>When measuring an engineer's performance and effectiveness, why don't we take the same approach?</p> <p>My experience has been that leaders look for one or more metrics to quantify a person. At the face of it, I understand why, as it's hard to compare people if there aren't numbers. </p> <p>However, the mistake I see leaders make is what they're trying to measure. For example, do you measure  the number of pull requests? What about the number of stories completed in a sprint? How about the number of bugs shipped to production? Something else entirely?</p> <p>The problem is that even if you use all of the above (please don't do this), you're still not seeing the whole picture, but only bits and pieces. This would be like reading five chapters at random from a book and then giving an opinion.</p> <p>The other problem with using metrics is that the measurement will cease to be effective as people will start gaming the system (see Goodhart's Law).</p> <p>For example, if we measure effectiveness by the number of completed pull requests, then what stops someone from creating hundreds of single-line pull requests that don't accomplish anything?</p> <p>On the other side, what about the engineer who reduced scope and time because they knew how to simplify the approach or came up with a more straightforward solution? This insight won't show up as a pull request or a completed story; however, this should be rewarded just the same.</p> <p>To really determine how effective someone is, we need to look at things holistically, which can be done by examining how well someone does in these three areas:</p> <ul> <li>Understanding the problem (e.g., why are we doing this?)</li> <li>Understanding the system (e.g., how are we doing this?)</li> <li>Understanding the people (e.g., whom are we doing this with?)</li> </ul> <p>By looking into these areas, you will see what your team is good at and where they could use coaching, helping you be more effective. You might also realize that your team is doing things that aren't so obvious.</p> <p>You can't write a report to generate these metrics. To understand this, you have to understand your team and how they work together. This involves paying attention, taking notes, and being engaged. Passive leaders will struggle if they use this approach.</p>"},{"location":"articles/2023/04/16/telling-the-story-the-pitfall-of-a-single-data-point/#understanding-the-problem-the-why","title":"Understanding the Problem (The Why)","text":"<p>To be successful, we first have to understand the problem that's being solved. Without this base knowledge, it's impossible to build the right solution or even ask the right question to the problem at hand.</p> <p>How comfortable are they within the problem domain? Do they know certain terminologies, our customers, the users, workflows, and expected behaviors?</p> <p>Besides quizzing, there may not be an obvious way to measure this; however, here's my approach.</p> <p>First, you can look at the questions that are being asked. Are they surface level or are they deep? You can see these questions through chats and meetings, comments on the stories or pull requests, and interactions with others.</p> <p>Second, look at the solution they came up with. Did they design it with domain knowledge in mind? For example, are things named correctly? Did their solution take care of the main workflows? What about the edge cases?</p> <p>Third, how are they handling support issues? Being on support is a quick way of learning a problem domain and system. As such, I'm looking at how much help they need and how they communicate with others.</p> <p>By using this approach, you can get a good sense of how knowledgeable someone is in the problem domain without quizzing them.</p>"},{"location":"articles/2023/04/16/telling-the-story-the-pitfall-of-a-single-data-point/#understanding-the-system-the-how","title":"Understanding the System (The How)","text":"<p>There's always a push to deliver more things, and in order to do that, we have to understand the current system, its limitations, what's easy vs. what's hard, and from these constraints, determine the correct path to take.</p> <p>In addition, once the system is live, we need to support it. If we don't know the moving parts, what it interacts with, and how it's used, we're going to have a bad time.</p> <p>Like understanding the problem, we can measure system knowledge without quizzing them. In particular, I've found pull request comments and code reviews to be insightful on someone's knowledge of the system. </p> <p>For example, do they call out that there's already something in the system that does this new piece of functionality? Do they suggest taking a simpler approach with what we have? Do they propose a different solution altogether because the system has a limitation? All of these are indicators of someone's system knowledge.</p> <p>Another way to gauge system knowledge is by looking at how the person handles support requests. If you can understand the problem, find the cause, and create a fix, then by definition, you have to have a solid understanding of the system. </p>"},{"location":"articles/2023/04/16/telling-the-story-the-pitfall-of-a-single-data-point/#understanding-the-people-the-who","title":"Understanding the People (The Who)","text":"<p>When it comes to the third part of being effective, we have to measure how they work with those around them. Most people think engineering is a solitary line of work, and that can be true when it comes to the development phase.</p> <p>However, in reality, engineers work with others to design, develop, and iterate on a solution, and this can only happen when working with others. As such, building these relationships are paramount to being successful.</p> <p>If you want to go fast, go alone. If you want to go far, go together.</p> <p>Measuring team cohesion can be difficult (it could be its own post), however, we start simply be getting peer feedback on the person. We can also look at the communication between them and others through their comments, messages, or meetings.</p> <p>Another way to measure this is through your company's recognition system. Whether it's an email or some other tool your company uses, you need to keep tabs on these recognitions, as you can use them as a talking point during 1:1s and review time. </p>"},{"location":"articles/2023/04/16/telling-the-story-the-pitfall-of-a-single-data-point/#wrapping-up","title":"Wrapping Up","text":"<p>So, how do we measure how effective someone is? We know that a single data point isn't sufficient and that if we limit ourselves to metrics, we can get a skewed sense of the person. To know, we have to take a holistic approach.</p> <p>To accomplish this goal, I recommend measuring the following areas:</p> <ul> <li>Understanding the problem (e.g., why are we doing this?)</li> <li>Understanding the system (e.g., how are we doing this?)</li> <li>Understanding the people (e.g., whom are we doing this with?)</li> </ul> <p>In each of these areas, we can get a sense by observing their interactions they have, the questions they ask, the approaches they take, and how likely people want to work with the individual.</p>"},{"location":"articles/2023/07/18/scaling-effectiveness-with-docs/","title":"Scaling Effectiveness with Docs","text":"<p>As a leader, I'm always looking for ways to help my team to be more efficient. To me, an efficient team is self-sufficient, able to find the information needed to solve their problems.</p> <p>I've found that having up-to-date documentation is critical for a team because it scales out knowledge in asynchronously, removing the need for manual knowledge transfers.</p> <p>For example, my team has a wiki that contains information for onboarding into our space, how to complete certain processes (requesting time off, resetting a password), how to run our Agile activities, and our support guidebook. At any point, if someone on the team doesn't know how to do something, they can consult the wiki and find the necessary information.</p>"},{"location":"articles/2023/07/18/scaling-effectiveness-with-docs/#docs-why-did-it-have-to-be-docs","title":"Docs. Why Did It Have to Be Docs?","text":"<p>I enjoy up-to-date documentation, but the main problem with them is that they captured the state of the world when they were written, but they don't react to changes. If the process for resetting your password changes, the documentation doesn't auto-update. So unless you're spending time reviewing the docs, they'll grow stale and be worthless, or even worse, mislead others to do the wrong things.</p> <p>A good mental model for documentation is to think of them as a garden. When planted, it looks great, and everyone enjoys the environment. Over time, weeds will grow, and plants will become overgrown, causing the garden to be less attractive. Eventually, people will stop visiting, and the garden will go into disrepair. To prevent this, we must take care of the garden, removing the weeds and trimming the plants.</p> <p> </p> Photo by Robin Wersich via Unsplash.com <p>Alright, I get it, documentation is important, but my team has commitments, so how do we carve out time to review?</p>"},{"location":"articles/2023/07/18/scaling-effectiveness-with-docs/#cameron-learns-about-document-control","title":"Cameron Learns About Document Control","text":"<p>I started my career in healthcare, and one of my first jobs was writing software for a medical diagnostic device. We were ISO 9001 certified, and the device was considered a Class II from the FDA. Long story short, this meant that we have to provide documentation for our device and software and also show that we were keeping things up to date.</p> <p>To comply, we would find docs that hadn't been updated in a specific time period (like 90 days) and review them. If everything checked out, we'd bump up the review date. Otherwise, we'd make the necessary changes and revalidate the document.</p> <p>At the time, all of our files were in Word, so it wasn't the easiest to search them (I recall that we had Outlook reminders, but this was many moons ago).</p> <p>By baking this into our process, this helped make our work more visible, which in turn, gave us a better idea of the team's capacity for that sprint.</p> <p>Thankfully, we have better technology than Word for sharing information, so how can we take this approach and bring it up to the modern day?</p>"},{"location":"articles/2023/07/18/scaling-effectiveness-with-docs/#modern-take-on-an-old-classic","title":"Modern Take on an Old Classic","text":"<p>First, I think that having your docs in source control is a great idea. If you're using tools like Git, you already have a way of leaving comments and keeping track of approvals through pull requests.</p> <p>To make the most of Git, you should keep your changes in plaintext as it's easy to see the differences. and I enjoy using Markdown and tools like Mkdocs make this workflow possible.</p> <p>With this figured out, our next step is to know when the file was last reviewed. We can do that by adding a new line to the bottom of each file, Last Reviewed On: YYYY/MM/DD. To come up with the initial date, we could use the last time the file was modified (thanks <code>git log</code>!).</p> <p>At this point, we have a way to see the last time the file was reviewed, next step is to write a script that can find files that haven't been reviewed in the last 90 days. At a high level, we'd do the following:</p> <ol> <li>Get the latest for the doc repository.</li> <li>Get all the markdown files for the repository.</li> <li>Get the last line of the file.</li> <li>If the line doesn't start with Last Reviewed On:, we flag it for review as it's never been reviewed.</li> <li>If the line has a date, but it's older than 90 days, we flag it for review as it might be stale.</li> <li>Print all flagged files to the screen.</li> </ol> <p>With the script created, we could manually run this on Mondays. But we're technical, right? Why not create a scheduled task to execute this script instead? This removes a manual task to be ran and it gives us visibility on what docs need reviewed.</p>"},{"location":"articles/2023/07/18/scaling-effectiveness-with-docs/#wrapping-up","title":"Wrapping Up","text":"<p>When scaling your knowledge out, having great documentation is necessary as it allows your team to self-serve and work in a more asynchronous manner. The main problem with documentation is that it captures the state of the world when the docs were written, but they don't automatically update when the world changes.</p> <p>Therefore, we need to have some process to flag and review stale docs. To ensure it gets done, we provide visibility by creating work items and committing to them during the sprint.</p>"},{"location":"articles/2025/07/07/career-update-and-a-new-chapter/","title":"Career Update and a New Chapter","text":"<p>A couple of months back, I announced on LinkedIn that some opportunities fell through and I found myself unexpectedly on the market looking for work. It has been a while since I've given an update on what was going on and what I've been up to.</p>"},{"location":"articles/2025/07/07/career-update-and-a-new-chapter/#the-search-begins-anew","title":"The Search Begins (Anew!)","text":"<p>After watching both opportunities fall through, I began my job search again, leading to me applying to over fifty different companies, focusing on leadership roles (think Team Lead, Engineering Manager, or Director of Engineering). Even though I have quite a few years of recent experience (I've been leading teams in some capacity since 2018), the most common piece of feedback is that it seemed like my recent roles were more technical than leadership.</p> <p>That's fair feedback as my leadership style is that I wouldn't ask a member of my team to do something that I wouldn't do and I'm naturally curious about how things work, so there are times when I've rolled up my sleeves to help the team get projects done and/or be the technical lead that the team needed.</p> <p>So, let's try a different approach.</p> <p>I retooled my resume to highlight my technical accomplishments and applied for more experienced technical roles (think Senior Software Engineer, Staff Software Engineer, Principal Engineer, and Architect). Given the first round of feedback, I figured this would be a shoe-in, right?</p> <p>Not quite...</p> <p>Most of the feedback I got for these roles is that I wasn't hands-on technical enough (though my most recent engagements had me coding the vast majority of the time), though my leadership skills were solid.</p>"},{"location":"articles/2025/07/07/career-update-and-a-new-chapter/#what-do-you-want-to-be-when-you-grow-up","title":"What Do You Want To Be When You Grow Up?","text":"<p>Not going to lie, it's a bit frustrating to be told that you're too technical for leadership, but not technical enough for engineering, especially when you've helped companies launch new products and new offerings.</p> <p>My theory (potential cope) is that companies don't know what to do when they find someone who's both a strong technical leader and a strong engineer as they don't come across them that often. Most of the companies I've seen typically want their leadership to be able to understand concepts (i.e., what is continuous deployment pipeline), but not enough to implement or fix it when there are issues. For the engineering side, my experience was that they wanted people who were deep in the technical weeds (we're talking edge cases, knowing the docs cold), but didn't ask a ton about how the work fits into the bigger picture of the business.</p> <p>For me, I need a combination of both to be happy. I enjoy doing the technical work, building new tools/products to solve problems as I'm great at finding issues with a process and making it smoother.</p> <p>On the other hand, I thoroughly enjoy leading people and coaching up a team. That was one of the original inspirations for this blog, The Software Mentor, a way to give back and be an (unofficial) mentor to those who don't have someone to level up from.</p> <p>I can't just ignore one side of the equation, that's throwing away half my strengths.</p> <p>So what do you do?</p> <p>In my case, change the game.</p>"},{"location":"articles/2025/07/07/career-update-and-a-new-chapter/#a-different-approach","title":"A Different Approach","text":"<p>Over my career, I've been lucky to work at quite a few companies and have built great relationships everywhere I went. In addition, I've spent the last ten years building up a reputation in the community as a technical leader (Microsoft MVP since 2017 and have been presenting on technical concepts since 2015).</p> <p>I figured if I can't get work as an engineer or as a leader, why not try something new.</p> <p>Back in 2014, some friends and I had started on an ill-fated attempt to build a replacement Point of Sale system to be used by liquor stores (this would have been before tools like Square would become ubiquitous). Though the project never launched, we gave it a name, Small Batch Software as it was both a tip of the cap to small batch distilling and a nod to working in small batches (a la Lean Manufacturing with batch sizes of 1).</p> <p>We always joked that Small Batch might take off at some point, but we retired the project and the idea of Small Batch was retired (like most ideas go).</p> <p>Fast forward to 2021, I started taking on some side work, helping other companies with their implementation and process improvement. At the time, I didn't have a formal company, but I started thinking more about forming a company and working through that.</p> <p>When I left Rocket Mortgage in 2023, I decided to form my own company, Small Batch Solutions LLC to help me with a more formal approach for moonlighting, partly to get some experience and partly to see how it would go.</p> <p>However, like most things, it was a good idea, but I didn't put the required energy into the company, so it has been mostly dormant since then.</p> <p>Which brings us to the present.</p>"},{"location":"articles/2025/07/07/career-update-and-a-new-chapter/#small-batch-solutions-iteration-one","title":"Small Batch Solutions - Iteration One","text":"<p>After pouring more energy into the company over the past two months, I've had success in landing clients, focusing on what I enjoy doing the most: </p> <ul> <li>Mentoring others, helping them advance in their career and technical skills</li> <li>Problem solving, figuring out pain points and solving them simply</li> </ul> <p>Given these successes, I've chosen to focus on Small Batch Solutions full-time, allowing me to continue building with the community and also allowing me to use my strengths without having to fit in a single \"box\".</p> <p>If you've ever found yourself thinking, \"I wish I could work with someone who just gets it and can help me\", then reach out, I think I might be able to help!</p>"},{"location":"articles/2023/04/23/keeping-track---my-task-tracking-approach/","title":"Keeping Track - My Task Tracking Approach","text":"<p>When it comes to keeping track of things to do, I recall an ill-fated attempt at using a planner. My middle school introduced these planners for the students that you had to use to keep track of dates (and, weirdly enough, as a hall pass to go to the bathroom).</p> <p>Looking back, the intent was to have the students be more organized, but that wasn't what I learned. I found it cumbersome and a pain to keep track of. Also, you had to pay to replace it if it was lost or stolen.</p> <p>What I learned to do instead was to keep track of everything I needed to do in my memory, and if I forgot, well, I had to pay the penalty.</p> <p>I recall seeing my peers in high school and college be much more organized, and they made it so simple. Just color code these things, add these other things to a book and highlight these things.</p> <p>I didn't realize that my peers had developed a system for studying and keeping track of what they needed to do. Since I didn't know what it was called and felt awkward admitting I didn't know what it was, I would continue relying on my memory to get things done. However, this approach doesn't scale and is prone to having tasks drop from the list.</p> <p>When I started working at my second professional job, I found my boss to be organized and meticulous, and he never let anything slip. I learned a ton from him about process improvement and was introduced to a Kanban board for the first time.</p> <p>As an engineer, I would use a version of his approach for years, but when I got into leadership, I felt that I needed a better system. As an individual contributor, I could rely on the task board for what I needed to do, but that approach doesn't work for a leader because not all of your tasks are timely or fit in a neat Jira ticket.</p>"},{"location":"articles/2023/04/23/keeping-track---my-task-tracking-approach/#why-a-system","title":"Why a System?","text":"<p>Why do we need a system at all? Isn't memory good enough? The problem is that the human mind is fantastic at problem-solving but isn't great when it comes to recollection. In fact, multiple studies (like this one or this one) have shown that the more stressed you are, the worse your memory can become.</p> <p>With this context, you need to have some system to get the tasks out of your head and stored elsewhere. Whether that's physical sticky notes in your office, a notebook that you use, or some other tooling, I don't particularly care, but you do need something.</p>"},{"location":"articles/2023/04/23/keeping-track---my-task-tracking-approach/#my-approach","title":"My Approach","text":"<p>I'm loosely inspired by the Getting Things Done approach to task completion, which I've implemented as a Trello board. Having an online tool works for me because I can access it anywhere on my phone (no need to carry a notebook or other materials).</p> <p>Another side effect of having an online tool is that at any point I have an idea or a task that I need to do, I can add it to my Trello board in two clicks. No more worries about remembering to add the task when I'm back home or in the office, which allows me to not stress about it.</p>"},{"location":"articles/2023/04/23/keeping-track---my-task-tracking-approach/#work-intake-process","title":"Work Intake Process","text":"<p>On my Trello board (which you can copy a template from here), all tasks end up in the first column, called Inbox. The inbox is the landing spot for anything and everything. Throughout the day, I will process the list and move it to the appropriate column.</p> <ul> <li>Is it a task that I can knock out in 5 minutes or less? Just do it!</li> <li>Is it a task that will take more than 5 minutes? Then I move it into the To Do column</li> <li>Is it a task that I might be interested in? Is it a bigger task that I need to think more about? Then that goes into the Some Day column</li> <li>If the task is no longer needed, then it gets deleted.</li> </ul>"},{"location":"articles/2023/04/23/keeping-track---my-task-tracking-approach/#deciding-what-to-do-next","title":"Deciding What To Do Next","text":"<p>Once the inbox is emptied, I look at the items in the To Do column and pick the most important one. However, determining the most important one is not always the easy.</p> <p>For this, I leverage the Eisenhower Matrix approach.</p> <p>Named after Dwight D. Eisenhower, the idea is that we have two axes, one labeled Important and the other labeled Urgent. With these labels, tasks fall into four buckets:</p> <ul> <li>Urgent and Important - (e.g., production broken, everything is on fire)</li> <li>Urgent and Not Important - (e.g., last minute request, something that needs to be done, but not necessarily by you)</li> <li>Not Urgent and Important - (e.g., strategic work, things that need to get done, but not necessarily this moment)</li> <li>Not Urgent and Not Important - (e.g., time wasters, delete these tasks)</li> </ul> (2023, March 7). In Wikipedia. https://en.wikipedia.org/wiki/Time_management"},{"location":"articles/2023/04/23/keeping-track---my-task-tracking-approach/#dealing-with-roadblocks","title":"Dealing with Roadblocks","text":"<p>In an ideal world, you could take an item and run it to completion, but things aren't always that easy. You might need help from another person or are waiting for someone to do their part.</p> <p>When this happens, I'll move the item to the Waiting column and pick up a new task as I don't like to be stalled.</p> <p>However, I keep an eye on the number of items in flight as I've found that if I have more than three items in flight, I struggle with making progress and spend my time context-switching between the items instead of completing work. It can be challenging if the tasks are wholly unrelated (development tasks, writing, and reviewing pull requests) as the cost of regaining the context feels higher than if the tasks are related (e.g., reviewing multiple pull requests for the same repository).</p>"},{"location":"articles/2023/04/23/keeping-track---my-task-tracking-approach/#getting-things-done","title":"Getting Things Done","text":"<p>As items get completed, I add them to the Done column for the week. To help keep track of what I got done for the week, I typically call my Done column the week it spans (e.g., Apr 17-23, 2023). Once the week ends, I can refer back to the column, see where I spent my time, and reflect if I made the right choices for the week.</p> <p>Finally, I'll archive the list, create a new column for next week and repeat.</p>"},{"location":"articles/2013/10/12/the-basics-source-control/","title":"The Basics: Source Control","text":"<p>As the first part of the Basics of Software Development series, we\u2019re going to talk about source control is, why you should be using it and commonly used source control management (SCM) tools.</p>"},{"location":"articles/2013/10/12/the-basics-source-control/#source-control-whats-that","title":"Source Control, what\u2019s that?","text":"<p>To put it simply, SCM is a set of tools that allow users to keep track of source code by using a repository. The most common workflow is to get a copy of the source code from SCM, make changes to some of the files (refactoring for example) and committing those changes back.</p>"},{"location":"articles/2013/10/12/the-basics-source-control/#benefits-of-source-control","title":"Benefits of Source Control","text":"<ul> <li>Allows for users to undo uncommitted changes</li> <li>Promotes developers to refactor code</li> <li>Keeps track of what changes have been made over time</li> <li>Great for seeing what changes have been made over time (for example between versions)</li> <li>By definition, it\u2019s a backup of the source code</li> <li>Perfect if your workstation crashes and you don\u2019t have a backup</li> <li>Makes it easier for other developers to get the latest changes</li> <li>Instead of one developer making changes and \u201cpushing\u201d them to the team, source control allows the team to \u201cpull\u201d changes when ready.</li> </ul> <p>Without source control, anytime that code changes were made, the developer who made changes would have to push the code to the other developers. This may not sound terrible, but what if you were in the middle of rewriting a file? You would have to figure out what files have been changed, hope that the files that were changed aren\u2019t files that you\u2019re currently working on. After ensuring all of that, you need to merge your changes to the latest version and hope that nothing broke. As you can see, this is a horrible workflow, prone for errors. However, using a SCM solves these issues.</p>"},{"location":"articles/2013/10/12/the-basics-source-control/#choosing-source-control","title":"Choosing Source Control","text":"<p>All SCM tools work either as centralized or distributed. Before you can choose a source control, you need to figure what type of source control type works best for your situation (tech requirements, ease of setup, company policies, etc..)</p> <p>In a centralized source control implementation, there is a main server (central repository) that holds the source code and history of changes. When a developer checks out source code, they are only getting the source code (no revision history). After the developer makes changes and commits the changes, the files that have been modified are sent back to the central repository. Since the repository has the source code and history of changes, the repository is known to have the \u201cblessed\u201d or the \u201csingle source of truth\u201d copy of the source code.</p> <p>Since the centralized implementation utilizes the central server, it\u2019s very easy to see what\u2019s the latest and greatest. However, due to the centralization, if the server ever goes down, the team cannot commit changes.</p> <p>In a distributed source control implementation, the big difference is that there is not a central repository. This means that every developer has both the latest source code and the revision history.</p> <p>The main advantage to this implementation is that a developer can commit changes and continue working if there is no network connection. However, due to the lack of a central repository, there is no such thing as \u201cblessed\u201d version.</p>"},{"location":"articles/2013/10/12/the-basics-source-control/#common-source-control-implementations","title":"Common Source Control Implementations","text":"<p>Team Foundation Server (TFS) \u2013 This is a centralized source control that is most likely working with .NET code and Visual Studio. Even though It is possible to use TFS for other languages and IDE\u2019s, you have to use Team Explorer Everywhere and is currently supported with Eclipse.</p> <p>Subversion (SVN) \u2013 This is a centralized source control platform that is designed to be used for any programming language and IDE.</p> <p>Git \u2013 This is one of the most widely used distributed source control platform. This platform is most commonly used with GitHub.</p> <p>Mercurial \u2013 Another common distributed source control platform. This is most commonly used with Bitbucket.</p> <p>Visual SourceSafe \u2013 Very old solution from Microsoft, if your company is using this for source control, you need to migrate to another solution. This was the precursor for TFS and should no longer be used for source control.</p>"},{"location":"articles/2024/04/09/exploring-map-with-property-based-thinking/","title":"Exploring Map with Property Based Thinking","text":"<p>When thinking about software, it's natural to think about the things that it can do (its features like generating reports or adding an item to a cart).</p> <p>But what about the properties that those actions have? Those things that are always true?</p> <p>In this post, let's take a look at a fundamental tool of functional programming, the <code>map</code> function.</p> <p>All the code examples in this post will be using TypeScript, but the lessons hold for other languages with <code>Map</code> (or <code>Select</code> if you're coming from .NET).</p>"},{"location":"articles/2024/04/09/exploring-map-with-property-based-thinking/#examining-map","title":"Examining Map","text":"<p>In JavaScript/TypeScript, <code>map</code> is a function for arrays that allow us to transform an array of values into an array of different values.</p> <p>For example, let's say that we have an array of names and we want to ensure that each name is capitalized, we can write the following:</p> <pre><code>const capitalize = (name:string): string =&gt; {\n  return n[0].toUpperCase() + n.substring(1);\n}\nconst names = [\"alice\",\"bob\",\"charlotte\"];\n\nconst properNames = names.map(capitalize);\n</code></pre> <p>In our example, as long as we have a pure function that takes a string and returns a new type, then <code>map</code> will work.</p>"},{"location":"articles/2024/04/09/exploring-map-with-property-based-thinking/#what-does-map-guarantee","title":"What Does Map Guarantee?","text":"<p>Map is a cool function because it has a lot of properties that we get for free.</p> <ol> <li> <p>Maintains Length - If you call <code>map</code> on an array of 3 elements, then you'll get a new array with 3 elements. If you call <code>map</code> on an empty array, you'll get an empty array.</p> </li> <li> <p>Maintains Type - If you call <code>map</code> on array of type <code>T</code> with a function that goes from <code>T</code> to <code>U</code>, then every element in the new array is of type <code>U</code>.</p> </li> <li> <p>Maintains Order - If you call <code>map</code> on array with one function, then call <code>map</code> with a function that \"undoes\" the original map, then you end up with the original array.</p> </li> </ol>"},{"location":"articles/2024/04/09/exploring-map-with-property-based-thinking/#writing-property-based-tests","title":"Writing Property Based Tests","text":"<p>To prove these properties, we can write a set of unit tests. However, it would be hard to write a single test that that covers a single property.</p> <p>Most tests are example based in the sense that for a specific input, we get a specific output. Property based tests, on the other hand, uses random data and ensures that a property holds for all inputs. If it finds an input where the property fails, the test fails and you know which input caused the issue.</p> <p>Most languages have a tool for writing property-based tests, so we'll be using fast-check for writing property based tests and jest for our test runner</p>"},{"location":"articles/2024/04/09/exploring-map-with-property-based-thinking/#checking-length-property","title":"Checking Length Property","text":"<pre><code>import fc from \"fast-check\";\n\ndescribe(\"map\", () =&gt; {\n  it(\"maintains length\", () =&gt; {\n    // This is known as the identify function\n    // as it returns whatever input it received\n    const identity = &lt;T&gt;(a: T): T =&gt; a;\n\n    // Fast Check assert that the following holds for all arrays of integers\n    fc.assert(\n      // data is the array of numbers\n      fc.property(fc.array(fc.integer()), (data): void =&gt; {\n        // We call the map function with the identify function\n        const result = data.map(identity);\n\n        // We make sure that our result has the same length\n        expect(result).toHaveLength(data.length);\n      })\n    );\n  });\n</code></pre> <p>If we run this test, we'll end up passing. But what is the value of <code>data</code>?</p> <p>By adding a <code>console.log</code> in the test, we'll see the following values printed when we run the test (there are quite a few, so we'll examine the first few).</p> <pre><code>console.log\n    [\n       2125251991,  1334674146,\n      -1531633149,   332890473,\n       1313556939,   907640912,\n        887735692, -1979633703,\n       -259341001,  2015321027\n    ]\n\n  console.log\n    [ 1307879257 ]\n\n  console.log\n    []\n\n  # quite a few more...\n</code></pre>"},{"location":"articles/2024/04/09/exploring-map-with-property-based-thinking/#checking-type-property","title":"Checking Type Property","text":"<p>We've proven that the length property is being followed, so let's look at how we can ensure that <code>result</code> has the right type.</p> <p>To keep things simple, we're going to start with a <code>string[]</code> and map them to their lengths, yielding <code>number[]</code>.</p> <p>If <code>map</code> is working, then the result should be all <code>number</code>s.</p> <p>We can leverage <code>typeof</code> to check the type of each element in the array.</p> <pre><code>// An additional test in the describe block\n\nit(\"maintains type\", () =&gt; {\n  const getLength = (s:string)=&gt;s.length;\n  fc.assert(\n    // asserting with an array of strings\n    fc.property(fc.array(fc.string()), (data): void =&gt; {\n      // mapping to lengths of strings\n      const result = data.map(getLength);\n\n      // checking that all values are numbers\n      const isAllValid = result.every((x) =&gt; typeof x === \"number\");\n      expect(isAllValid).toBeTruthy();\n    })\n  );\n});\n</code></pre> <p>Like before, we can add a <code>console.log</code> to the test to see what strings are being generated</p> <pre><code>console.log\n  [ 'ptpJTR`G4', 's &gt;xmpXI', 'H++%;a3Y', 'OFD|+X8', 'gp' ]\n\nconsole.log\n  [ 'Rq', '', 'V&amp;+)Zy2VD8' ]\n\n\nconsole.log\n  [ 'o%}', '$o', 'w7C', 'O+!e', 'NS$:4\\\\9aq', 'xPbb}=F7h', 'z' ]\n\nconsole.log\n  [ '' ]\n\nconsole.log\n  [ 'apply', '' ]\n\nconsole.log\n  []\n## And many more entries...\n</code></pre>"},{"location":"articles/2024/04/09/exploring-map-with-property-based-thinking/#checking-order-property","title":"Checking Order Property","text":"<p>For our third property, we need to ensure that the order of the array is being maintained.</p> <p>To make this happen, we can use our <code>identity</code> function from before and check that our result is the same as the input. If so, then we know that the order is being maintained.</p> <pre><code>it(\"maintains order\", () =&gt; {\n  const identity = &lt;T&gt;(a: T) =&gt; a;\n\n  fc.assert(\n    fc.property(fc.array(fc.string()), (data): void =&gt; {\n      const result = data.map(identity);\n\n      expect(result).toEqual(data);\n    })\n  );\n});\n</code></pre> <p>And with that, we've verified that our third property holds!</p>"},{"location":"articles/2024/04/09/exploring-map-with-property-based-thinking/#so-what-why-properties","title":"So What, Why Properties?","text":"<p>When I think about the code I write, I'm thinking about the way it works, the way it should work, and the ways it shouldn't work. I find example based tests to help understand a business flow because of it's concrete values while property based tests help me understand the general guarantees of the code.</p> <p>I find that once I start thinking in properties, my code became cleaner because there's logic that I no longer had to write. In our <code>map</code> example, we don't have to write checks for if we have null or undefined because <code>map</code> always returns an array (empty in the worse case). There's also no need to write error handling because as long as the mapping function is pure, <code>map</code> will always return an array.</p> <p>For those looking to learn more about functional programming, you'll find that properties help describe the higher level constructs (functors, monoids, and monads) and what to look for.</p> <p>Finding properties can be a challenge, however, Scott Wlaschin (of FSharpForFunAndProfit) has a great post talking about design patterns that I've found to be immensely helpful.</p>"},{"location":"articles/2020/12/12/thinking-with-properties-examining-where/","title":"Thinking With Properties: Examining Where","text":"<p>Note: This post is for C# Advent Calendar 2020 organized by Matthew Groves. Check out some of the other posts that are happening over the month of December!</p>"},{"location":"articles/2020/12/12/thinking-with-properties-examining-where/#what-do-we-mean-by-properties","title":"What Do We Mean By Properties?","text":"<p>When I think about software, I will generally think about the properties that the solution has to have <code>Where</code> properties are the characteristics that the code has. Sometimes, the property is obvious (for example, if you square a number, the result should always be positive). In this post, we\u2019re going to look at LINQ\u2019s <code>Where</code> method, examine some of the properties, and come up with a more efficient way of creating filters.</p>"},{"location":"articles/2020/12/12/thinking-with-properties-examining-where/#examining-linqs-where-method","title":"Examining LINQ's <code>Where</code> Method","text":"<p>For those not familiar with <code>Where</code>, it\u2019s a method that promises that it will return a subset of a list <code>Where</code> each item fulfills some criteria (referred to as a predicate). The type signature for <code>Where</code> is</p> <p><code>IEnumerable&lt;T&gt; =&gt; Func&lt;T, bool&gt; =&gt; IEnumerable&lt;T&gt;</code></p> <p>At face value, this sounds pretty straightforward, but there are a few more properties that <code>Where</code> provides that aren\u2019t obvious at first, but are beneficial</p> <ul> <li>The results can\u2019t be null (worse case, it\u2019ll be an empty list because no item fulfilled the criteria)</li> <li>The results can\u2019t be larger than the original list</li> <li>The results are in the same order that the original list was in (i.e. if we\u2019re trying to find the even numbers in a list that is 1..10, then you\u2019re guaranteed to get 2, 4, 6, 8, and 10. Not, 8, 2, 6, 10, 4)</li> <li>The results will only contain elements that were in the original list (i.e. it can\u2019t create elements out of thin air and it can\u2019t copy elements in the original list)</li> </ul>"},{"location":"articles/2020/12/12/thinking-with-properties-examining-where/#common-linq-mistake-with-where","title":"Common LINQ Mistake with <code>Where</code>","text":"<p>That\u2019s a ton of guarantees that you get from leveraging <code>Where</code> instead of doing your own filtering inside loops. With these properties in mind, let\u2019s take a look at a common mistake that developers make when working with <code>Where</code></p> <pre><code>// Generate numbers from -10 .. 100\nvar numbers = Enumerable.Range(-10, 111);\n\n// Determines all positive numbers that are divisible by 6\nvar positiveDivisbleBySix = numbers\n                            .Where(x=&gt;x &gt; 0) // iterates through the whole list (111 comparisons, returning 100 results)\n                            .Where(x=&gt;x % 2 == 0) // iterates through the new list (100 comparisons, returning 50 results)\n                            .Where(x=&gt;x % 3 == 0); // iterates through the new list (50 comparisons, returning 16 results)\n// Overall metrics: 261 comparisons over 111 total elements)\n</code></pre> <p>By leveraging multiple Where statements, the list will be iterated once per statement. which may not be a big deal for small lists, but for larger lists, this will become a performance hit. In order to help cut down on the iterations, it\u2019d be ideal to combine the multiple Where statements into a single one like so</p> <pre><code>// Generate numbers from -10 .. 100\nvar numbers = Enumerable.Range(-10, 111);\n\nvar positiveDivisibleBySix = numbers.Where(x =&gt; x &gt; 0 &amp;&amp; x % 2 == 0 &amp;&amp; x % 3 == 0);\n</code></pre> <p>By combining the criteria in a single Where statement, we eliminate the multiple iteration problem, however, we introduce code that\u2019s a bit harder to read and if we want to combine a non-fixed number of predicates, then this approach won\u2019t work.</p> <p>Since the goal is to take multiple predicates and combine them to a single predicate, my intuition is to leverage LINQ\u2019s Aggregate method where we can take a List of items and reduce down to a single item.</p>"},{"location":"articles/2020/12/12/thinking-with-properties-examining-where/#refactoring-multiple-where-with-aggregate","title":"Refactoring Multiple Where with Aggregate","text":"<p>In order to leverage Aggregate, we\u2019ll first need to have a list of item to reduce down. Since all of the predicates are Func, we can easily create a List like so <pre><code>Func&lt;int, bool&gt; isPositive = x =&gt; x &gt; 0;\nFunc&lt;int, bool&gt; isEven = x =&gt; x % 2 == 0;\nFunc&lt;int, bool&gt; isDivisibleByThree = x =&gt; x % 3 == 0;\nvar predicates = new List&lt;Func&lt;int, bool&gt;&gt; {isPositive, isEven, isDivisibleByThree};\n</code></pre> <p>Now that we have a list of predicates, we can go ahead and start stubbing out the Aggregate call.</p> <pre><code>var combinedPredicate = predicates.Aggregate(...., ....);\n</code></pre> <p>In order to use Aggregate, we need to determine two pieces of information. First, what should the predicate be if there are no predicates to combine? Second, how do we we combine two predicates into a single predicate?</p>"},{"location":"articles/2020/12/12/thinking-with-properties-examining-where/#defining-the-base-case","title":"Defining the Base Case","text":"<p>When using Aggregate, the first thing that we need to think about is the base case, or in other words, what should the default value be if there are no elements to reduce down?</p> <p>Given that the result needs to be a predicate, we know that the type should be <code>Func&lt;int, bool&gt;</code>, but how do we implement that? We\u2019ve got one of two choices for the base case, we can either filter every item out (i.e. if no predicates are specified, then no items are kept) or we keep every item.</p> <p>For our use case, we want to keep every item if there are no predicates, so our base case looks like the following</p> <pre><code>Func&lt;int, bool&gt; andIdentity = _ =&gt; true;\n</code></pre>"},{"location":"articles/2020/12/12/thinking-with-properties-examining-where/#defining-how-to-combine-predicates","title":"Defining How To Combine Predicates","text":"<p>Since we\u2019re combining predicates, our combine function will need to have the following type</p> <p><code>Func&lt;int, bool&gt; =&gt; Func&lt;int, bool&gt; =&gt; Func&lt;int, bool&gt;</code></p> <pre><code>Func&lt;int, bool&gt; combinedPredicateWithAnd(Func&lt;int, bool&gt; a, Func&lt;int, bool&gt; b)\n{\n  return x =&gt; ...;\n}\n</code></pre> <p>With this in mind, we know that for an item to be valid, it has to match every predicate in the list which implies that we\u2019ll be leveraging the <code>&amp;&amp;</code> operator</p> <pre><code>Func&lt;int, bool&gt; combinedPredicateWithAnd(Func&lt;int, bool&gt; a, Func&lt;int, bool&gt; b)\n{\n  return x =&gt; ... &amp;&amp; ...;\n}\n</code></pre> <p>Now that we know to use <code>&amp;&amp;</code>, we can then use a and b to determine if the item is valid</p> <pre><code>Func&lt;int, bool&gt; combinedPredicateWithAnd(Func&lt;int, bool&gt; a, Func&lt;int, bool&gt; b)\n{\n  return x =&gt; a(x) &amp;&amp; b(x);\n}\n</code></pre>"},{"location":"articles/2020/12/12/thinking-with-properties-examining-where/#bringing-it-all-together","title":"Bringing It All Together","text":"<p>With the base case established and a way to combine predicates, here\u2019s how we can solve the original problem.</p> <pre><code>// Define the predicates\nFunc&lt;int, bool&gt; isPositive = x =&gt; x &gt; 0;\nFunc&lt;int, bool&gt; isEven = x =&gt; x % 2 == 0;\nFunc&lt;int, bool&gt; isDivisibleByThree = x =&gt; x % 3 == 0;\nvar predicates = new List&lt;Func&lt;int, bool&gt;&gt; {isPositive, isEven, isDivisibleByThree};\n\n// Defining the Aggregate functions\nFunc&lt;int, bool&gt; andIdentity = _ =&gt; true;\nFunc&lt;int, bool&gt; combinedPredicateWithAnd(Func&lt;int, bool&gt; a, Func&lt;int, bool&gt; b)\n{\n  return x =&gt; a(x) &amp;&amp; b(x);\n}\n\n// Combining the predicates\nFunc&lt;int, bool&gt; combinedAndPredicate = predicates.Aggregate(andIdentity, combinedPredicateWithAnd);\n\n// The new solution\n// Generate numbers from -10 .. 100\nvar numbers = Enumerable.Range(-10, 111);\nvar positiveDivisbleBySix = numbers.Where(combinedAndPredicate);\n</code></pre> <p>Going forward, if we need to add more predicates, all we need to do is to add it to the <code>List</code> and the rest of the application will work as expected</p>"},{"location":"articles/2020/12/12/thinking-with-properties-examining-where/#wrapping-up","title":"Wrapping Up","text":"<p>In this post, we explored LINQ\u2019s <code>Where</code> method by examining its various properties. From there, we took a look at a common mistake developers make with <code>Where</code> and then showed how to resolve that issue by using <code>Aggregate</code>.</p> <p>Shout out to Matthew Groves for letting me participate in C# Christmas (csadvent.christmas)</p>"},{"location":"articles/2023/04/09/three-steps-to-better-interviews/","title":"Three Steps to Better Interviews","text":"<p>At some point in your career, you're going to be conducting interviews. Regardless of the role, you have the opportunity to shape the future of the company as your recommendation controls whether this person is going to be a colleague or not.</p> <p>What a lot of people don't realize is that an interview is can be the first experience that someone has with your company. As such, you want this experience to be fantastic, even if they're not hired, as they could be a future customer of yours.</p> <p>With interviewing to be so important (there are whole books about the subject), it's confounding to me when companies don't invest in training or resources to help grow their leaders into being better interviewers. Especially, when making a bad hire can cause so much damage and is expensive to resolve in the long run.</p> <p>Over my career, I've seen my share of good and bad interviews and have some tips and tricks to improve your interviewing skills. In this post, I'm going to share three tips that help me have better conversations with candidates.</p>"},{"location":"articles/2023/04/09/three-steps-to-better-interviews/#1-build-better-conversations-using-scenarios","title":"1. Build Better Conversations Using Scenarios","text":"<p>The first mistake I seen interviewers make is that they have a set of questions that they want to pepper the candidate with, in an effort to figure out if they're going to be a good fit or not.</p> <p>An ideal interview should flow more like a conversation where the candidate is getting to know you and the company and where you are learning about the candidate. As such, a never-ending list of questions makes the candidate feel like they're being interrogated and it doesn't allow for a natural conversation. A great interview should feel like tennis, each player receiving and sending the ball to the other side.</p> <p>For example, let's say that I want to know a candidates familiarity with REST APIs. I could ask questions like</p> <p>What's the difference between GET and POST?</p> <p>What's the difference between a 404 and 400 response code?</p> <p>Even though I'll get answers, this is not much of a conversation, but more of a quiz. Instead, I ask the following</p> <p>In this scenario, I'm a newer engineer sitting down to make some changes to one of your APIs and I seem to be running into some issues.</p> <p>For example, when I invoke the endpoint via GET, I'm getting back a 404 (Not Found). Doing some digging, it seems like it's related to the resource not being there, but I'm not sure how to troubleshoot. What would you recommend?</p> <p>With the above, the candidate has a clear problem (e.g. can't communicate with the API) and has plenty of space to talk about what they're thinking (incorrect route, API not running, etc..). As the candidate is talking things through, I'm getting more insight on what they know and how they think about things. For example, if they mention that a firewall could be blocking the request, I could dig into that a bit more and learn that they have knowledge in networking or cloud technologies.</p> <p>Another advantage of this approach is that we can add more steps. For example, here's one of the scenarios I ask to measure understanding of REST APIs.</p> <p>In this scenario, I'm a newer engineer sitting down to make some changes to one of your APIs and I seem to be running into some issues.</p> <p>For example, when I invoke the endpoint via GET, I'm getting back a 404 (Not Found). Doing some digging, it seems like it's related to the resource not being there, but I'm not sure how to troubleshoot. What would you recommend?</p> <p>I've fixed the typo, made another request, and I'm now getting a 401 (Unauthorized). Looking up the response code, this implies that I don't have permissions, but I'm stuck on next steps. What would you recommend for troubleshooting?</p> <p>Oh right, Bearer Token, I remember reading that in the README, but I didn't understand at the time. After generating the token and making another request, I'm now getting a 400 (Bad Request). Looking up the status code, it seems like it's something related to the payload or route. How would you troubleshoot?</p> <p>Finally! After fixing that issue, I was able to get a 200 (Ok) response back, thanks for the help!</p> <p>By using the above scenario, I can learn quite a bit about what systems an engineer has worked with, what gaps they might have, and how they troubleshoot issues. This is a lot more effective than knowing if an engineer can tell the difference between GET and POST.</p>"},{"location":"articles/2023/04/09/three-steps-to-better-interviews/#2-build-better-conversations-using-open-ended-questions","title":"2. Build Better Conversations Using Open-Ended Questions","text":"<p>Another common mistake I see is asking closed-ended questions to gauge knowledge. Even though these are binary in nature (Yes/No) or have a specific answer (What's the capital of North Carolina?), they come off as interogative instead of conversational. In addition, these types of questions are informational and could easily be looked up, where as open-ended questions are opinion based and come from experience.</p> <p>For example, if we were to ask:</p> <p>What's the difference between an Observable and a Promise?</p> <p>We would know if the candidate knows the difference or not and that's about it. Even though this knowledge is helpful, we could learn this (and more) by rephrasing it to be open-ended instead.</p> <p>For example, if we were to ask:</p> <p>When would you use an Observable over a Promise?</p> <p>With this question, not only do we learn if the candidate can talk about Observable vs Promise, but we also know if they know which scenarios to use one over the other.</p> <p>For more effective questions, we could turn this question into a scenario, by asking the following</p> <p>Let's say that we're working on a web component that has to call an API to get some data. It looks like we could call the API and have the value returned be either an Observable or a Promise. What would you recommend and why?</p> <p>In this scenario, we get to learn if the candidate knows the differences between Observable and Promise, can reason about why one approach would be better than another, and explain that to another engineer. No matter their choice, we could follow up by asking why they wouldn't pick the other one option.</p>"},{"location":"articles/2023/04/09/three-steps-to-better-interviews/#3-build-better-conversations-by-asking-for-examples","title":"3. Build Better Conversations By Asking For Examples","text":"<p>For the final mistake, I see interviewers ask some form of a leading question, where based on the phrasing of the question, the candidate would be pressured or coerced into answering a particular way.</p> <p>For example</p> <p>This position involves mentoring interns to be associate engineers. Is that something you're comfortable with?</p> <p>This is a leading question because if the candidate were to say \"No\", then they would believe that they wouldn't get the job. So they would always answer yes, regardless of how they feel, which makes this question useless as it doesn't tell us anything about the candidate. Most leading questions tend to also be close-ended questions, so a double strike for this style of interviewing.</p> <p>But Cameron! I need to know this information as this person would be responsible for coaching up our engineers! Cool, then let's tell the candidate, but let's also provide some context and allow them to tell us their experience.</p> <p>For example, we could phrase the question this way</p> <p>One of the responsibilities for the role is to help grow interns into associate engineers so we can grow terrific engineers internally. With this context, can you walk us through a time where you had to coach someone up? What was your approach? What would you do differently?</p> <p>With this question, you've still mentioned the skill you're looking for, however, you've added context on the \"why\" behind the question and you've set the candidate up to talk about their experience, which in turn, gives you more context about the person.</p>"},{"location":"articles/2024/07/02/today-i-learned---using-typespec-to-generate-openapi-specs/","title":"Today I Learned - Using TypeSpec to Generate OpenAPI Specs","text":"<p>Recently, I was doing analysis for a project where we needed to build out a set of APIs for consumers to use. Even though I'm a big believer of iterative design, we wanted to have a solid idea of what the routes and data models were going to look like. </p> <p>In the past, I most likely would have generated a .NET Web API project, created the controllers/models, finally leveraging NSwag to generate Swagger documentation for the api. Even though this approach works, it does take more time on the implementation side (spinning up controllers, configuring ASP.NET, creating the models, adding attributes). In addition, if the actual API isn't being written in with .NET, then this code becomes throwaway pretty quickly.</p> <p>Since tooling is always evolving, I stumbled across another tool, TypeSpec. Heavily influenced by TypeScript, this allows you to write your contracts and models that, when compiled, produces an OpenAPI compliant spec.</p> <p>As a bonus, it's not restricted to just API spec as it has support for generating JSON schemas and gRPC's Protocol Buffers (protobuf)</p>"},{"location":"articles/2024/07/02/today-i-learned---using-typespec-to-generate-openapi-specs/#getting-started","title":"Getting Started","text":"<p>All code for this post can be found on my GitHub.</p> <p>Given that it's inspired by TypeScript, the tooling requires having Node installed (at least 20, but I'd recommend the long-term-supported (LTS) version).</p> <p>From there, we can install the TypeSpec tooling with.</p> <pre><code>npm install @typespec/compiler\n</code></pre> <p>Even though this is all the tooling that's required, I'd recommend installing an extension for either Visual Studio or Visual Studio Code so that you can get Intellisense and other visual cues while you're writing the code.</p>"},{"location":"articles/2024/07/02/today-i-learned---using-typespec-to-generate-openapi-specs/#bootstrapping-the-project","title":"Bootstrapping the project","text":"<p>Now that we've got the tooling squared away, let's create our project. </p> <pre><code>mkdir bookstore-api # let's make a directory to hold everything\ncd bookstore-api\ntsp init --template rest\n</code></pre> <p>Enter a project name and choose the defaults. Once it's finished bootstrapping, you can install necessary dependencies using <code>tsp install</code>.</p>"},{"location":"articles/2024/07/02/today-i-learned---using-typespec-to-generate-openapi-specs/#building-our-first-api","title":"Building Our First API","text":"<p>For our bookstore application, let's say that we want to have an <code>inventory</code> route where someone can retrieve information about a book. </p> <p>For this work, I'm picturing the following</p> <pre><code># Route -&gt; GET api/inventory/{id}\n# Returns 200 or 404\n</code></pre> <p>In the project, locate the <code>main.tsp</code> file and add the following</p> <pre><code>using TypeSpec.Http;\nusing TypeSpec.Rest;\n\n@service({\n    title: \"Bookstore Service\"\n})\nnamespace Bookstore {\n\n}\n</code></pre> <p>After adding this code, run <code>tsp compile .</code> (note the period). This will create a file in the <code>tsp-output/@typespec/openapi3</code> folder, <code>openapi.yaml</code>.</p> <p>We can open that file and see what our OpenAPI spec looks like </p> <pre><code>openapi: 3.0.0\ninfo:\n  title: Bookstore Service\n  version: 0.0.0\ntags: []\npaths: {}\ncomponents: {}\n</code></pre> <p>So far, not much to look at. However, if we copy this code and render feed it to an online render (like https://editor.swagger.io/), we'll get a message about no operations.</p> <p></p> <p>Let's change that by building out our GET endpoint.</p> <p>Back in <code>main.tsp</code>, let's add more code to our <code>Bookstore</code> namespace.</p> <pre><code>namespace Bookstore {\n    @tag(\"Inventory\")\n    @route(\"inventory\")\n    namespace Inventory {\n        @get op getBook(@path bookId:string): string\n    }\n}\n</code></pre> <p>After running <code>tsp compile .</code>, we'll see that our yaml has been updated and if we render it again, we'll have our first endpoint</p> <p></p> <p>This is closer to what we want, however, we know that we're returning back a <code>string</code>, but a <code>Book</code>.</p> <p>For this exercise, we'll say that a <code>Book</code> has the following:</p> <ul> <li>an id (of number)</li> <li>a title (of string)</li> <li>a price (of number, minimum 1)</li> <li>author name (of string)</li> </ul> <p>Let's add this model to <code>main.tsp</code></p> <pre><code>namespace Bookstore {\n    // Note that we've added this to the Bookstore namespace\n    model Book {\n        id: string;\n        title: string;\n\n        @minValue(1)\n        price: decimal;\n\n        authorName: string;\n    }\n    @tag(\"Inventory\")\n    @route(\"inventory\")\n    namespace Inventory {\n\n        // For our get, we're now returning a Book, instead of a string.\n        @get op getBook(@path bookId: string): Book; \n    }\n} \n</code></pre> <p>After another run of <code>tsp compile</code> and rendering the yaml file, we see that we have a schema for our get method now.</p> <p></p>"},{"location":"articles/2024/07/02/today-i-learned---using-typespec-to-generate-openapi-specs/#refactoring-a-model","title":"Refactoring a Model","text":"<p>Even though this works, the <code>Book</code> model is a bit lazy as it has the <code>authorName</code> as a property instead of an <code>Author</code> model which would have name (and a bit more information). Let's update <code>Book</code> to have an <code>Author</code> property.</p> <pre><code>model Author {\n    id: string;\n\n    @minLength(1)\n    surname: string;\n\n    @minLength(1)\n    givenName: string;\n}\nmodel Book {\n    id: string;\n    title: string;\n\n    @minValue(1)\n    price: decimal;\n\n    author: Author;\n}\n</code></pre> <p>After making this change, we can see that we now have a nested model for <code>Book</code>.</p> <p></p>"},{"location":"articles/2024/07/02/today-i-learned---using-typespec-to-generate-openapi-specs/#handling-failures","title":"Handling Failures","text":"<p>We're definitely a step in the right direction, however, our API definition isn't quite done. Right now, it says that we'll always return a 200 status code.</p> <p>I don't know about you, but our bookstore isn't good enough to generate books with fictitious IDs, so we need to update our contract to say that it can also return 404s.</p> <p>Back in <code>main.tsp</code>, we're going to change our return type of the <code>@get</code> operation to instead of being a <code>Book</code>, it's actually a union type.</p> <pre><code>@get op getBook(@path bookId: string): \n// Either it returns a 200 with a body of Book\n{\n    @statusCode statusCode: 200;\n    @body book: Book;\n} | { // Or it will return a 404 with an empty body\n    @statusCode statusCode: 404;\n};\n</code></pre> <p>With this final change, we can compile and render the yaml and see that route can return a 404 as well.</p> <p></p>"},{"location":"articles/2024/07/02/today-i-learned---using-typespec-to-generate-openapi-specs/#next-steps","title":"Next Steps","text":"<p>When I first started with TypeSpec, my first thought was that you could put this code under continuous integration (CI) and have it produce the OpenAPI format as an artifact for other teams to pull in and auto-generate clients from.</p> <p>If you're interested in learning more about that approach, drop me a line at the Coaching Corner and I may write up my results in a future post.</p>"},{"location":"articles/2014/12/22/today-i-learned--how-to-break-down-a-massive-method/","title":"Today I Learned \u2013 How to Break Down A Massive Method","text":"<p>During this past week, I was working with our intern and showing him some cool debugging tricks when I came across a massive method. I gave him 30 seconds to look at it and tell me what he thought the method was doing. After a while, he was able to figure it out, but the fact that it wasn\u2019t easy discernible was enough to give pause.</p> <p>The lesson here is that if you can\u2019t determine what the method is doing easily, then it\u2019s probably doing way too much (violating the Single Responsibility Principle) and needs to be broken into more easily readable pieces.</p> <p>To demonstrate what I mean, I wrote a program that inserts Messages into a database. A Message contains a description, the number (for identification when troubleshooting) and the module. We would have issues where different messages would have the same number which would cause confusion when troubleshooting errors.</p> <p>In the program I wrote, the user provides the message and what module the message belongs to and the program automatically generates the message number and inserts the message into the database.</p> <p>For brevity\u2019s sake, shown below is the logic for determining what the next message number should be.</p> <pre><code>public int GetNextAlertAndErrorModuleNumber(string module)\n{\n  if (String.IsNullOrEmpty(module))\n    throw new ArgumentException(\"module cannot be null or empty\");\n  if (_connection == null)\n    _connection = CreateConnection();\n\n  var results = new List&lt;int&gt;();\n\n  _connection.Open();\n  var cmd = new SqlCommand(\"dbo.GetAlertData\", _connection);\n  cmd.CommandType = CommandType.StoredProcedure;\n\n  var reader = cmd.ExecuteReader();\n  while (reader.Read())\n  {\n    if (!reader[\"ALERT_ID_NUMBER\"].ToString().Contains(module))\n      continue;\n\n    var pieces = reader[\"ALERT_ID_NUMBER\"].ToString().Split(\u2018 \u2018);\n\n    results.Add(Int32.Parse(pieces[1]));\n  }\n  if (reader != null)\n    reader.Close();\n\n  cmd = new SqlCommand(\"dbo.GetErrorData\";, _connection);\n  cmd.CommandType = CommandType.StoredProcedure;\n\n  reader = cmd.ExecuteReader();\n  while (reader.Read())\n  {\n    if (!reader[\"ERROR_ID_NUMBER\"].ToString().Contains(module))\n      continue;\n\n    var pieces = reader[\"ERROR_ID_NUMBER\"].ToString().Split(\u2018 \u2018);\n\n    results.Add(Int32.Parse(pieces[1]));\n  }\n  if (reader != null)\n    reader.Close();\n\n  if (_connection != null)\n    _connection.Close();\n\n  return results.Max() + 1;\n}\n</code></pre> <p>The method itself isn\u2019t complex, just calling some stored procedures, parsing the output and adding the number to a list. However, it\u2019s not abundantly clear what the purpose of the calling the stored procedures.</p> <p>First, it looks like we\u2019re reading the alerts error numbers from a stored procedure call, why don\u2019t we extract that logic out to a helper method and have the public method call the helper?</p> <pre><code>public int GetNextAlertAndErrorModuleNumber(string module)\n{\n  if (String.IsNullOrEmpty(module))\n    throw new ArgumentException(&amp;amp;amp;quot;module cannot be null or empty&amp;amp;amp;quot;);\n\n  if (_connection == null)\n    _connection = CreateConnection();\n\n  var results = new List&lt;int&gt;();\n\n  _connection.Open();\n  results.AddRange(ReadAlerts(module.ToUpper()));\n\n  var cmd = new SqlCommand(\"dbo.GetErrorData\", _connection);\n  cmd.CommandType = CommandType.StoredProcedure;\n\n  var reader = cmd.ExecuteReader();\n  while (reader.Read())\n  {\n    if (!reader[\"ERROR_ID_NUMBER\"].ToString().Contains(module))\n      continue;\n\n    var pieces = reader[\"ERROR_ID_NUMBER&amp;\"].ToString().Split(\u2018 \u2018);\n\n    results.Add(Int32.Parse(pieces[1]));\n  }\n  if (reader != null)\n    reader.Close();\n\n  if (_connection != null)\n    _connection.Close();\n\n  return results.Max() + 1;\n  }\n\n  private List&lt;int&gt; ReadAlerts(string module)\n  {\n    var results = new List&lt;int&gt;();\n    var cmd = new SqlCommand(\"dbo.GetAlertData\", _connection);\n    cmd.CommandType = CommandType.StoredProcedure;\n\n    var reader = cmd.ExecuteReader();\n    while (reader.Read())\n    {\n      if (!reader[\"ALERT_ID_NUMBER\"].ToString().Contains(module))\n        continue;\n\n      var pieces = reader[\"ALERT_ID_NUMBER\"].ToString().Split(\u2018 \u2018);\n      results.Add(Int32.Parse(pieces[1]));\n    }\n    if (reader != null)\n    reader.Close();\n\n    return results;\n}\n</code></pre> <p>By doing this, we fix two issues at once. First, we\u2019ve given a name to the process of reading the alerts which in turns allows us to quickly understand what the public method should be doing (i.e. improved readability).</p> <p>Second, it allows us for easier debugging because we now have smaller components. For example, let\u2019s say that we were getting the wrong value. In the first implementation, we would have to put breakpoints in different areas trying to determine which part was broken. However, in the new form, we can check to see if ReadAlerts is behaving correctly. If it isn\u2019t, we now know the bug has to be in that method, otherwise, it\u2019s in the rest.</p> <p>For the next step, you may have noticed that we can repeat the same refactoring trick again, except this time, we can extract the logic for reading the errors into a helper method.</p> <pre><code>public int GetNextAlertAndErrorModuleNumber(string module)\n{\n  if (String.IsNullOrEmpty(module))\n    throw new ArgumentException(\"module cannot be null or empty\");\n  if (_connection == null)\n    _connection = CreateConnection();\n\n  _connection.Open();\n\n  var results = new List&lt;int&gt;();\n  results.AddRange(ReadAlerts(module.ToUpper()));\n  results.AddRange(ReadErrors(module.ToUpper()));\n\n  if (_connection != null)\n    _connection.Close();\n\n  return results.Max() + 1;\n}\n\nprivate List&lt;int&gt; ReadAlerts(string module)\n{\n  var results = new List&lt;int&gt;();\n  var cmd = new SqlCommand(\"dbo.GetAlertData\", _connection);\n  cmd.CommandType = CommandType.StoredProcedure;\n\n  var reader = cmd.ExecuteReader();\n  while (reader.Read())\n  {\n    if (!reader[\"ALERT_ID_NUMBER\"].ToString().Contains(module))\n      continue;\n\n    var pieces = reader[\"ALERT_ID_NUMBER\"].ToString().Split(\u2018 \u2018);\n    results.Add(Int32.Parse(pieces[1]));\n  }\n  if (reader != null)\n    reader.Close();\n\n  return results;\n}\n\nprivate List&lt;int&gt; ReadErrors(string module)\n{\n  var results = new List&lt;int&gt;();\n  var cmd = new SqlCommand(\"dbo.GetErrorData\", _connection);\n  cmd.CommandType = CommandType.StoredProcedure;\n\n  var reader = cmd.ExecuteReader();\n  while (reader.Read())\n  {\n    if (!reader[\"ERROR_ID_NUMBER\"].ToString().Contains(module))\n      continue;\n\n    var pieces = reader[\"ERROR_ID_NUMBER\"].ToString().Split(\u2018 \u2018);\n    results.Add(Int32.Parse(pieces[1]));\n  }\n  if (reader != null)\n    reader.Close();\n\n  return results;\n}\n</code></pre> <p>After the changes, anyone who looks at the public API can easily see that it\u2019s reading from both Alerts and Errors. This is really powerful because now you can communicate with non-technical people about requirements and what the code is doing.</p> <p>Let\u2019s say that in the future, the requirements change and this conversation plays out:</p> <p>Alice (QA, finding an error) \u2013 Hey Bob, I was running one of our test plans and it looks like that we\u2019re getting the wrong message number if we\u2019re trying to add a new message and there are warning messages in the database. We are including the warning table when figuring that out, right?</p> <p>Bob (Engineer, finding the root cause) \u2013 Hey you\u2019re right, it looks like we\u2019re only using the alerts and error tables when calculating the next number. Why don\u2019t we write up a ticket for that and get a fix in?</p> <p>The key point is that no matter how large a method is, there always have to be steps being performed in some order (by definition of an algorithm) and this is the key to refactoring larger methods into more maintainable pieces of code. The trick is determining what those steps are and making decisions on whether to make helper methods or helper classes.</p> <p>If those steps become complicated, then they should be broken out into helper methods. As time progresses and those helper methods start to become more complicated, then those helper methods should in turn become classes of their own.</p>"},{"location":"articles/2024/03/19/today-i-learned--the-difference-between-bubble-and-capture-for-events/","title":"Today I Learned \u2013 The Difference Between Bubble and Capture for Events","text":"<p>I've recently been spending some time learning about Svelte and have been going through the tutorials.</p> <p>When I made it to the event modifiers section, I saw that there's a modifier for <code>capture</code> where it mentions firing the handler during the capture phase instead of the bubbling phase.</p> <p>I'm not an expert on front-end development, but I'm not familiar with either of these concepts. Thankfully, the Svelte docs refer out to MDN for a better explanation of the two.</p>"},{"location":"articles/2024/03/19/today-i-learned--the-difference-between-bubble-and-capture-for-events/#what-is-event-bubbling","title":"What is Event Bubbling?","text":"<p>Long story short, by default, when an event happens, the element that's been interacted with will fire first and then each parent will receive the event.</p> <p>So if we have the following HTML structure where there's a <code>body</code> that has a <code>div</code> that has a <code>button</code></p> <pre><code>&lt;body&gt;\n  &lt;div id=\"container\"&gt;\n    &lt;button&gt;Click me!&lt;/button&gt;\n  &lt;/div&gt;\n  &lt;pre id=\"output\"&gt;&lt;/pre&gt;\n&lt;/body&gt;\n</code></pre> <p>With the an event listener at each level:</p> <pre><code>// Setting up adding string to the pre element\nconst output = document.querySelector(\"#output\");\nconst handleClick = (e)=&gt; output.textContent += `You clicked on a ${e.currentTarget.tagName} element\\n`;\n\nconst container = document.querySelector(\"#container\");\nconst button = document.querySelector(\"button\");\n\n// Wiring up the event listeners\ndocument.body.addEventListener(\"click\", handleClick);\ncontainer.addEventListener(\"click\", handleClick);\nbutton.addEventListener(\"click\", handleClick);\n</code></pre> <p>And we click the button, our <code>&lt;pre&gt;</code> element will have:</p> <pre><code>You clicked on a BUTTON element\nYou clicked on a DIV element\nYou clicked on a BODY element\n</code></pre>"},{"location":"articles/2024/03/19/today-i-learned--the-difference-between-bubble-and-capture-for-events/#what-is-event-capturing","title":"What is Event Capturing?","text":"<p>Event Capturing is the opposite of Event Bubbling where the root parent receives the event and then each inner parent will receive the event, finally making it to the innermost child of the element that started the event.</p> <p>Let's see what happens with our example when we use the capture approach.</p> <pre><code>// Wiring up the event listeners\ndocument.body.addEventListener(\"click\", handleClick, {capture:true});\ncontainer.addEventListener(\"click\", handleClick, {capture:true});\nbutton.addEventListener(\"click\", handleClick, {capture:true});\n</code></pre> <p>After clicking the button, we'll see the following messages:</p> <pre><code>You clicked on a BODY element\nYou clicked on a DIV element\nYou clicked on a BUTTON element\n</code></pre>"},{"location":"articles/2024/03/19/today-i-learned--the-difference-between-bubble-and-capture-for-events/#why-would-you-use-capture","title":"Why Would You Use Capture?","text":"<p>By default, events will work in a bubbling fashion and this intuitively makes sense to me since the element that was interacted with is most likely the right element to handle the event.</p> <p>One case that comes to mind is if you finding yourself attaching the same event listener to every child element. Instead, we could move that up.</p> <p>For example, let's say that we had the following layout</p> <p><pre><code>&lt;div&gt;\n    &lt;ul style=\"list-style-type: none; padding: 0px; margin:0px; float:left\"&gt;\n      &lt;li&gt;&lt;a id=\"one\"&gt;Click on 1&lt;/a&gt;&lt;/li&gt;\n      &lt;li&gt;&lt;a id=\"two\"&gt;Click on 2&lt;/a&gt;&lt;/li&gt;\n      &lt;li&gt;&lt;a id=\"three\"&gt;Click on 3&lt;/a&gt;&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/div&gt;\n</code></pre> <pre><code>li {\n  list-style-type: none;\n  padding: 0px;\n  margin:0px;\n  float:left\n}\n\nli a {\n  color:black;\n  background:#eee;\n  border: 1px solid #ccc;\n  padding: 10px 15px;\n  display:block\n}\n</code></pre> Which gives us the following layout</p> <ul> <li>Click on 1</li> <li>Click on 2</li> <li>Click on 3</li> </ul> <p>With this layout, let's say that we need to do some business rules for when any of those buttons are clicked. If we used the bubble down approach, we would have the following code:</p> <pre><code>// Stand-in for real business rules\nfunction handleClick(e) {\n  console.log(`You clicked on ${e.target.id}`);\n}\n// Get all the a elements\nconst elements = document.querySelectorAll(\"a\");\n// Wire up the click handler\nfor (const e of elements) {\n  e.addEventListener(\"click\", handleClick);\n}\n</code></pre> <p>This isn't a big deal with three elements, but let's pretend that you had a list with tens of items, or a hundred items. You may run into a performance hit because of the overhead of wiring up that many event listeners.</p> <p>Instead, we can use one event listener, bound to the common parent. This can accomplish the same affect, without as much complexity.</p> <p>Let's revisit our JavaScript and make the necessary changes.</p> <pre><code>// Stand-in for real business rules\nfunction handleClick(e) {\n  // NEW: To handle the space of the unordered list, we'll return early\n  // if the currentTarget is the same as the original target\n  if (e.currentTarget === e.target) {\n    return;\n  }\n  console.log(`You clicked on ${e.target.id}`);\n}\n// NEW: Getting the common parent\nconst parent = document.querySelector(\"ul\");\n// NEW setting the eventListener to be capture based\nparent.addEventListener(\"click\", handleClick, {capture:true});\n</code></pre> <p>With this change, we're now only wiring up a single listener instead of having multiple wirings.</p>"},{"location":"articles/2024/03/19/today-i-learned--the-difference-between-bubble-and-capture-for-events/#wrapping-up","title":"Wrapping Up","text":"<p>In this post, we looked at two different event propagation models, bubble and capture, the differences between the two and when you might want to use capture.</p>"},{"location":"articles/2015/01/16/today-i-learned--the-chain-of-responsibility-design-pattern/","title":"Today I Learned \u2013 The Chain of Responsibility Design Pattern","text":"<p>There is nothing new in the world except the history you do not know. \u2013 Harry S. Truman</p> <p>The more experience I gain problem solving, the more this holds true. For this post, I\u2019m going to first discuss the problem that I was trying to solve. Next, I\u2019ll show what my first solution was, followed by the shortcomings of this solution. Thirdly, we\u2019ll iterate over a better solution to the problem. This in turn, will provide the motivation for what the Chain of Responsibility is and how to implement. Finally, I\u2019ll wrap up with what the benefits were of using this design. .</p>"},{"location":"articles/2015/01/16/today-i-learned--the-chain-of-responsibility-design-pattern/#problem-i-was-trying-to-solve","title":"Problem I was trying to solve","text":"<p>As part of the process of installing our software, there are scripts that will update the database from it\u2019s current version to the latest version. As it stands, it needs to be able to upgrade a database from any version to the current version.</p>"},{"location":"articles/2015/01/16/today-i-learned--the-chain-of-responsibility-design-pattern/#previous-solution","title":"Previous Solution","text":"<p>The first thing that comes to me is that I need to apply database scripts in a sequential way. For example, if the database\u2019s current version is 1.0 and the latest version is 3.0, it would need to apply the script to upgrade the database from 1.0 to 2.0 and then apply the script to upgrade the database from 2.0 to 3.0.</p> <p>For the first implementation, there were only two versions, 1.0 and 2.0. Since I didn\u2019t want to build in a lot of functionality if it wasn\u2019t needed yet, I created a helper method that returns the correct updater for a given version. In the below code, if the version does not exist, I assume the database does not exist and return the class that will create the database. Otherwise if the version is 1.0, I return a class that is responsible for the upgrading a database from 1.0 to 2.0. If the version is 2.0, I return a class that doesn\u2019t do anything (i.e. there\u2019s no upgrades to be done).</p> <pre><code>public IDatabaseUpdater GetDatabaseUpdater(string version)\n{\n  if (string.IsNullOrWhiteSpace(version))\n    return new DatabaseCreator();\n  if (version == \"1.0\")\n    return new Database100To200Updater();\n  if (version == \"2.0\")\n    return new CurrentVersionUpdater();\n  throw new ArgumentException(\"The version \" + version + \" is not supported for database upgrades.\");\n}\n</code></pre>"},{"location":"articles/2015/01/16/today-i-learned--the-chain-of-responsibility-design-pattern/#problem-with-solution","title":"Problem with solution","text":"<p>This solution worked well when there only three possible actions (create a new database, apply the single script, or do nothing). However, we are now going to be shipping version 3.0 and there will need to be a new class that is responsible for upgrading the 2.0 to 3.0. In order to add this functionality, I\u2019d have to do the following:</p> <ol> <li>Create the Database200To300Updater class that contained the logic for updating the database from 2.0 to 3.0.</li> <li>Modify the Database100To200Updater class to also use the Database200To300Updater in order to perform the next part of the upgrade.</li> <li>Add additional logic to the above method so that if the database is 2.0 to return the Database200To300Updater class.</li> </ol> <p>After making the modifications, the method now looks like:</p> <pre><code>public IDatabaseUpdater GetDatabaseUpdater(string version)\n{\n  if (string.IsNullOrWhiteSpace(version))\n    return new DatabaseCreator();\n  if (version == \"1.0\")\n    return new Database100To200Updater(new Database200To300Updater());\n  if (version == \"2.0\")\n    return new Database200To300Updater();\n  if (version == \"3.0\")\n    return new CurrentVersionUpdater();\n\n  throw new ArgumentException(\"The version \" + version + \" is not supported for database upgrades.\");\n}\n</code></pre> <p>So far, so good, we now have the logic to be able to apply scripts in order, however, now that we\u2019ve added version 3.0, I start to wonder what I would do if we added more versions? After some thought, it would look identical to the previous steps (see below for what would happen if we added version 4.0).</p> <pre><code>public IDatabaseUpdater GetDatabaseUpdater(string version)\n{\n  if (string.IsNullOrWhiteSpace(version))\n    return new DatabaseCreator();\n  if (version == \"1.0\")\n    return new Database100To200Updater(new Database200To300Updater(new Database300To400Updater()));\n  if (version == \"2.0\")\n    return new Database200To300Updater(new Database300To400Updater());\n  if (version == \"3.0\")\n    return new Database300To400Updater();\n  if (version == \"4.0\")\n    return new CurrentVersionUpdater();\n  throw new ArgumentException(\"The version \" + version + \" is not supported for database upgrades.\");\n}\n</code></pre> <p>If we create some variables to hold onto these classes, and reorder the if statements, we can write this helper method as:</p> <pre><code>public IDatabaseUpdater GetDatabaseUpdater(string version)\n{\n  if (string.IsNullOrWhiteSpace(version))\n    return new DatabaseCreator();\n  if (version == \"4.0\")\n    return new CurrentVersionUpdater();\n  var database300Updater = new Database300To400Updater();\n  var database200Updater = new Database200To300Updater(database300To400Updater);\n  var database100Updater = new Database100To200Updater(database200To300Updater);\n\n  if (version == \"1.0\")\n    return database100Updater;\n  if (version == \"2.0\")\n    return new database200Updater;\n  if (version == \"3.0\")\n    return new database300Updater;\n\n  throw new ArgumentException(\"The version \" + version + \" is not supported for database upgrades.\");\n}\n</code></pre>"},{"location":"articles/2015/01/16/today-i-learned--the-chain-of-responsibility-design-pattern/#motivation-for-the-chain-of-responsibility","title":"Motivation for the Chain of Responsibility","text":"<p>What I find interesting in this design is that I\u2019ve now chained these updater classes together so that if the version 1.0 is returned, it will also use the 2.0 updater, which in turn calls the 3.0 updater. It was at this point, that I remembered a design pattern that followed this structure.</p> <p>In this design pattern, you essentially have Handlers (in my case updaters) that check to see if they can handle the request. If so, they do and that stops the chain. However, if they can\u2019t handle the request, they pass it to their Successor (which was also a Handler) to handle the request. The design pattern I was thinking about is the Chain of Responsibility pattern.</p> <p>In order to implement this pattern, you need to have an IHandler interface that exposes a Handle method and either a method or property to set the Successor. The method is the action to take (in our case Update) and the Successor represents the next Handler in the chain if the request could not be handled. The second component is referred to as ConcreteHandlers and they are just the implementors of the interface. One way to implement this is like the following:</p> <pre><code>public interface IHandler\n{\n  IHandler Successor { get; set; }\n  void Update(int version);\n}\n\npublic class ConcreteHandlerA : IHandler\n{\n  public IHandler Successor { get; set; }\n\n  public void Update(int version)\n  {\n    if (CanTheRequestBeHandled) {\n      // handle the request\n    }\n    else {\n      Successor.Update(version);\n    }\n  }\n}\n</code></pre> <p>The main difference between the pattern and what I need is that instead of doing if (canHandle)/else call Successor, what I\u2019m really looking for is to run the upgrade script if the version we\u2019re upgrading to is higher than our current version and then always call the successor. Given this change, here\u2019s what that new implementation looks like:</p> <pre><code>public class ConcreteHandlerA : IHandler\n{\n  public Successor { get; set; }\n  public void Update(int version)\n  {\n    if (CanTheRequestBeHandled) {\n      // handle the request\n    }\n    Successor.Update(version);\n  }\n}\n</code></pre>"},{"location":"articles/2015/01/16/today-i-learned--the-chain-of-responsibility-design-pattern/#implementing-the-chain-of-responsibility","title":"Implementing the Chain of Responsibility","text":"<p>Now that I know the pattern to use and how it works, I need to update the IDatabaseUpdater interface to follow the IHandler interface. Next, I will need to modify the concrete handlers to use the new interface correctly.</p>"},{"location":"articles/2015/01/16/today-i-learned--the-chain-of-responsibility-design-pattern/#implementing-the-handler","title":"Implementing the Handler","text":"<p>First, we will update our IDatabaseUpdater interface to follow the IHandler look:</p>"},{"location":"articles/2015/01/16/today-i-learned--the-chain-of-responsibility-design-pattern/#before","title":"Before","text":"<pre><code>public interface IDatabaseUpdater\n{\n  void Update(int version);\n}\n</code></pre>"},{"location":"articles/2015/01/16/today-i-learned--the-chain-of-responsibility-design-pattern/#after","title":"After","text":"<pre><code>public interface IDatabaseUpdateHandler\n{\n  void Update(int version);\n  IDatabaseUpdateHandler Successor { get; set; }\n}\n</code></pre>"},{"location":"articles/2015/01/16/today-i-learned--the-chain-of-responsibility-design-pattern/#implementing-the-concrete-handler","title":"Implementing the Concrete Handler","text":"<p>Second, we will need to update our concrete handlers to implement the interface correctly and to update their UpdateMethod to follow the design. In my case, the concrete handlers perform similar logic, so one of the classes is used for an example.</p>"},{"location":"articles/2015/01/16/today-i-learned--the-chain-of-responsibility-design-pattern/#before_1","title":"Before","text":"<pre><code>public class Database100To200Updater : IDatabaseUpdater\n{\n  private Database200To300Updater _successor;\n  public Database100To200Updater(Database200To300Updater successor)\n  {\n    if (successor == null)\n      throw new ArgumentNullException(\"successor\");\n    _successor = successor;\n  }\n\n  public void Update()\n  {\n    Console.WriteLine(\"Updating the database to version 2.0\");\n    _successor.Update();\n  }\n}\n</code></pre>"},{"location":"articles/2015/01/16/today-i-learned--the-chain-of-responsibility-design-pattern/#after_1","title":"After","text":"<p>Thanks to the public property, I was able to remove the private member and that in turn allowed me to remove the constructor. <pre><code>public class Database100To200Updater : IDatabaseUpdateHandler\n{\n  public void Update(int version)\n  {\n    if (version &gt;= 2)\n      Console.WriteLine(\"Updating the database to version 2.0\");\n    if (Successor != null)\n      Successor.Update(version);\n  }\n\n  public IDatabaseUpdateHandler Successor { get; set;}\n}\n</code></pre></p>"},{"location":"articles/2015/01/16/today-i-learned--the-chain-of-responsibility-design-pattern/#updating-the-helper-method","title":"Updating the Helper Method","text":"<p>Now that we\u2019ve updated the interface and implementors, it\u2019s time to update the helper method to take advantage of the new design.</p> <pre><code>public IDatabaseUpdateHandler GetDatabaseUpdater(string version)\n{\n  if (string.IsNullOrWhiteSpace(version))\n    return new DatabaseCreator();\n\n  var database300To400 = new Database300To400Updater();\n  var database200To300 = new Database200To300Updater();\n  var database100To200 = new Database100To200Updater();\n\n  database100To200.Successor = database200To300;\n  database200To300.Successor = database300To400;\n\n  return database100To200;\n}\n</code></pre>"},{"location":"articles/2015/01/16/today-i-learned--the-chain-of-responsibility-design-pattern/#chain-of-responsibility-is-great-heres-why","title":"Chain of Responsibility is great, here\u2019s why","text":"<p>What I really like about the chain of responsibility pattern is that I was able to connect my upgrade classes together in a consistent fashion. Another reason why I like this pattern is that it forces me to have the logic to determine whether I should run the update or not inside the individual classes instead of the helper method. This produces more readable code which then lends itself to easier maintainability.</p>"},{"location":"articles/2024/08/06/today-i-learned-changing-the-entrypoint-for-a-docker-container/","title":"Today I Learned: Changing the Entrypoint for a Docker Container","text":"<p>A common pattern for building software today is to develop using containers. This is a great strategy because it can ensure that everyone is building/deploying the same environment every time. However, like any other tooling, it can take a bit to create the file and make sure that you're setting it up correctly.</p> <p>When I'm building a container, I typically will run <code>bash</code> in the container so I can inspect files, paths, permissions and more.</p> <p>For example, let's say that I wanted to see what all is in the <code>node</code> image, I could run the following</p> <pre><code>docker run -it node:22 bash\n</code></pre> <p>And as long as the <code>node</code> image has bash installed, I can take a look.</p> <p>This trick works just fine when I have access to the Dockerfile and can change it. But what if you didn't have access to the Dockerfile? How would you troubleshoot?</p>"},{"location":"articles/2024/08/06/today-i-learned-changing-the-entrypoint-for-a-docker-container/#the-scenario","title":"The Scenario","text":"<p>Let's say that I'm using a container that was created from another team, called <code>deps</code>. When I try to run it though, I get the following error:</p> <pre><code>docker run -it deps:latest\n</code></pre> <p></p> <p>Looking at the error message, it says that it couldn't find <code>/app/hello.js</code>. I could let the other team know and let them figure it out. However, I'd like to give them a bit more info and possible advice, so I could use the <code>bash</code> trick from before and see if I can spot the problem.</p> <pre><code>docker run -it deps:latest bash\n</code></pre> <p></p> <p>Wait a minute! Why did I get the same error?</p> <p>The reason is because the image has an <code>ENTRYPOINT</code> defined, which means that whenever the container starts, it's going to run that command. Since that command is the one that's failing, the container crashes before it executes bash or any other command.</p>"},{"location":"articles/2024/08/06/today-i-learned-changing-the-entrypoint-for-a-docker-container/#the-solution","title":"The Solution","text":"<p>Since I don't have access to the Dockerfile, I need a way to change the entrypoint to allow it to run bash. Luckily, it turns out that the <code>docker run</code> command has a <code>--entrypoint</code> param that you can set to be whatever the command should be.</p> <p>So let's run the container, except this time, specifying bash as the entrypoint.</p> <pre><code>docker run -it --entrypoint \"bash\" deps:latest\n</code></pre> <p>And if I run the <code>ls</code> command, I can see that the issue is that the file is called index.js, not hello.js. </p> <p></p> <p>From here, I can give this information to the other team and they can make the necessary changes to their Dockerfile.</p>"},{"location":"articles/2025/03/05/today-i-learned-configuring-httpclient-via-service-registration/","title":"Today I Learned: Configuring HttpClient via Service Registration","text":"<p>When integrating with an external service via an API call, it's common to create a class the encapsulates dealing with the API. For example, if I was interacting with the GitHub API, I might create a C# class that wraps the <code>HttpClient</code>, like the following:</p> <pre><code>public interface IGitHubService\n{\n    Task&lt;string&gt; GetCurrentUsername();\n}\n\npublic class GitHubService : IGitHubService\n{\n    private readonly HttpClient _client;\n    public GitHubService(HttpClient client)\n    {\n        _client = client;\n    }\n    public async Task&lt;string&gt; GetCurrentUsername()\n    {\n        // code implementation\n    }\n}\n</code></pre>"},{"location":"articles/2025/03/05/today-i-learned-configuring-httpclient-via-service-registration/#repetition-of-values","title":"Repetition of Values","text":"<p>This is a great start, but over time, your class might end up like the following:</p> <pre><code>public class GitHubService\n{\n    private readonly HttpClient _client;\n    public GitHubService(HttpClient client)\n    {\n        _client = client;\n    }\n    public async Task&lt;string&gt; GetCurrentUsername()\n    {\n        var result = _client.GetFromJsonAsync(\"https://api.github.com/user\")\n        return result.Login;\n    }\n    public async Task&lt;List&lt;string&gt;&gt; GetAllUsers()\n    {\n        var result = _client.GetFromJsonAsync(\"https://api.github.com/users\");\n        return result.Select(x =&gt; x.Login).ToList();\n    }\n    public async Task&lt;List&lt;string&gt;&gt; GetTeamNamesForOrg(string org)\n    {\n        var result = _client.GetFromJsonAsync($\"https://api.github.com/orgs/{org}/teams\");\n        return result.Select(x =&gt; x.Name).ToList();\n    }\n}\n</code></pre> <p>Right off the bat, it seems like we're repeating the URL for each method call. To remove the repetition, we could extract to a constant.</p> <pre><code>public class GitHubService\n{\n    private readonly HttpClient _client;\n    // Setting the base URL for later usage\n    private const string _baseUrl = \"https://api.github.com\";\n    public GitHubService(HttpClient client)\n    {\n        _client = client;\n    }\n    public async Task&lt;string&gt; GetCurrentUsername()\n    {\n        var result = _client.GetFromJsonAsync($\"{_baseUrl}/user\")\n        return result.Login;\n    }\n    public async Task&lt;List&lt;string&gt;&gt; GetAllUsers()\n    {\n        var result = _client.GetFromJsonAsync($\"{_baseUrl}/users\");\n        return result.Select(x =&gt; x.Login).ToList();\n    }\n    public async Task&lt;List&lt;string&gt;&gt; GetTeamNamesForOrg(string org)\n    {\n        var result = _client.GetFromJsonAsync($\"{_baseUrl}/orgs/{org}/teams\");\n        return result.Select(x =&gt; x.Name).ToList();\n    }\n}\n</code></pre> <p>This helps remove the repetition, however, we're now keeping track of a new field, <code>_baseUrl</code>. Instead of using this, we could leverage the <code>BaseAddress</code> property and set that in the service's constructor.</p> <pre><code>public class GitHubService\n{\n    private readonly HttpClient _client;\n    public GitHubService(HttpClient client)\n    {\n        _client = client;\n        _client.BaseAddress = \"https://api.github.com\"; // Setting the base address for the other requests.\n    }\n    public async Task&lt;string&gt; GetCurrentUsername()\n    {\n        var result = _client.GetFromJsonAsync(\"/user\")\n        return result.Login;\n    }\n    public async Task&lt;List&lt;string&gt;&gt; GetAllUsers()\n    {\n        var result = _client.GetFromJsonAsync(\"/users\");\n        return result.Select(x =&gt; x.Login).ToList();\n    }\n    public async Task&lt;List&lt;string&gt;&gt; GetTeamNamesForOrg(string org)\n    {\n        var result = _client.GetFromJsonAsync($\"/orgs/{org}/teams\");\n        return result.Select(x =&gt; x.Name).ToList();\n    }\n}\n</code></pre> <p>I like this refactor because we remove the field and we have our configuration in one spot. That being said, interacting with an API typically requires more information than just the URL. For example, setting up the API token or that we're always expecting JSON for the response. We could add the header setup in each method, but that seems quite duplicative.</p>"},{"location":"articles/2025/03/05/today-i-learned-configuring-httpclient-via-service-registration/#leveraging-default-request-headers","title":"Leveraging Default Request Headers","text":"<p>We can centralize our request headers by leveraging the <code>DefaultRequestHeaders</code> property and updating our constructor.</p> <pre><code>public class GitHubService\n{\n    private readonly HttpClient _client;\n    public GitHubService(HttpClient client)\n    {\n        _client = client;\n        _client.BaseAddress = \"https://api.github.com\";\n        _client.DefaultRequestHeaders.Add(\"Accept\", \"application/vnd.github+json\");\n        _client.DefaultRequestHeaders.Add(\"Authentication\", $\"Bearer {yourTokenGoesHere}\");\n        _client.DefaultRequestHeaders.Add(\"X-GitHub-Api-Version\", \"2022-11-28\");\n    }\n    public async Task&lt;string&gt; GetCurrentUsername()\n    {\n        var result = _client.GetFromJsonAsync(\"/user\")\n        return result.Login;\n    }\n    public async Task&lt;List&lt;string&gt;&gt; GetAllUsers()\n    {\n        var result = _client.GetFromJsonAsync(\"/users\");\n        return result.Select(x =&gt; x.Login).ToList();\n    }\n    public async Task&lt;List&lt;string&gt;&gt; GetTeamNamesForOrg(string org)\n    {\n        var result = _client.GetFromJsonAsync($\"/orgs/{org}/teams\");\n        return result.Select(x =&gt; x.Name).ToList();\n    }\n}\n</code></pre> <p>I like this refactor because all of our configuration of the service is right next to how we're using it, so easy to troubleshoot. At this point, we would need to register our service in the Inversion of Control (IoC) container and then everything would work.</p> <p>Generally, you'll find this in <code>Startup.cs</code> and would look like:</p> <pre><code>services.AddTransient&lt;IGitHubService, GitHubService&gt;();\n</code></pre>"},{"location":"articles/2025/03/05/today-i-learned-configuring-httpclient-via-service-registration/#an-alternative-approach-for-service-registration","title":"An Alternative Approach for Service Registration","text":"<p>However, I learned that when you're building a service that's wrapping an <code>HttpClient</code>, there's another service registration method you could use, <code>AddHttpClient</code> with the Typed Client approach.</p> <p>Let's take a look at what this would look like.</p> <pre><code>// In Startup.cs\n\nservices.AddHttpClient&lt;IGitHubService, GitHubService&gt;(client =&gt; {\n    client.BaseAddress = new Uri(\"https://api.github.com\");\n    client.DefaultRequestHeaders.Add(\"Accept\", \"application/vnd.github+json\");\n    client.DefaultRequestHeaders.Add(\"Authorization\", $\"Bearer {apiTokenGoesHere}\");\n    client.DefaultRequestHeaders.Add(\"X-GitHub-Api-Version\", \"2022-11-28\");\n});\n</code></pre> <p>We've essentially moved our configuration logic from the <code>GitHubService</code> to the IoC container, simplifying the service.</p> <pre><code>public class GitHubService : IGitHubService\n{\n    private readonly HttpClient _client;\n    public GitHubService(HttpClient client)\n    {\n        _client = client;\n    }\n    public async Task&lt;string&gt; GetCurrentUsername()\n    {\n        var result = _client.GetFromJsonAsync(\"/user\")\n        return result.Login;\n    }\n    public async Task&lt;List&lt;string&gt;&gt; GetAllUsers()\n    {\n        var result = _client.GetFromJsonAsync(\"/users\");\n        return result.Select(x =&gt; x.Login).ToList();\n    }\n    public async Task&lt;List&lt;string&gt;&gt; GetTeamNamesForOrg(string org)\n    {\n        var result = _client.GetFromJsonAsync($\"/orgs/{org}/teams\");\n        return result.Select(x =&gt; x.Name).ToList();\n    }\n}\n</code></pre>"},{"location":"articles/2025/03/05/today-i-learned-configuring-httpclient-via-service-registration/#my-thoughts","title":"My Thoughts","text":"<p>Even though this is a new approach, I'm kind of torn if I like it or not. On one hand, I appreciate that we can centralize the logic in one spot so that everything for the <code>GitHubService</code> is one spot. However, if we needed other dependencies to configure the service (for example, we needed to get the bearer token from <code>AppSettings</code>), I could see this getting a bit more complicated, though contained.</p> <p>On the other hand, we could shift all that config to the IoC and let it deal with that. It definitely streamlines the <code>GitHubService</code> so we can focus on the endpoints and their logic, however, now I've got to look for two spots to see where the client is being configured.</p>"},{"location":"articles/2025/03/12/today-i-learned-configuring-git-to-use-a-specific-key-for-a-specific-repo/","title":"Today I Learned: Configuring Git to Use A Specific Key for a Specific Repo","text":"<p>When working with a Git repository, there are two ways to authenticate yourself, either by using your name/password or by leveraging an SSH key, proving who you are. I like the SSH key the best since I can create the key for my machine, associate it to my profile, and then I'm good to go.</p> <p>However, the problem becomes when I have to juggle different SSH keys. For example, I may have two different accounts (personal and a work one) and these accounts can't use the same SSH key (in fact, if you try to add the same SSH key to two different accounts, you'll get an error message).</p> <p>In these cases, I'd like to be able to specify which SSH key to use for a given repository.</p> <p>Doing some digging, I found the following configures the sshCommand that git uses</p> <pre><code>git config core.sshCommand 'ssh -i C:\\\\users\\\\&lt;name&gt;\\\\.ssh\\\\&lt;name_of_private_key&gt;'\n</code></pre> <p>Because we're not specifying a scope (like <code>--global</code>), this will only apply to the single repository.</p>"},{"location":"articles/2024/04/13/today-i-learned---iterating-through-union-types/","title":"Today I Learned - Iterating Through Union Types","text":"<p>In a previous post, we cover on how using union types in TypeScript is a great approach for domain modeling because it limits the possible values that a type can have.</p> <p>For example, let's say that we're modeling a card game with a standard deck of playing cards. We could model the domain as such.</p> <pre><code>type Rank = \"Ace\" | \"Two\" | \"Three\" | \"Four\" | \"Five\" | \"Six\" | \"Seven\"\n           | \"Eight\" | \"Nine\" | \"Ten\" |\"Jack\" | \"Queen\" | \"King\"\ntype Suite = \"Hearts\" | \"Clubs\" | \"Spades\" | \"Diamonds\"\n\ntype Card = {rank:Rank; suite:Suit}\n</code></pre> <p>With this modeling, there's no way to create a <code>Card</code> such that it has an invalid <code>Rank</code> or <code>Suite</code>.</p> <p>With this definition, let's create a function to build the deck.</p> <pre><code>function createDeck(): Card[] {\n  const ranks = [\"Ace\", \"Two\", \"Three\", \"Four\", \"Five\", \"Six\", \"Seven\", \"Eight\", \"Nine\", \"Ten\", \"Jack\", \"Queen\", \"King\"];\n  const suites = [\"Hearts\", \"Clubs\", \"Spades\", \"Diamonds\"];\n\n  const deck:Card[] = [];\n  for (const rank of ranks) {\n    for (const suite of suites) {\n      deck.push({rank, suite});\n    }\n  }\n  return deck;\n}\n</code></pre> <p>This code works, however, I don't like the fact that I had to formally list the option for both <code>Rank</code> and <code>Suite</code> as this means that I have two different representtions for <code>Rank</code> and <code>Suite</code>, which implies tthat if we needed to add a new <code>Rank</code> or <code>Suite</code>, then we'd need to add it in two places (a violation of DRY).</p> <p>Doing some digging, I found this StackOverflow post that gave a different way of defining our <code>Rank</code> and <code>Suite</code> types. Let's try that new definition.</p> <pre><code>const ranks = [\"Ace\", \"Two\", \"Three\", \"Four\", \"Five\", \"Six\", \"Seven\", \"Eight\", \"Nine\", \"Ten\", \"Jack\", \"Queen\", \"King\"] as const;\ntype Rank = typeof ranks[number];\nconst suites = [\"Hearts\", \"Clubs\", \"Spades\", \"Diamonds\"] as const;\ntype Suite = typeof suites[number]\n</code></pre> <p>In this above code, we're saying that <code>ranks</code> cannot change (either by assignment or by operations like <code>push</code>). With that definition, we can say that <code>Rank</code> is some entry in the <code>ranks</code> array. Similar approach for our <code>suites</code> array and <code>Suite</code> type.</p> <p>I prefer this approach much more because we have our ranks and suites defined in one place and our code reads cleaner as this says Here are the possible ranks and Rank can only be one of those choices.</p>"},{"location":"articles/2024/04/13/today-i-learned---iterating-through-union-types/#limitations","title":"Limitations","text":"<p>The main limitation is that it only works for \"enum\" style unions. Let's change example and say that we want to model a series of shapes with the following.</p> <pre><code>type Circle = {radius:number};\ntype Square = {length:number};\ntype Rectangle = {height:number, width:number}\n\ntype Shape = Circle | Square | Rectangle\n</code></pre> <p>To use the same trick, we would need to have an array of constant values. However, we can't have a constant value for any of the <code>Shape</code>s because there are an infinite number of valid <code>Circle</code>s, <code>Square</code>s, and <code>Rectangle</code>s.</p>"},{"location":"articles/2024/08/19/today-i-learned---leveraging-mock-names-with-jest/","title":"Today I Learned - Leveraging Mock Names with Jest","text":"<p>I was working through the Mars Rover kata the other day and found myself in a predicament when trying to test one of the functions, the <code>convertCommandToAction</code> function.</p> <p>The idea behind the function is that based on the Command you pass in, it'll return the right function to call. The code looks something like this.</p> <pre><code>type Command = 'MoveForward' | 'MoveBackward' | 'TurnLeft' | 'TurnRight' | 'Quit'\ntype Action = (r:Rover):Rover;\n\nconst moveForward:Action = (r:Rover):Rover =&gt; {\n  // business rules\n}\nconst moveBackward:Action = (r:Rover): Rover =&gt; {\n  // business rules\n}\nconst turnLeft:Action = (r:Rover):Rover =&gt; {\n  // business rules\n}\nconst turnRight:Action = (r:Rover): Rover =&gt; {\n  // business rules\n}\nconst quit:Action = (r:Rover):Rover =&gt; {\n  // business rules\n}\n\n// Function that I'm wanting to write tests against.\nfunction convertCommandToAction(c:Command): Action {\n  switch (c) {\n    case 'MoveForward': return moveForward;\n    case 'MoveBackward': return moveBackward;\n    case 'TurnLeft': return turnLeft;\n    case 'TurnRight': return turnRight;\n    case 'Quit': return quit;\n  }\n}\n</code></pre> <p>I'm able to write tests across all the other functions easily enough, but for the <code>convertCommandToAction</code>, I needed some way to know which function is being returned.</p> <p>Since I don't want the real functions to be used, my mind went to leveraging <code>Jest</code> and mocking out the module that the actions were defined in, yielding the following test setup.</p> <pre><code>import { Command } from \"./models\";\nimport { convertCommandToAction, convertStringToCommand } from \"./parsers\";\n\njest.mock(\"./actions\", () =&gt; ({\n  moveForward: jest.fn(),\n  moveBackward: jest.fn(),\n  turnLeft: jest.fn(),\n  turnRight: jest.fn(),\n  quit: jest.fn(),\n}));\n\ndescribe(\"When converting a Command to an Action\", () =&gt; {\n  it(\"and the command is MoveForward, then the right action is returned\", () =&gt; {\n    const result = convertCommandToAction(\"MoveForward\");\n\n    // What should my expect be?\n    expect(result);\n  });\n});\n</code></pre> <p>One approach that I have used in the past is jest's ability to test if a function is a mocked function, however, that approach doesn't work here because all of the functions are being mocked out. Meaning, that my test would pass, but if I returned <code>moveBackward</code> instead of <code>moveForward</code>, my test would still pass (but now for the wrong reason). I need a way to know which function was being returned.</p> <p>Doing some digging, I found that the <code>jest.fn()</code> has a way of setting a name for a mock by leveraging the <code>mockName</code> function. This in turn allowed me to change my setup to look like this.</p> <p><pre><code>jest.mock(\"./actions\", () =&gt; ({\n  moveForward: jest.fn().mockName('moveForward'),\n  moveBackward: jest.fn().mockName('moveBackward'),\n  turnLeft: jest.fn().mockName('turnLeft'),\n  turnRight: jest.fn().mockName('turnRight'),\n  quit: jest.fn().mockName('quit'),\n}));\n</code></pre> Note: It turns out that the <code>mockName</code> function is part of a fluent interface, which allows it to return a jest.Mock as the result of the mockName call</p> <p>With my setup updated, my tests can now check that the result has the right mockName.</p> <pre><code>describe(\"When converting a Command to an Action\", () =&gt; {\n  it(\"and the command is MoveForward, then the right action is returned\", () =&gt; {\n\n    // have to convert result as a jest.Mock to make TypeScript happy\n    const result = convertCommandToAction(\"MoveForward\") as unknown as Jest.Mock;\n\n    expect(result.getMockName()).toBe(\"moveForward\");\n  });\n});\n</code></pre>"},{"location":"articles/2024/08/19/today-i-learned---leveraging-mock-names-with-jest/#wrapping-up","title":"Wrapping Up","text":"<p>If you find yourself writing functions that return other function (i.e., leveraging functional programming concepts), then you check out using <code>mockName</code> for keeping track of which functions are being returned.</p>"},{"location":"articles/2025/06/10/today-i-learned---javascript-private-fields-and-properties/","title":"Today I Learned - JavaScript Private Fields and Properties","text":"<p>One of my favorite times of year has started, intern season! I always enjoy getting a new group of people who are excited to learn something new and are naturally curious about everything!</p> <p>As such, one of the first coding katas we complete is Mars Rover as it's a great exercise to introduce union types, records, functions, and some of the basic array operators (map and reduce). It also provides a solid introduction to automated testing practices (Arrange/Act/Assert, naming conventions, test cases). Finally, you can solve it multiple ways and depending on the approach, lends itself to refactoring and cleaning up.</p> <p>Now my preferred approach to the kata is to leverage functional programming (FP) techniques, however, it wouldn't be correct to only show that approach, so I tackled it using more object-oriented (OO) instead.</p> <p>One of the things that we run into pretty quickly is that we're going to need a <code>Rover</code> class that will have the different methods for moving and turning. Since the <code>Rover</code> will need to keep track of its <code>X</code>, <code>Y</code>, and <code>Direction</code>, I ended up with the following:</p> <pre><code>type Direction = \"North\" | \"South\" | \"East\" | \"West\";\nclass Rover {\n  constructor(private x:number, private y:number, private direction:Direction){}\n  // moveForward, moveBackward, turnLeft, and turnRight definitions below...\n}\n</code></pre> <p>This approach works just fine as it allows the caller to give us a starting point, but they can't manipulate <code>X</code>, <code>Y</code>, or <code>Direction</code> directly, they have to use one of the methods (i.e., we have encapsulation).</p>"},{"location":"articles/2025/06/10/today-i-learned---javascript-private-fields-and-properties/#the-problem","title":"The Problem","text":"<p>However, we run into a slight problem once we get to the user interface. We would like to be able to display the Rover's location and direction, however, we don't have a way of accessing that data since we marked those as private.</p> <p>In other words, we can't do the following:</p> <pre><code>const rover = new Rover(0, 0, 'North');\nconsole.log(`Rover is at (${r.x}, ${r.y}) facing ${r.direction}`)\n</code></pre> <p>One way to fix this problem is to remove the <code>private</code> modifier and allow the values to be public, however, this would mean that the state of my object could be manipulated either through it's methods (i.e., <code>moveForward</code>) or through global access <code>rover.X = 100</code>. </p> <p>What I'd like to do instead is to have a way to get the value to the outside world, but not allow them to modify it.</p> <p>In languages like C#, we would leverage a public get/private set on properties, which would look something like this:</p> <pre><code>public class Rover \n{\n  public int X {get; private set;}\n  public int Y {get; private set;}\n  public Direction Direction {get; private set;}\n  public Rover (int x, int y, Direction direction)\n  {\n    X = x;\n    Y = y;\n    Direction = direction;\n  }\n}\n</code></pre> <p>Let's take a look at how we can build the same idea in TypeScript (and by extension, JavaScript)</p>"},{"location":"articles/2025/06/10/today-i-learned---javascript-private-fields-and-properties/#introducing-fields","title":"Introducing Fields","text":"<p>Introducing fields are simple enough, we just define them in the class like so:</p> <pre><code>class Rover {\n  private x:number;\n  private y:number;\n  private direction:Direction;\n}\n</code></pre> <p>However, if you're working in JavaScript, the <code>private</code> keyword doesn't exist. However, JavaScript still allows you to mark something as private by prefixing <code>#</code> to the name.</p> <pre><code>class Rover {\n  #x;\n  #y;\n  #direction;\n}\n</code></pre> <p>With these fields in place, we now update our constructor to explicitly set the values.</p> <p>In TypeScript <pre><code>constructor(x:number, y:number, direction:Direction){\n  this.x = x;\n  this.y = y;\n  this.direction = direction;\n}\n</code></pre></p> <p>In JavaScript <pre><code>constructor(x, y, direction){\n  this.#x = x;\n  this.#y = y;\n  this.#direction = direction;\n}\n</code></pre></p> <p>At this point, we can update the various methods (<code>moveForward</code>, <code>moveBackward</code>, <code>turnLeft</code>, <code>turnRight</code>) to use the fields.</p>"},{"location":"articles/2025/06/10/today-i-learned---javascript-private-fields-and-properties/#introducing-properties","title":"Introducing Properties","text":"<p>With out fields in use, we can now expose their values by defining the get (colloquially known as the getter) for the fields.</p> <p>In TypeScript</p> <pre><code>get x(): number {\n  return this.x;\n}\nget y(): number {\n  return this.y;\n}\nget direction(): Direction {\n  return this.direction;\n}\n</code></pre> <p>In JavaScript <pre><code>get x() {\n  return this.#x;\n}\nget y() {\n  return this.#y;\n}\nget direction() {\n  return this.#direction;\n}\n</code></pre></p> <p>With our properties in place, the following code will work now</p> <pre><code>const rover = new Rover(4, 2, 'North');\nconsole.log(`Rover is at (${rover.X}, ${rover.Y}) facing ${rover.Direction})`);\n// prints \"Rover is at (4, 2) facing North\n\n// But this doesn't work\nrover.X = 100; // can't access X\n</code></pre>"},{"location":"articles/2025/06/10/today-i-learned---javascript-private-fields-and-properties/#closing-thoughts","title":"Closing Thoughts","text":"<p>When working with data that requires different access levels, think about leveraging private fields and then providing access through public properties.</p>"},{"location":"articles/2014/12/11/today-i-learned-the-law-of-demeter/","title":"Today I Learned: The Law of Demeter","text":"<p>Don\u2019t pass in more information that you need. It sounds simple, but when working with messy legacy code, it\u2019s easy to forget.</p> <p>The impetus for this post came from a peer code review. During the review, I found this method:</p> <pre><code>public IStrategy GetStrategy(Project project, bool isAffected)\n{\n  var type = project.Type;\n  if (type == ProjectType.A &amp;&amp; isAffected)\n    return new ProjectAIsAffectedStrategy();\n  if (type == ProjectType.B)\n    return new ProjectBStrategy();\n  // Similar if statements\n}\n</code></pre> <p>At first glance, it looks pretty good. Logic was sound and it seemed to be returning a class implementing an interface similar to what we would expect for the Factory pattern. However, there\u2019s a slight problem, can you spot it?</p> <p>The issue is with the first parameter, Project. The method takes a Project, however, we\u2019re only really depending on the Project\u2019s Type property.</p> <pre><code>public IStrategy GetStrategy(Project project, bool isAffected)\n{\n  var type = project.Type;\n  if (type == ProjectType.A &amp;&amp; isAffected)\n    return new ProjectAIsAffectedStrategy();\n  if (type == ProjectType.B)\n    return new ProjectBStrategy();\n  // Similar if statements\n}\n</code></pre> <p>So why don\u2019t we get rid of the dependency on the Project and instead replace it with the dependency on the ProjectType instead?</p> <pre><code>public IStrategy GetStrategy(ProjectType type, bool isAffected)\n{\n  if (type == ProjectType.A &amp;amp;&amp;amp; isAffected)\n    return new ProjectAIsAffectedStrategy();\n  if (type == ProjectType.B)\n    return new ProjectBStrategy();\n  // Similar if statements\n}\n</code></pre> <p>Instinctual, I knew this was the right call, but I couldn\u2019t remember why I knew it was a good choice. After some digging, I remembered that this is a Law of Demeter violation, or better known as the Principle of Least Knowledge violation.</p> <p>In general, this principle states that a method should have the least amount of information it needs to do it\u2019s job. Other classic violations of this principle is when you use a class\u2019s internals internals. For example,</p> <pre><code>SomeClassA.SomePropertyB.WithSomeMethodC()\n</code></pre> <p>One of the reasons that I really like the Law of Demeter is that if you follow it, you create easier to test methods. Don\u2019t believe me? Which is easier to create, the Project class (which may have many dependencies that would need to be stubbed) or the ProjectType enum (which by definition has zero dependencies)?</p> <p>Another reason that following the Law of Demeter is good practice is that it forces your code to be explicit about what dependencies are required. For example, in the first implementation, the caller knew that the method needed a Project, but had no clue on how much of the Project it needed (does it need all of the properties set? Does it need further setup besides basic instantiation?). However, with the refactored version, now it\u2019s much clearer that the method has a looser dependency on not Project, but just the ProjectType.</p>"},{"location":"articles/2024/04/22/today-i-learned-libyear/","title":"Today I Learned: LibYear","text":"<p>When writing software, it's difficult (if not impossible) to write everything from scratch. We're either using a framework or various third-party libraries to make our code work.</p> <p>On one hand, this is a major win for the community because we're not all having to solve the same problem over and over again. Could you imagine having to implement your own authentication framework (on second thought...)</p> <p>However, this power comes at a cost. These libraries aren't free as in beer, but more like puppies. So if we're going to take in the library, then we need to make sure that our dependencies are up-to-date with the latest and greatest. There are new features, bug fixes, and security patches occurring all the time and the longer we let a library drift, the more painful it can be to upgrade.</p> <p>If the library is leveraging semantic versioning, then we can take a guess on the likelihood of a breaking change base on which number (Major.Minor.Maintenance) has changed.</p> <ul> <li>Major - We've made a breaking change that's not backward compatible. Your code may not work anymore.</li> <li>Minor - We've added added new features that you might be interested in or made other changes that  are backwards compatible.</li> <li>Maintenance - We've fixed some bugs, sorry about that!</li> </ul>"},{"location":"articles/2024/04/22/today-i-learned-libyear/#keeping-up-with-dependencies","title":"Keeping Up With Dependencies","text":"<p>For libraries that have known vulnerabilities, you can leverage tools like GitHub's Dependabot to auto-create pull requests that will upgrade those dependencies for you. Even though the tool might be \"noisy\", this is a great way to take an active role in keeping libraries up to date.</p> <p>However, this approach only works for vulnerabilities, what about libraries that are just out-of-date? There's a cost/benefit of upgrading where the longer you go between the upgrades, the riskier the upgrade will be.</p> <p>In the JavaScript world, we know that dependencies are listed in the <code>package.json</code> file with minimum versions and the <code>package-lock.json</code> file states the exact versions to use.</p>"},{"location":"articles/2024/04/22/today-i-learned-libyear/#using-libyear","title":"Using LibYear","text":"<p>I was working with one of my colleagues the other day and he referred me to a library called LibYear that will check your package.json and lock file to determine how much \"drift\" that you have between your dependencies.</p> <p>Under the hood, it's combining the <code>npm outdated</code> and <code>npm view &lt;package&gt;</code> commands to determine the drift.</p> <p>What I like about this tool is that you can use this as a part of a \"software health\" report for your codebase.</p> <p>As engineers, we get pressure to ship features and hit delivery dates, but it's our responsibility to make sure that our codebase is in good shape (however we defined that term). I think this library is a good way for us to capture a data point about software health which then allows the team to make a decision on whether we should update our libraries now (or defer).</p> <p>The nice thing about the <code>LibYear</code> package is that it lends itself to be ran in a pipeline and then you could take those results and post them somewhere. For example, maybe you could write your own automation bot that could post the stats in your Slack or Teams chat.</p> <p>It looks like there's already a GitHub Action for running this tool today, so you could start there as a basis.</p>"},{"location":"articles/2024/05/06/today-i-learned---effective-pairing-with-mobsh/","title":"Today I Learned - Effective Pairing with Mob.sh","text":"<p>As someone who enjoys leveraging technology and teaching, I'm always interested in ways to simplify the teaching process.</p> <p>For example, when I'm teaching someone a new skill, I follow the \"show one, do one, lead one\" approach and my tool of choice for the longest time was LiveShare by Microsoft.</p>"},{"location":"articles/2024/05/06/today-i-learned---effective-pairing-with-mobsh/#using-vs-liveshare","title":"Using VS LiveShare","text":"<p>I think this extension is pretty slick as it allows you to have multiple collaborators, the latency is quite low, and it's built into both Visual Studio Code (VS Code) and Visual Studio.</p>"},{"location":"articles/2024/05/06/today-i-learned---effective-pairing-with-mobsh/#drawbacks-to-liveshare","title":"Drawbacks to LiveShare","text":""},{"location":"articles/2024/05/06/today-i-learned---effective-pairing-with-mobsh/#editor-lock-in","title":"Editor Lock-In","text":"<p>First, participants have to be using Visual Studio or VS Code. Since there's support for VS Code, this isn't quite a blocker as it could be. However, let's say that I'm wanting to work with a team on a Java application. They're more likely to be using IntelliJ or Eclipse as their editor and I don't want someone to have to change their editor just to collaborate.</p>"},{"location":"articles/2024/05/06/today-i-learned---effective-pairing-with-mobsh/#security-concerns","title":"Security Concerns","text":"<p>Second, there are some security considerations to be aware of.</p> <p>Given the nature of LiveShare, collaborators either connect to your machine (peer-to-peer) or they go through a relay in Azure. Companies that are sensitive to where traffic is routed to won't allow the Azure relay option and given the issues with the URL creation (see next section), the peer-to-peer connection isn't much better.</p> <p>To start a session, LiveShare generates a URL that the owner would share with their collaborators. As of today, there's no way to limit who can access that link. The owner has some moderator tools to block people, but there's not a way to stop anyone from joining who doesn't have the right kind of email address for example.</p>"},{"location":"articles/2024/05/06/today-i-learned---effective-pairing-with-mobsh/#introducing-mobsh","title":"Introducing Mob.sh","text":"<p>While pairing with a colleague, he introduced me to an alternative tool, mob.sh</p> <p>At first, I was a bit skeptical of this tooling as I enjoyed the ease of use that I got with LiveShare. However, after a few sessions, I find that this tool solves the problems that I was using LiveShare for just as good, if not better.</p>"},{"location":"articles/2024/05/06/today-i-learned---effective-pairing-with-mobsh/#how-it-works","title":"How It Works","text":"<p>At a high level, mob.sh is a command line tool that is a wrapper around basic <code>git</code> commands.</p> <p>Because of this design choice, it doesn't matter what editor that a participant has, as long as the code under question is under <code>git</code> source control, the tooling works.</p> <p>Let's explore how a pair, Adam and Brittany, would use this tool for work.</p>"},{"location":"articles/2024/05/06/today-i-learned---effective-pairing-with-mobsh/#adam-and-brittany-start-pairing","title":"Adam and Brittany Start Pairing","text":"<p>Adam is looking to solve a logic issue in an AWS lambda could use Brittany's guidance since he's new to that domain.</p> <p>Adam creates a new feature branch, <code>fixing-logic-issue</code> and starts a new mobbing session.</p> <pre><code>git switch -c fixing-logic-issue\nmob start --create\n# --create is needed because fixing-logic issue is not on the server yet\n</code></pre> <p>Under the hood, <code>mob.sh</code> has created a new branch off of <code>fixing-logic-issue</code> called <code>mob/fixing-logic-issue</code>. While Adam is making changes, they're going to occur on the <code>mob/fixing-logic-issue</code>.</p> <p>Because the pair is working remotely, Adam shares his screen so that they're on the same page.</p> <p>While on this branch, Adam writes a failing unit test that exposes the logic issue that he's running into. From here he signals that Brittany is up by running <code>mob next</code></p> <pre><code>mob next\n</code></pre> <p>By running this command, <code>mob.sh</code> adds and commits all the changes made on this branch and pushes them up to the server. Once this command completes, it's Brittany's turn to lead.</p> <p>Once Brittany see's the <code>mob next</code> command complete, she checks out the <code>fixing-logic-issue</code> branch and picks up the next portion of the work by running <code>mob start</code></p> <pre><code>git pull # To get fixing-logic-issue branch\ngit switch fixing-logic-issue\nmob start\n</code></pre> <p>Because she was on the <code>fixing-logic-issue</code> branch, <code>mob.sh</code> was able to see that there was a <code>mob/fixing-logic-issue</code> branch already, so that branch is checked out.</p> <p>Based on the test, Brittany shows Adam where the failure is occurring and they write up a fix that passes the failing test.</p> <p>Though there are more changes to be done, Brittany has a meeting to attend, so she ends the session by running <code>mob done</code>, committing, and then pushing the changes.</p> <pre><code>mob done\ngit commit -m \"Fixed first logic bug\"\ngit push\n</code></pre> <p>By running <code>mob done</code> command, all the changes that were on the <code>mob/fixing-logic-issue</code> are applied to the <code>fixing-logic-issue</code> branch. From here, Brittany can commit the changes and push them back to the server.</p>"},{"location":"articles/2024/05/06/today-i-learned---effective-pairing-with-mobsh/#wrapping-up","title":"Wrapping Up","text":"<p>If you're looking to expand your pairing/mobbing toolkit, I recommend giving <code>mob.sh</code> a try. Not only is the initial investment small, but I find the tooling natural to pick up after a few tries and since it's a wrapper around Git, it reduces the amount of learning needed before getting started.</p>"},{"location":"articles/2025/03/19/today-i-learned-triggering-powerautomate-through-http-request/","title":"Today I Learned: Triggering PowerAutomate Through Http Request","text":"<p>If you're working within the Microsoft ecosystem, you might have ran across a tool called PowerAutomate, this is Microsoft's low-code solution for building out simple automation (similar to tools like If This Then Than (IFTTT)).</p> <p>A common use I've used with PowerAutomate is integrating with Teams to kick off a workflow when someone posts a message. For example, you could have a trigger listening for when someone posts in your support channel to auto-respond back with a canned message and tagging whoever your support person or support team is.</p> <p>The problem I've ran into with PowerAutomate is that it has a steep learning curve and isn't very intuitive for engineers to pick up. As such, we typically use PowerAutomate for simpler flows (or if we need to integrate with another Microsoft product like Teams or Planner).</p> <p>Recently, I was tackling a problem where we had an application that needed to post a message into a Teams channel. Back in the day, I would have use a webhook and published the message that way, however, Microsoft removed the ability to use Webhooks into Teams in late 2024. The main solution going forward is that you would have had to build a Teams application and install it for your tenant/channel. This can be a bit annoying if all you have is a simple workflow that you need to stand-up.</p> <p>This got me thinking, I know I can use PowerAutomate to send a message to Teams, but can I trigger the flow programmatically (at this point, I was familiar with timers and when something happened).</p> <p>Doing some digging, I found that PowerAutomate can trigger a workflow off of a HTTP Request. </p> <p>Jackpot! </p> <p>Even better, we can give the PowerAutomate flow a specific payload to expect as part of the request body, even better!</p> <p>Combining these concepts, I can create a flow like the following:</p> <p>First, let's set up a trigger that takes in a payload. In this case, the caller will give us the message content and a bit more info on which team and channel to post this message into.</p> <p></p> <p>After setting up the trigger, we can then add a step for posting a message in Teams. Instead of using the pre-populated options that we get from our connection, we can use the values from the trigger instead.</p> <p></p> <p>After saving, our entire flow looks like the following</p> <p></p> <p>From here, we can then use the URL that was created as part of the trigger and have our application invoke the flow.</p>"},{"location":"articles/2024/05/26/today-i-learned---primary-constructors/","title":"Today I Learned - Primary Constructors","text":"<p>I've recently found myself picking up C# again for a project and even though much of my knowledge applies, I recently found the following and it took me a minute to figure out what's up.</p> <p>Let's say that we're working on the Mars Rover kata and we've decided to model the Rover type as a class with three fields: x, y, and direction.</p> <p>My normal approach to this problem would have been the following:</p> <pre><code>public enum Direction\n{\n  North, South, East, West\n}\n\npublic class Rover\n{\n  private int _x;\n  private int _y;\n  private Direction _direction;\n\n  public Rover(int x, int y, Direction direction)\n  {\n    _x = x;\n    _y = y;\n    _direction = Direction;\n  }\n\n  public void Print()\n  {\n    Console.WriteLine($\"Rover is at ({_x}, {_y}) facing {_direction}\");\n  }\n}\n\n// Example usage\n\nvar rover = new Rover(10, 20, Direction.North);\nrover.Print(); // Rover is at (10, 20) facing North\n</code></pre> <p>However, in the code I was working with, I saw the <code>Rover</code> definition as this</p> <pre><code>// note the params at the class line here\npublic class Rover(int x, int y, Direction direction)\n{\n  public void Print()\n  {\n    Console.WriteLine($\"Rover is at ({x}, {y}) facing {direction}\");\n  }\n}\n\n// Example Usage\nvar rover = new Rover(10, 20, Direction.North);\nrover.Print(); // Rover is at (10, 20) facing North\n</code></pre> <p>At first, I thought this was similar to the record syntax for holding onto data</p> <pre><code>public record Rover(int X, int Y, Direction Direction);\n</code></pre> <p>And it turns out, that it is! This feature is known as a primary constructor and when used with classes, it gives you some flexibility on how you want to access those inputs.</p> <p>For example, in our second implementation of Rover, we're directly using <code>x</code>, <code>y</code>, and <code>direction</code> in  the <code>Print</code> method.</p> <p>However, let's say that we didn't want to use those properties directly (or if we need to set some state based on those inputs), then we could do the following.</p> <pre><code>public class Rover(int x, int y, Direction direction)\n{\n    private readonly bool _isFacingRightDirection = direction == Direction.North;\n    public void Print()\n    {\n        if (_isFacingRightDirection)\n        {\n            Console.WriteLine(\"Rover is facing the correct direction!\");\n        }\n        Console.WriteLine($\"Rover is at ({x}, {y}) facing {direction}\");\n    }\n}\n</code></pre> <p>After playing around this for a bit, I can see how this feature would be beneficial for classes that only store their constructor arguments for later usage.</p> <p>Even though Records accomplish that better, you can't attach functionality to Records, but you can with classes, so it does provide better organization from that front.</p> <p>That being said, I'm not 100% sure why we needed to add the primary constructor feature to the language as this now opens up multiple ways of setting up constructors. I'm all for giving developers choices, but this seems ripe for bike shedding where teams have to decide which approach to stick with.</p>"},{"location":"articles/2015/10/28/til--how-to-check-if-a-substitute-was-called-zero-times/","title":"TIL \u2013 How To Check If a Substitute Was Called Zero Times","text":""},{"location":"articles/2015/10/28/til--how-to-check-if-a-substitute-was-called-zero-times/#setup","title":"Setup","text":"<p>During this past week, I\u2019ve been working on a new feature and during development, I ended up with code that looked like this:</p> <pre><code>public class PermissionChecker\n{\n  public PermissionChecker(IModuleDisabler moduleDisabler, User user)\n  {\n      if (user.IsAdmin) return;\n      else if (user.HasFullRights) ConfigureFullRights(moduleDisabler);\n      else if (user.HasPartialRights) ConfigurePartialRights(moduleDisabler);\n  }\n\n  private void ConfigureFullRights(IModuleDisabler disabler)\n  {\n      disabler.DisableSystemAdminModule();\n  }\n\n  private void ConfigurePartialRights(IModuleDisabler disabler)\n  {\n      disabler.DisableSystemAdminModule();\n      disabler.DisableReportModule();\n      disabler.DisableUserManagementModule();\n  }\n}\n</code></pre> <p>So the code is pretty straight forward, I have a PermissionChecker whose job is to use the IModuleDisabler to turn off certain modules depending upon the user permissions. Pretty straightforward implementation.</p> <p>Now that the solution is fleshed out, it\u2019s time to write some tests around this. When it comes to testing classes that have dependencies on non-trivial classes, I use NSubstitute, a mocking tool, to create mock versions of those dependencies. In this case, NSubstitute allows me to test how the IModuleDisabler is being used by the PermissionsChecker.</p> <p>For example, let\u2019s say that I wanted to test how the PermissionChecker interacts with the IModuleDisabler when the user has a partial access, I\u2019d write a test that looks like the following:</p> <pre><code>[Test]\npublic void And_the_user_has_partial_access_then_the_disabler_disables_the_report_module()\n{\n     // Arrange\n     var permissionChecker = new PermissionChecker();\n     var mockDisabler = Substitute.For();\n     var user = new User {HasPartialAccess = true};\n\n     // Act\n     permissionChecker.CheckPermissions(mockDisabler, user);\n\n     // Assert\n     mockDisabler.Received(1).DisableReportModule();\n}\n</code></pre> <p>In the above test, our assertion step is to check if the mockDisabler received a single call to the DisableReportModule. If it didn\u2019t receive a call, then the test fails. We can write similar tests for the different modules that should be disabled for the partial rights permission and follow a similar pattern for the full rights permission.</p> <p>However, things get a bit more interesting when we\u2019re testing what happens if the user is an admin. If we follow the same pattern, we\u2019d end up with a test that looks like this:</p> <pre><code>[Test]\npublic void And_the_user_has_admin_permissions_then_the_disabler_is_not_used()\n{\n     // Arrange\n     var permissionChecker = new PermissionChecker();\n     var mockDisabler = Substitute.For();\n     var user = new User {IsAdmin = true};\n\n     // Act\n     permissionChecker.CheckPermissions(mockDisabler, user);\n\n     // Assert\n     mockDisabler.DidNotReceive().DisableSystemAdminModule();\n     mockDisabler.DidNotReceive().DisableReportModule();\n     mockDisabler.DidNotReceive().DisableUserManagementModule();\n}\n</code></pre> <p>This solution works for now, however, there is a major maintainability issue, can you spot it?</p>"},{"location":"articles/2015/10/28/til--how-to-check-if-a-substitute-was-called-zero-times/#problem","title":"Problem","text":"<p>The issue arises when we add a new module to be disabled which forces the IModuleDisabler to implement a new method. In that case, you need to remember to update this test to also check that the new method wasn\u2019t being called. If you forget, this test would still pass, but it\u2019d pass for the wrong reason.</p> <p>To help illustrate, let\u2019s say that another method, DisableImportModule, has been added to the IModuleDisabler interface. In addition, we also need to make sure that this is called when users have partial access, but should not be called for users who are admins or users who have full access.</p> <p>To fulfill those requirements, we modify the PermissionChecker as so:</p> <pre><code>public class PermissionChecker\n{\n  public PermissionChecker(IModuleDisabler moduleDisabler, User user)\n  {\n      if (user.IsAdmin) return;\n      else if (user.HasFullRights) ConfigureFullRights(moduleDisabler);\n      else if (user.HasPartialRights) ConfigurePartialRights(moduleDisabler);\n  }\n\n  private void ConfigureFullRights(IModuleDisabler disabler)\n  {\n      disabler.DisableSystemAdminModule();\n  }\n\n  private void ConfigurePartialRights(IModuleDisabler disabler)\n  {\n      disabler.DisableSystemAdminModule();\n      disabler.DisableReportModule();\n      disabler.DisableUserManagementModule();\n      disabler.DisableImportModule();\n  }\n}\n</code></pre> <p>At this point, we\u2019d write another test for when the a user has partial access, the import module should be disabled. However, it\u2019s very unlikely that we\u2019d remember to update the test for the admin. Remember, for the admin, we\u2019re checking that it received no calls to any disable methods and the way we\u2019re doing that is by checking each method individually.</p> <pre><code>[Test]\npublic void And_the_user_has_admin_permissions_then_the_disabler_is_not_used()\n{\n  // Arrange\n  var permissionChecker = new PermissionChecker();\n  var mockDisabler = Substitute.For();\n  var user = new User {IsAdmin = true};\n\n  // Act\n  permissionChecker.CheckPermissions(mockDisabler, user);\n\n  // Assert\n  mockDisabler.DidNotReceive().DisableSystemAdminModule();\n  mockDisabler.DidNotReceive().DisableReportModule();\n  mockDisabler.DidNotReceive().DisableUserManagementModule();\n  // Need to add check for DidNotReceive().DisableImportModule();\n}\n</code></pre>"},{"location":"articles/2015/10/28/til--how-to-check-if-a-substitute-was-called-zero-times/#solution","title":"Solution","text":"<p>There\u2019s got to be a better way. After some digging around, I found that any NSubstitute mock, has a ReceivedCalls method that returns all calls that the mock received. With this new knowledge, we can refactor the previous test with the following:</p> <pre><code>[Test]\npublic void And_the_user_has_admin_permissions_then_the_disabler_is_not_used()\n{\n  // Arrange\n  var permissionChecker = new PermissionChecker();\n  var mockDisabler = Substitute.For();\n  var user = new User {IsAdmin = true};\n\n  // Act\n  permissionChecker.CheckPermissions(mockDisabler, user);\n\n  // Assert\n  CollectionAssert.IsEmpty(mockDisabler.ReceivedCalls());\n}\n</code></pre> <p>This solution is much better because if we add more modules, this test is still checking to make sure that admin users do not have any modules disabled.</p>"},{"location":"articles/2015/10/28/til--how-to-check-if-a-substitute-was-called-zero-times/#summary","title":"Summary","text":"<p>When using a NSubstitute mock and you need to make sure that it received no calls to any methods or properties, you can using NSubstitute\u2019s ReceivedCalls in conjunction with CollectionAssert.IsEmpty to ensure that the substitute was not called.</p>"},{"location":"articles/2024/05/10/today-i-learned-destructure-objects-in-function-signatures/","title":"Today I Learned: Destructure Objects in Function Signatures","text":"<p>When modeling types, one thing to keep in mind is to not leverage primitive types for your domain. This comes up when we use a primitive type (like a string) to represent core domain concepts (like a Social Security Number or a Phone Number).</p> <p>Here's an example where it can become problematic:</p> <pre><code>// Definition for Customer\ntype Customer = {\n  firstName: string,\n  lastName: string,\n  email: string,\n  phoneNumber: string\n}\n\n// Function to send an email to customer about a new sale\nasync function sendEmailToCustomer(c:Customer): Promise&lt;void&gt; {\n  const content = \"Look at these deals!\";\n\n  // Uh oh, we're trying to send an email to a phone number...\n  await sendEmail(c.phoneNumber, content);\n}\n\nasync function sendEmail(email:string, content:string): Promise&lt;void&gt; {\n  // logic to send email\n}\n</code></pre> <p>There's a bug in this code, where we're trying to send an email to a phone number. Unfortunately, this code type checks and compiles, so we have to lean on other techniques (automated testing or code reviews) to discover the bug.</p> <p>Since it's better to find issues earlier in the process, we can make this a compilation error by introducing a new type for <code>Email</code> since not all strings should be treated equally.</p> <p>One approach we can do is to create a tagged union like the following:</p> <pre><code>type Email = {\n  label:\"Email\",\n  value:string\n}\n</code></pre> <p>With this in place, we can change our <code>sendEmail</code> function to leverage the new <code>Email</code> type.</p> <pre><code>function sendEmail(email:Email, content:string): Promise&lt;void&gt; {\n  // logic to send email\n}\n</code></pre> <p>Now, when we get a compilation error when we try passing in a phoneNumber.</p> <p></p> <p>One downside to this approach is that if you want to get the value from the <code>Email</code> type, you need to access it's <code>value</code> property. This can be a bit hard to read and keep track of.</p> <pre><code>function sendEmail(email:Email, content:string): Promise&lt;void&gt; {\n  const address = email.value;\n  // logic to send email\n}\n</code></pre>"},{"location":"articles/2024/05/10/today-i-learned-destructure-objects-in-function-signatures/#leveraging-object-destructuring-in-functions","title":"Leveraging Object Destructuring in Functions","text":"<p>One technique to avoid this is to use destructuring to get the individual properties. This allows us to \"throw away\" some properties and hold onto the ones we care about. For example, let's say that we wanted only the <code>phoneNumber</code> from a <code>Customer</code>. We could get that with an assignment like the following:</p> <pre><code>const customer: Customer = {\n  firstName: \"Cameron\",\n  lastName: \"Presley\",\n  phoneNumber: \"555-5555\",\n  email: {label:\"Email\", value:\"Cameron@domain.com\"}\n}\n\nconst {phoneNumber} = customer; // phoneNumber will be \"555-555\"\n</code></pre> <p>This works fine for assignments, but it'd be nice to have this at a function level. Thankfully, we can do that like so:</p> <pre><code>// value is the property from Email, we don't have the label to deal with\nfunction sendEmail({value}:Email, content:string): Promise&lt;void&gt; {\n  const address = value; // note that we don't have to do .value here\n  // logic to send email\n}\n</code></pre> <p>If you find yourself using domain types like this, then this is a handy tool to have in your toolbox.</p>"},{"location":"articles/2025/05/13/leveraging-tuples-in-typescript/","title":"Leveraging Tuples in TypeScript","text":"<p>In preparation for StirTrek, I'm revisiting my approach for how to implement the game of Blackjack. I find card games to be a great introduction to functional concepts as you hit the major concepts quickly and the use cases are intuitive.</p> <p>Let's take a look at one of the concepts in the game, <code>Points</code>.</p> <p>Blackjack is played with a standard deck of cards (13 Ranks and 4 Suits) where the goal is to get the closest to 21 points without going over. A card is worth Points based on its Rank. So let's go ahead and model what we know so far.</p> <pre><code>type Rank = \"Ace\" | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | \"Jack\" | \"Queen\" | \"King\"\ntype Suit = \"Hearts\" | \"Clubs\" | \"Spades\" | \"Diamonds\"\ntype Card = {readonly rank:Rank, readonly suit:Suit}\n</code></pre> <p>We know that a <code>Card</code> is worth points based on its rank, the rule are:</p> <ul> <li>Cards with a Rank of 2 through 10 are worth that many points (i.e., 2's are worth 2 points, 3's are worth 3 points, ..., 10's are worth 10 points)</li> <li>Cards with a Rank of Jack, Queen, or King are worth 10 points</li> <li>Cards with a Rank of Ace can be worth either 1 or 11 points (depending on which one is the most advantageous)</li> </ul> <p>Let's explore the Ace in more detail.</p> <p>For example, if we had a hand consisting of an <code>Ace</code> and a <code>King</code>, then it could be worth either 11 (treating the Ace as a 1) or as 21 (treating the Ace as an 11). In this case, we'd want to treat the Ace as an 11 as that gives us 21 exactly (specifically, a Blackjack).</p> <p>In another example, if we had a hand consisting of an <code>Ace</code>, <code>6</code>, and <code>Jack</code>, then it could either be worth 17 (treating the Ace as a 1) or 27 (treating the Ace as an 11). Since 27 is greater than 21 (which would cause us to bust), we wouldn't want the Ace to be worth 11.</p>"},{"location":"articles/2025/05/13/leveraging-tuples-in-typescript/#creating-cardtopoints","title":"Creating cardToPoints","text":"<p>Now that we have this detail, let's take a look at trying to write the <code>cardToPoints</code> function.</p> <pre><code>function cardToPoints(c:Card): Points { // Note we don't know what the type of this is yet\n  switch(c.rank) {\n    case 'Ace': return ???\n    case 'King': return 10;\n    case 'Queen': return 10;\n    case 'Jack': return 10;\n    default:\n      return c.rank; // we can do this because TypeScript knows all the remaining options for Rank are numbers\n  }\n}\n</code></pre> <p>At this point, we don't know how to score <code>Ace</code> because we would need to know the other cards to get points for. Since we don't have that context here, why not capture both values?</p> <pre><code>function cardToPoints(c:Card): Points { // Note we don't know what the type of this is yet\n  switch(c.rank) {\n    case 'Ace': return [1,11];\n    case 'King': return 10;\n    case 'Queen': return 10;\n    case 'Jack': return 10;\n    default:\n      return c.rank; // we can do this because TypeScript knows all the remaining options for Rank are numbers\n  }\n}\n</code></pre> <p>In TypeScript, we can denote a tuple by using <code>[]</code>. Going forward, TypeScript knows that it's a two element array and guarantees that we can index using 0 or 1.</p> <p>This works, however, anything using <code>cardToPoints</code> has to deal with that it could either be a number or a tuple.</p> <p>When I come across cases like this, I reach for setting up a sum type to model each case.</p> <pre><code>type Hard = {tag:'hard', value:number};\ntype Soft = {tag:'soft', value:[number,number]}; // note that value here is a tuple of number*number\ntype Points = Hard | Soft\n</code></pre> <p>Now, when I call <code>cardToPoints</code>, I can use the <code>tag</code> field to know whether I'm working with a number or a tuple.</p>"},{"location":"articles/2025/05/13/leveraging-tuples-in-typescript/#adding-points-together","title":"Adding Points Together","text":"<p>A common workflow in Blackjack is to figure out how many points someone has. At a high level, we'd want to do the following</p> <ul> <li>Convert each Card to Points</li> <li>Add all the Points together</li> </ul> <p>Summing things together is a common enough pattern, so we know our code is going to look something like this:</p> <pre><code>function handToPoints(cards:Card[]): Points {\n  return cards.map((c)=&gt;cardToPoints(c)).reduce(SOME_FUNCTION_HERE, SOME_INITIAL_VALUE_HERE);\n}\n</code></pre> <p>We don't have the reducer function defined yet, but we do know that it's a function that'll take two <code>Points</code> and return a <code>Points</code>. So let's stub that out.</p> <pre><code>function addPoints(a:Points, b:Points): Points {\n  // implementation\n}\n</code></pre> <p>Since we modeled <code>Points</code> as a sum type, we can use the <code>tag</code> field to go over the possible cases</p> <pre><code>function addPoints(a:Points, b:Points): Points {\n  if (a.tag === 'hard' &amp;&amp; b.tag === 'hard') {\n    // logic\n  }\n  if (a.tag === 'hard' &amp;&amp; b.tag === 'soft'){\n    // logic\n  }\n  if (a.tag === 'soft' &amp;&amp; b.tag === 'hard'){\n    // logic\n  } \n  // last case is both of them are soft\n}\n</code></pre> <p>With this skeleton in place, let's start implementing each of the branches</p>"},{"location":"articles/2025/05/13/leveraging-tuples-in-typescript/#adding-two-hard-values","title":"Adding Two Hard Values","text":"<p>The first case is the easiest, if we have two hard values, then we add their values together. So a King and 7 us a 17 for example. </p> <pre><code>function addHardAndHard(a:Hard, b:Hard): Points { // note that I'm defining a and b as Hard and not just Points\n  const value = a.value + b.value;\n  return {tag:'hard', value};\n}\n</code></pre> <p>With this function defined, we can update <code>addPoints</code> like so</p> <pre><code>function addPoints(a:Points, b:Points): Points {\n  if (a.tag === 'hard' &amp;&amp; b.tag === 'hard'){\n    return addHardAndHard(a,b);\n  }\n  // other branches\n}\n</code></pre>"},{"location":"articles/2025/05/13/leveraging-tuples-in-typescript/#adding-hard-and-soft","title":"Adding Hard and Soft","text":"<p>The next two cases are the same, where we're adding a Hard value to a Soft value. For example, we're adding a 6 to an Ace. We can't assume that the answer is 7 since that might not be what the player wants. We also can't assume that the value is 17 because that might not be to the players advantage, which means that we need to keep track of both options, which implies that the result would be a <code>Soft</code> value. Let's go ahead and write that logic out</p> <pre><code>function addHardAndSoft(a:Hard, b:Soft): Points { // note that a is typed to be Hard and b is typed as Soft\n  const [bLow, bHigh] = b.value; // destructuring the tuple into specific pieces\n  return {tag:'soft', value:[a.value+bLow, a.value+bHigh]};\n}\n</code></pre> <p>With this function in place, we can write out the next two branches</p> <pre><code>function addPoints(a:Points, b:Points): Points {\n  if (a.tag === 'hard' &amp;&amp; b.tag === 'hard'){\n    return addHardAndHard(a, b);\n  }\n  if (a.tag === 'hard' &amp;&amp; b.tag === 'soft'){\n    return addHardAndSoft(a, b);\n  }\n  if (a.tag === 'soft' &amp;&amp; b.tag === 'hard'){\n    return addHardAndSoft(b, a); \n  }\n  // remaining logic\n}\n</code></pre>"},{"location":"articles/2025/05/13/leveraging-tuples-in-typescript/#adding-soft-and-soft","title":"Adding Soft and Soft","text":"<p>The last case we need to handle is when both <code>Points</code> are <code>Soft</code>. If we were to break this down, we have four values (aLow, aHIgh for a, and bLow,bHigh for b) we need to keep track of:</p> <ol> <li>aLow + bLow</li> <li>aHigh + bLow</li> <li>aLow + bHigh</li> <li>aHigh + bHigh</li> </ol> <p>However, let's play around with this by assuming that <code>Points</code> in question are both Ace. We would get the following:</p> <ol> <li>aLow + bLow = 1 + 1 = 2</li> <li>aHigh + bLow = 11 + 1 = 12</li> <li>aLow + bHigh = 1 + 11 = 12</li> <li>aHigh + bHigh = 11 + 11 = 22</li> </ol> <p>Right off the bat, we can discard the case 4, (aHigh + bHigh), because there is no situation where the player would want that score as they would bust.</p> <p>For cases 2 and 3, they yield the same value, so they're essentially the same case.</p> <p>Which means, that our real cases are</p> <ol> <li>aLow + bLow</li> <li>aHigh + bLow (which is the same as aLow + bHigh)</li> </ol> <p>So let's go ahead and write that function</p> <pre><code>function addSoftAndSoft(a:Soft, b:Soft): Points {\n  const [aLow, aHigh] = a.value;\n  const [bLow] = b.value; // note that we're only grabbing the first element of the tuple here\n  return {tag:'soft', value:[aLow+bLow, aHigh+bLow]};\n}\n</code></pre> <p>Which gives us the following for <code>addPoints</code></p> <pre><code>function addPoints(a:Points, b:Points): Points {\n  if (a.tag === 'hard' &amp;&amp; b.tag === 'hard'){\n    return addHardAndHard(a, b);\n  }\n  if (a.tag === 'hard' &amp;&amp; b.tag === 'soft'){\n    return addHardAndSoft(a, b);\n  }\n  if (a.tag === 'soft' &amp;&amp; b.tag === 'hard'){\n    return addHardAndSoft(b, a);\n  }\n  return addSoftAndSoft(a as Soft, b as Soft);\n}\n</code></pre> <p>Now that we have <code>addPoints</code>, let's revisit <code>handToPoints</code></p> <pre><code>// Original Implementation\n// function handToPoints(cards:Card[]): Points {\n//   return cards.map((c)=&gt;cardToPoints(c)).reduce(SOME_FUNCTION_HERE, SOME_INITIAL_VALUE_HERE;\n// }\n\nfunction handToPoints(cards:Card[]): Points {\n  return cards.map((c)=&gt;cardToPoints(c)).reduce(addPoints, SOME_INITIAL_VALUE_HERE);\n}\n</code></pre> <p>Now we need to figure out what SOME_INITIAL_VALUE_HERE would be. When working with <code>reduce</code>, a good initial value would be what would we return if we had no cards in the hand? Well, they would have 0 points, right? We can use 0, but we can't just return 0 since our function returns <code>Points</code>, so we need to go from 0 to Points. Easy enough, we can use <code>Hard</code> to accomplish this.</p> <pre><code>function handToPoints(cards:Card[]): Points {\n  const initialValue:Points = {tag:'hard', value:0};\n  return cards.map((c)=&gt;cardToPoints(c)).reduce(addPoints, initialValue);\n}\n\nconst hand = [{rank:'Ace', suit:'Hearts'}, {rank:7, suit:'Clubs'}]\nconsole.log(handToPoints(hand)); // {tag:'soft', value:[8, 18]};\n</code></pre> <p>For those who know a bit of category theory, you might notice that <code>addPoints</code> is the operation and <code>Hard 0</code> is the identity for a monoid over <code>Points</code>.</p>"},{"location":"articles/2025/05/13/leveraging-tuples-in-typescript/#one-last-improvement","title":"One Last Improvement","text":"<p>So this code works and everything is fine, however, we can make one more improvement to <code>addPoints</code>. Let's take a look at what happens when we try to get the <code>Points</code> for the following:</p> <pre><code>const hand: Card[] = [\n  {rank:'Ace', suit:'Diamonds'},\n  {rank:8, suit:'Hearts'},\n  {rank:4, suit:'Clubs'},\n  {rank:8, suit:'Spades'}\n]\n\nconsole.log(handToPoints(hand)); // {tag:'soft', value:[21, 31]};\n</code></pre> <p>Huh, we got the right value, but we know that for <code>Soft</code>, it doesn't make sense to allow the player a choice between 21 and 31 because 31 is always invalid. Even though the answer isn't wrong per se, it does allow the user to do the wrong thing later on, which isn't the greatest.</p> <p>Let's add one more function, <code>normalize</code> that will check to see if the <code>Points</code> is <code>Soft</code> with a value over 21. If so, we convert to a <code>Hard</code> and throw out the value over 21. Otherwise we return the value (since it's possible for someone to get a <code>Hard</code> score over 21).</p> <pre><code>function normalize(p:Points): Points {\n  if (p.tag === 'soft' &amp;&amp; p.value[1] &gt; 21){\n    return {tag:'hard', value:p.value[0]}\n  }\n  return p;\n}\n\n// updated addPoints with normalize being used\nfunction addPoints(a:Points, b:Points): Points {\n  if (a.tag === 'hard' &amp;&amp; b.tag === 'hard'){\n    return normalize(addHardAndHard(a, b));\n  }\n  if (a.tag === 'hard' &amp;&amp; b.tag === 'soft'){\n    return normalize(addHardAndSoft(a, b));\n  }\n  if (a.tag === 'soft' &amp;&amp; b.tag === 'hard'){\n    return normalize(addHardAndSoft(b, a));\n  }\n  return normalize(addSoftAndSoft(a as Soft, b as Soft));\n}\n\n// Note: There's some minor refactoring that we could do here (for example, creating an internal function for handling the add logic and updating `addPoints` to use that function with normalize),\n// but will leave that as an exercise to the reader :)\n</code></pre>"},{"location":"articles/2025/05/13/leveraging-tuples-in-typescript/#wrapping-up","title":"Wrapping Up","text":"<p>In this post, we took a look at using tuples in TypeScript by tackling a portion of the game of Blackjack. Whether it's through using it in types (like we did for <code>Soft</code>) or for destructuring values (like we did in the various <code>addX</code> functions), they can be a handy way of grouping data together for short-term operations.</p>"},{"location":"articles/2025/05/13/leveraging-tuples-in-typescript/#interested-in-knowing-more","title":"Interested in knowing more?","text":"<p>If you've enjoyed the above, then you might be interested in my new course (launching Summer 2025) where we build out the game of Blackjack using these concepts in TypeScript. Click here if you're interested in getting an update for when the course goes live!</p>"},{"location":"articles/2024/06/13/today-i-learned-validating-data-in-zod/","title":"Today I Learned: Validating Data in Zod","text":"<p>Validating input. You've got to do it, otherwise, you're going to be processing garbage, and that never goes well, right?</p> <p>Whether it's through the front-end (via a form) or through the back-end (via an API call), it's important to make sure that the data we're processing is valid.</p> <p>Coming from a C# background, I was used to ASP.NET Web Api's ability to create a class and then use the <code>FromBody attribute</code> for the appropriate route to ensure the data is good. By using this approach, ASP.NET will reject requests automatically that don't fit the data contract.</p> <p>However, picking up JavaScript and TypeScript, that's not the case. At first, this surprised me because I figured that this would automatically happen when using libraries like Express or Nest.js. Thinking more about it, though, it shouldn't have surprised me. ASP.NET can catch those issues because it's a statically typed/ran language. JavaScript isn't and since TypeScript types are removed during the compilation phase, neither is statically typed at runtime.</p> <p>When writing validations, I find zod to be a delightful library to leverage. There are a ton of useful built-in options, you can create your own validators (which you can then compose!) and you can infer models based off of your validations.</p>"},{"location":"articles/2024/06/13/today-i-learned-validating-data-in-zod/#building-the-amazin-bookstore","title":"Building the Amazin' Bookstore","text":"<p>To demonstrate some of the cool things that you can do with Zod, let's pretend that we're building out a new POST endpoint for creating a new book. After talking to the business, we determine that the payload for a new book should look like this:</p> <pre><code>// A valid book will have the following\n// - A non-empty title\n// - A numeric price (can't be negative or zero)\n// - A genre from a list of possibilities (mystery, fantasy, history are examples, platypus would not be valid)\n// - An ISBN which must be in a particular format\n// - A valid author which must have a first name, a last name, and an optional middle name\n</code></pre>"},{"location":"articles/2024/06/13/today-i-learned-validating-data-in-zod/#whats-in-a-name","title":"What's in a Name?","text":"<p>Since the <code>Book</code> type needs a valid <code>Author</code>, let's build that out first:</p> <pre><code>import {z} from \"zod\";\n\nexport const AuthorSchema = z.object({\n\n});\n</code></pre> <p>Since <code>Author</code> will need to be an object, we'll use <code>z.object</code> to signify that. Right off the bat, this prevents a string, number, or other primitive types from being accepted.</p> <pre><code>AuthorSchema.safeParse(\"someString\"); // will result in a failure\nAuthorSchema.safeParse(42); // will result in a failure\nAuthorSchema.safeParse({}); // will result in success!\n</code></pre> <p>This is a great start, but we know that Author has some required properties (like a first name), so let's implement that by using <code>z.string()</code></p> <pre><code>export const AuthorSchema = z.object({\n    firstName: z.string()\n});\n</code></pre> <p>With this change, let's take a look at our schema validation</p> <pre><code>AuthorSchema.safeParse({}); // fails because no firstName property\nAuthorSchema.safeParse({firstName:42}); // fails because firstName is not a string\nAuthorSchema.safeParse({firstName: \"Cameron\"}); // succeeds because firstName is present and a string\n</code></pre> <p>However, there's one problem with our validation. We would allow an empty <code>firstName</code></p> <pre><code>AuthorSchema.safeParse({firstName:\"\"}); // succeeds, but should have failed :(\n</code></pre> <p>To make our validation stronger, we can update our <code>firstName</code> property to have a minimum length of 1 like so.</p> <pre><code>export const AuthorSchema = z.object({\n    firstName: z.string().min(1)\n});\n</code></pre> <p>Finally, we have a way to enforce that an author has a non-empty firstName!. Looking at the requirements, it seems like <code>lastName</code> is going to be similar, so let's update our <code>AuthorSchema</code> to include lastName.</p> <pre><code>export const AuthorSchema = z.object({\n    firstName: z.string().min(1),\n    lastName: z.string().min(1)\n});\n</code></pre> <p>Hmmm, it looks like we have the same concept in multiple places, the idea of a non empty string. Let's refactor that to its own schema.</p> <pre><code>export const NonEmptyStringSchema = z.string().min(1);\n\nexport const AuthorSchema = z.object({\n    firstName: NonEmptyStringSchema,\n    lastName: NonEmptyStringSchema\n});\n</code></pre> <p>Nice! We're almost done with <code>Author</code>, we need to implement <code>middleName</code>. Unlike the other properties, an author may not have a middle name. In this case, we're going to leverage the <code>optional</code> function from zod to signify that as so.</p> <pre><code>export const NonEmptyStringSchema = z.string().min(1);\n\nexport const AuthorSchema = z.object({\n    firstName: NonEmptyStringSchema,\n    lastName: NonEmptyStringSchema,\n    // This would read that middleName may or not may be present. \n    // If it is, then it must be a string (could be empty)\n    middleName: z.string().optional(), \n});\n</code></pre> <p>With the implementation of <code>AuthorSchema</code>, we can start working on the <code>BookSchema</code>.</p>"},{"location":"articles/2024/06/13/today-i-learned-validating-data-in-zod/#judging-a-book-by-its-cover","title":"Judging a Book By It's Cover","text":"<p>Since we have <code>AuthorSchema</code>, we can use that as our start as so:</p> <pre><code>export const BookSchema = z.object({\n    author: AuthorSchema\n});\n</code></pre> <p>We know that a book must have a non-empty title, so let's add that to our definition. Since it's a string that must have at least one character, we can reuse the <code>NonEmptyStringSchema</code> definition from before.</p> <pre><code>export const BookSchema = z.object({\n    author: AuthorSchema,\n    title: NonEmptyStringSchema\n});\n</code></pre>"},{"location":"articles/2024/06/13/today-i-learned-validating-data-in-zod/#putting-a-price-on-knowledge","title":"Putting a Price on Knowledge","text":"<p>With title in place, let's leave the string theory alone for a bit and look at numbers. In order for the bookstore to function, we've got sell books for some price. Let's use <code>z.number()</code> and add a <code>price</code> property.</p> <pre><code>export const BookSchema = z.object({\n    author: AuthorSchema,\n    title: NonEmptyStringSchema,\n    price: z.number()\n});\n</code></pre> <p>This works, however, <code>z.number()</code> will accept any number, which includes numbers like <code>0</code> and <code>-5</code>. While those values would be great for the customer, we can't run our business that way. So let's update our price to only include positive numbers, which can be accomplished by leveraging the <code>positive</code> function.</p> <pre><code>export const BookSchema = z.object({\n    author: AuthorSchema,\n    title: NonEmptyStringSchema,\n    price: z.number().positive()\n});\n</code></pre> <p>With price done, let's look at validating the genre.</p>"},{"location":"articles/2024/06/13/today-i-learned-validating-data-in-zod/#would-you-say-its-a-mystery-or-history","title":"Would You Say It's a Mystery or History?","text":"<p>Up to this point, all of our properties have been straightforward (simple strings and numbers). However, with <code>genre</code>, things get more complicated because it can only be one of a particular set of values. Thankfully, we can define a <code>GenreSchema</code> by using <code>z.enum()</code> like so. </p> <pre><code>export const GenreSchema = z.enum([\"Fantasy\", \"History\", \"Mystery\"]);\n</code></pre> <p>With this definition, a valid genre can only be fantasy, history, or mystery. Let's update our book definition to use this new schema.</p> <pre><code>export const BookSchema = z.object({\n    author: AuthorSchema,\n    title: NonEmptyStringSchema,\n    price: z.number().positive(),\n    genre: GenreSchema\n});\n</code></pre> <p>Now, someone can't POST a book with a genre of \"platypus\" (though I'd enjoy reading such a book).</p>"},{"location":"articles/2024/06/13/today-i-learned-validating-data-in-zod/#id-please","title":"ID Please","text":"<p>Last, let's take a look at implementing the <code>isbn</code> property. This is interesting because ISBNs can be in one of two shapes: ISBN-10 (for books pre-2007) and ISBN-13 (all other books).</p> <p>To make this problem easier, let's focus on the ISBN-10 format for now. A valid value will be in the form of <code>#-###-#####-#</code> (where # is a number). Now, you can take this a whole lot further, but we'll keep on the format.</p> <p>Now, even though <code>zod</code> has built-in validators for emails, ips, and urls, there's not a built-in one for ISBNs. In these cases, we can use <code>.refine</code> to add our logic. But this is a good use case for a basic regular expression. Using regex101 as a guide, we end up with the following expression and schema for the ISBN.</p> <pre><code>const isbn10Regex = /^\\d-\\d{3}-\\d{5}-\\d/;\nexport const Isbn10Schema = z.string().regex(isbn10Regex);\n</code></pre> <p>Building onto that, an ISBN-13 is in a similar format, but has the form of <code>###-#-##-######-#</code>. By tweaking our regex, we end up with the following:</p> <pre><code>const isbn13Regex = /^\\d{3}-\\d-\\d{2}-\\d{6}-\\d/;\nexport const Isbn13Schema = z.string().regex(isbn13Regex);\n</code></pre> <p>When modeling types in TypeScript, I'd like to be able to do something like the following as this makes it clear that an ISBN can in one of these two shapes.</p> <pre><code>type Isbn10 = string;\ntype Isbn13 = string;\ntype Isbn = Isbn10 | Isbn13;\n</code></pre> <p>While we can't use the <code>|</code> operator, we can use the <code>.or</code> function from zod to have the following</p> <pre><code>const isbn10Regex = /^\\d-\\d{3}-\\d{5}-\\d/;\nexport const Isbn10Schema = z.string().regex(isbn10Regex);\nconst isbn13Regex = /^\\d{3}-\\d-\\d{2}-\\d{6}-\\d/;\nexport const Isbn13Schema = z.string().regex(isbn13Regex);\n\nexport const IsbnSchema = Isbn10Schema.or(Isbn13Schema);\n</code></pre> <p>With the <code>IsbnSchema</code> in place, let's add it to <code>BookSchema</code></p> <pre><code>export const BookSchema = z.object({\n    author: AuthorSchema,\n    title: NonEmptyStringSchema,\n    price: z.number().positive(),\n    genre: GenreSchema\n    isbn: IsbnSchema\n});\n</code></pre>"},{"location":"articles/2024/06/13/today-i-learned-validating-data-in-zod/#getting-models-for-free","title":"Getting Models for Free","text":"<p>Lastly, one of the cooler functions that zod supports is <code>infer</code> where if you pass it a schema, it can build out a type for you to use in your application. </p> <pre><code>export const BookSchema = z.object({\n    author: AuthorSchema,\n    title: NonEmptyStringSchema,\n    price: z.number().positive(),\n    genre: GenreSchema\n    isbn: IsbnSchema\n});\n\n// TypeScript knows that Book must have an author (which has a firstName, lastName, and maybe a middleName)\n// a title (string), a price (number), a genre (string), and an isbn (string).\nexport type Book = z.infer&lt;typeof BookSchema&gt;; \n</code></pre>"},{"location":"articles/2024/06/13/today-i-learned-validating-data-in-zod/#full-solution","title":"Full Solution","text":"<p>Here's what the full solution looks like</p> <pre><code>const NonEmptyStringSchema = z.string().min(1);\nconst GenreSchema = z.enum([\"Fantasy\", \"History\", \"Mystery\"]);\n\nexport const AuthorSchema = z.object({\n  firstName: NonEmptyString,\n  lastName: NonEmptyString,\n  middleName: z.string().optional(),\n});\n\nexport const Isbn10Schema = z.string().regex(/^\\d-\\d{2}-\\d{6}-\\d/);\nexport const Isbn13Schema = z.string().regex(/^\\d{3}-\\d-\\d{2}-\\d{6}-\\d/);\nexport const IsbnSchema = Isbn10Schema.or(Isbn13Schema);\n\nexport const BookSchema = z.object({\n  title: NonEmptyString,\n  author: AuthorSchema,\n  price: z.number().positive(),\n  genre: GenreSchema,\n  isbn: IsbnSchema,\n});\n\nexport type Book = z.infer&lt;typeof BookSchema&gt;;\n</code></pre> <p>With these schemas and models defined, we can leverage the <code>safeParse</code> function to see if our input is valid.</p> <pre><code>describe('when validating a book', () =&gt; {\n    it(\"and the author is missing, then it's not valid\", () =&gt; {\n        const input = {title:\"best book\", price:200, genre:\"History\", isbn:\"1-23-456789-0\"}\n\n        const result = BookSchema.safeParse(input);\n\n        expect(result.success).toBe(false);\n    });\n    it(\"and all the fields are valid, then the book is valid\", () =&gt; {\n        const input = {\n            title:\"best book\", \n            price:200, \n            genre:\"History\", \n            isbn:\"1-23-456789-0\", \n            author: {\n                firstName:\"Super\", \n                middleName:\"Cool\", \n                lastName:\"Author\"\n            }\n        };\n\n        const result = BookSchema.safeParse(input);\n\n        expect(result.success).toBe(true);\n        const book:Book = result.data as Book;\n        // now we can start using properties from book\n        expect(book.title).toBe(\"best book\");\n    });\n});\n</code></pre>"},{"location":"articles/2025/05/27/simplifying-console-logic-with-the-model-view-update/","title":"Simplifying Console Logic with the Model-View-Update","text":"<p>When I first started dabbling in Functional Programming, a new front-end language called Elm had been released and it was generating a lot of buzz about how it simplified web development by introducing four parts (i.e., The Elm Architecture\" (TEA)) that provided a mental model when creating web pages. This way of thinking was so powerful that it inspired popular libraries like Redux and ngrx which took this architecture mainstream.</p>"},{"location":"articles/2025/05/27/simplifying-console-logic-with-the-model-view-update/#spilling-the-tea","title":"Spilling the TEA","text":"<p>At a high level, the architecture has four parts:</p> <ol> <li>Model -&gt; What are we rendering?</li> <li>View -&gt; How do we want to render it?</li> <li>Update -&gt; Given the current model and a Command, what's the new model?</li> <li>Command -&gt; What did the user do?</li> </ol> <p>To help make this a bit more clear, let's define some types for these parts and see how they would work together</p> <pre><code>type Model = any;\ntype View = (m:Model)=&gt;Promise&lt;Command&gt;;\ntype Update = (m:Model, c:Command)=&gt;Model;\ntype Command = \"firstOption\" | \"secondOption\" ... | 'quit';\n\nasync function main(model:Model, view:View, update:Update): Promise&lt;void&gt;{\n  const command = await view(model);\n  if (command === 'quit'){\n    return;\n  }\n  const newModel = update(model, command);\n  return main(newModel);\n}\n</code></pre> <p>With some types in play, let's go ahead and build out a small application, a counter (the \"Hello World\" for Elm).</p>"},{"location":"articles/2025/05/27/simplifying-console-logic-with-the-model-view-update/#building-a-counter","title":"Building a Counter","text":"<p>First, we need to figure out what the model will be. Since we're only keeping tracking of a number, we can define our model as a number.</p> <pre><code>type Model = number;\n</code></pre> <p>Next, we need to define what the user can do. In this case, they can either increment, decrement, or quit so let's set the command up.</p> <pre><code>type Command = 'increment' | 'decrement' | 'quit';\n</code></pre> <p>Now that we have <code>Command</code>, we can work on the <code>update</code> function. Given the type signature from before, we know its going to look like this:</p> <pre><code>function update(model:Model, command:Command): Model {\n  // logic\n}\n</code></pre> <p>We can leverage a <code>switch</code> and put in our business rules</p> <pre><code>function update(model:Model, command:Command): Model {\n  switch(command){\n    case 'increment': return model+1;\n    case 'decrement': return model-1;\n    case 'quit': return model;\n  }\n}\n</code></pre> <p>Finally, we need to define our <code>view</code> function. Like before, we can get the skeleton for the function based on the types from earlier.</p> <pre><code>async function view(model:Model): Promise&lt;Command&gt;{\n\n}\n</code></pre> <p>Let's update the function with our rendering logic</p> <pre><code>async function view(model:Model): Promise&lt;Command&gt;{\n  console.log(\"Counter:\", model);\n  console.log(\"Choose to (i)ncrement, (d)ecrement, or (q)uit\");\n}\n</code></pre> <p>We've got our render up and running, however, we need to get input from the user. Since we're working within Node, we could use readline, however, I've recently been using @inquirer/prompts and find it to be a nice abstraction to use. So let's use that package.</p> <pre><code>import {input} from \"@inquirer/prompts\";\n\nasync function getChoice(): Promise&lt;Command&gt;{\n  console.log(\"Choose to (i)ncrement, (d)ecrement, or (q)uit\");\n  const validChoices = [\"i\", \"d\", \"q\"];\n  const validator = (s:string) =&gt; validChoices.include(s?.trim().toLowerCase());\n  const selection = await input({message:message, validate:validator});\n  if (selection === \"i\") {\n    return \"increment\";\n  } else if (selection === \"d\"){\n    return \"decrement\";\n  } else {\n    return \"terminate\"\n  }\n}\n// Let's change the view function to use getChoice\n\nasync function view(model:Model): Promise&lt;Command&gt;{\n  console.log(\"Counter:\", model);\n  return getChoice();\n}\n</code></pre> <p>With these pieces defined, we can use the <code>main</code> function from before.</p> <pre><code>async function main(model:Model, view:View, update:Update): Promise&lt;void&gt;{\n  const command = await view(model);\n  if (command === 'quit'){\n    return;\n  }\n  const newModel = update(model, command);\n  return main(newModel);\n}\n\n// Invoking Main\nmain(10, view, update);\n</code></pre>"},{"location":"articles/2025/05/27/simplifying-console-logic-with-the-model-view-update/#starting-back-at-zero","title":"Starting Back at Zero","text":"<p>Now that we have increment and decrement working, it would be nice to be able to reset the counter without having to restart the application, so let's see how bad that would be.</p> <p>First, we need to add a new choice to <code>Command</code> (called reset). This will force us to update the rest of the code that's working with <code>Command</code>.</p> <pre><code>type Command = \"increment\" | \"decrement\" | \"reset\" | \"quit\";\n</code></pre> <p>Next, we need to update the <code>update</code> function so it knows how to handle a <code>reset</code> command. In our case, we need to set the model back to zero.</p> <pre><code>function update(model:Model, command:Command): Model {\n  switch(command){\n    case 'increment': return model+1;\n    case 'decrement': return model-1;\n    case 'reset': return 0;\n    case 'quit': return model;\n  }\n}\n</code></pre> <p>At this point, the application knows how to handle the new <code>Command</code>, however, we need to update our <code>view</code> function to allow the user to select reset.</p> <pre><code>async function view(model:Model): Promise&lt;Command&gt;{\n  console.log(\"Counter:\", model);\n  return getChoice();\n}\n\nasync function getChoice(): Promise&lt;Command&gt;{\n  // updating the console.log\n  console.log(\"Choose to (i)ncrement, (d)ecrement, (r)eset, or (q)uit\"); \n  const validChoices = [\"i\", \"d\", \"r\", \"q\"];\n  const validator = (s:string) =&gt; validChoices.include(s?.trim().toLowerCase());\n  const selection = await input({message:message, validate:validator});\n  if (selection === \"i\") {\n    return \"increment\";\n  } else if (selection === \"d\"){\n    return \"decrement\";\n  } else if (selection === \"r\"){\n    return \"reset\";\n  } else {\n    return \"terminate\"\n  }\n}\n</code></pre>"},{"location":"articles/2025/05/27/simplifying-console-logic-with-the-model-view-update/#whats-next","title":"What's Next?","text":"<p>Now that we have have a working version, you could start implementing some fun functionality. For example, how would you allow someone to set how much to increment or decrement by? What if you needed to keep track of previous values (i.e., maintaining history)? I highly encourage you trying this out with a simple kata (for example, how about giving Mars Rover a try?)</p>"},{"location":"articles/2025/05/27/simplifying-console-logic-with-the-model-view-update/#full-working-solution","title":"Full Working Solution","text":"<pre><code>import {input} from \"@inquirer/prompts\";\n\ntype Model = number;\ntype Command = \"increment\" | \"decrement\" | \"reset\" | \"quit\";\ntype View = (model:Model) =&gt; Promise&lt;Command&gt;;\ntype Update = (model:Model, command:Command) =&gt; Model;\n\nfunction update(model:Model, command:Command): Model {\n  switch(command){\n    case \"increment\": return model+1;\n    case \"decrement\": return model-1;\n    case \"reset\": return 0;\n    case \"quit\": return model;\n  }\n}\n\nfunction view(model:Model): Promise&lt;Command&gt;{\n  console.log(`Counter:${model}`);\n  return getChoice();\n}\n\nasync function getChoice(): Promise&lt;Command&gt;{\n  console.log(\"Choose to (i)ncrement, (d)ecrement, (r)eset, or (q)uit\"); \n  const validChoices = [\"i\", \"d\", \"r\", \"q\"];\n  const validator = (s:string) =&gt; validChoices.include(s?.trim().toLowerCase());\n  const selection = await input({message:message, validate:validator});\n  if (selection === \"i\") {\n    return \"increment\";\n  } else if (selection === \"d\"){\n    return \"decrement\";\n  } else if (selection === \"r\"){\n    return \"reset\";\n  } else {\n    return \"terminate\"\n  }\n}\n\nasync function main(model:Model, view:View, update:Update): Promise&lt;void&gt;{\n  const command = await view(model);\n  if (command === 'quit'){\n    return;\n  }\n  const newModel = update(model, command);\n  return main(newModel, view, update);\n}\n\nmain(10, view, update);\n</code></pre>"},{"location":"articles/2025/08/05/running-single-file-typescript-in-59-with-ts-node-and-tsx/","title":"Running Single File TypeScript in 5.9 with ts-node and tsx","text":"<p>On July 31, 2025, Microsoft released the next version of TypeScript, v5.9 (click here for full release notes)</p> <p>One of the changes that caught my attention is what the default <code>tsconfig.json</code> looks like when you leverage <code>npx tsc --init</code>.</p> <p>Before 5.9, when you ran <code>npx tsc --init</code>, you would get a default <code>tsconfig.json</code> that had all the options outlined with most of them commented, but you could see what their default values would be.</p>"},{"location":"articles/2025/08/05/running-single-file-typescript-in-59-with-ts-node-and-tsx/#what-changed","title":"What Changed?","text":"<p>In this release, two changes were made to the <code>tsconfig.json</code></p> <ul> <li>Instead of all the options being shown, now a subset of commonly tweaked settings are generated.</li> <li>There's now a section of recommended settings that have values set.</li> </ul> <p>Before</p> <pre><code>{\n  \"compilerOptions\": {\n    /* Visit https://aka.ms/tsconfig to read more about this file */\n\n    /* Projects */\n    // \"incremental\": true,                              /* Save .tsbuildinfo files to allow for incremental compilation of projects. */\n    // \"composite\": true,                                /* Enable constraints that allow a TypeScript project to be used with project references. */\n    // \"tsBuildInfoFile\": \"./.tsbuildinfo\",              /* Specify the path to .tsbuildinfo incremental compilation file. */\n    // \"disableSourceOfProjectReferenceRedirect\": true,  /* Disable preferring source files instead of declaration files when referencing composite projects. */\n    // \"disableSolutionSearching\": true,                 /* Opt a project out of multi-project reference checking when editing. */\n    // \"disableReferencedProjectLoad\": true,             /* Reduce the number of projects loaded automatically by TypeScript. */\n\n    /* Language and Environment */\n    \"target\": \"es2016\",                                  /* Set the JavaScript language version for emitted JavaScript and include compatible library declarations. */\n    // \"lib\": [],                                        /* Specify a set of bundled library declaration files that describe the target runtime environment. */\n    // \"jsx\": \"preserve\",                                /* Specify what JSX code is generated. */\n    // \"libReplacement\": true,                           /* Enable lib replacement. */\n    // \"experimentalDecorators\": true,                   /* Enable experimental support for legacy experimental decorators. */\n    // \"emitDecoratorMetadata\": true,                    /* Emit design-type metadata for decorated declarations in source files. */\n    // \"jsxFactory\": \"\",                                 /* Specify the JSX factory function used when targeting React JSX emit, e.g. 'React.createElement' or 'h'. */\n    // \"jsxFragmentFactory\": \"\",                         /* Specify the JSX Fragment reference used for fragments when targeting React JSX emit e.g. 'React.Fragment' or 'Fragment'. */\n    // \"jsxImportSource\": \"\",                            /* Specify module specifier used to import the JSX factory functions when using 'jsx: react-jsx*'. */\n    // \"reactNamespace\": \"\",                             /* Specify the object invoked for 'createElement'. This only applies when targeting 'react' JSX emit. */\n    // \"noLib\": true,                                    /* Disable including any library files, including the default lib.d.ts. */\n    // \"useDefineForClassFields\": true,                  /* Emit ECMAScript-standard-compliant class fields. */\n    // \"moduleDetection\": \"auto\",                        /* Control what method is used to detect module-format JS files. */\n\n    /* Modules */\n    \"module\": \"commonjs\",                                /* Specify what module code is generated. */\n    // \"rootDir\": \"./\",                                  /* Specify the root folder within your source files. */\n    // \"moduleResolution\": \"node10\",                     /* Specify how TypeScript looks up a file from a given module specifier. */\n    // \"baseUrl\": \"./\",                                  /* Specify the base directory to resolve non-relative module names. */\n    // \"paths\": {},                                      /* Specify a set of entries that re-map imports to additional lookup locations. */\n    // \"rootDirs\": [],                                   /* Allow multiple folders to be treated as one when resolving modules. */\n    // \"typeRoots\": [],                                  /* Specify multiple folders that act like './node_modules/@types'. */\n    // \"types\": [],                                      /* Specify type package names to be included without being referenced in a source file. */\n    // \"allowUmdGlobalAccess\": true,                     /* Allow accessing UMD globals from modules. */\n    // \"moduleSuffixes\": [],                             /* List of file name suffixes to search when resolving a module. */\n    // \"allowImportingTsExtensions\": true,               /* Allow imports to include TypeScript file extensions. Requires '--moduleResolution bundler' and either '--noEmit' or '--emitDeclarationOnly' to be set. */\n    // \"rewriteRelativeImportExtensions\": true,          /* Rewrite '.ts', '.tsx', '.mts', and '.cts' file extensions in relative import paths to their JavaScript equivalent in output files. */\n    // \"resolvePackageJsonExports\": true,                /* Use the package.json 'exports' field when resolving package imports. */\n    // \"resolvePackageJsonImports\": true,                /* Use the package.json 'imports' field when resolving imports. */\n    // \"customConditions\": [],                           /* Conditions to set in addition to the resolver-specific defaults when resolving imports. */\n    // \"noUncheckedSideEffectImports\": true,             /* Check side effect imports. */\n    // \"resolveJsonModule\": true,                        /* Enable importing .json files. */\n    // \"allowArbitraryExtensions\": true,                 /* Enable importing files with any extension, provided a declaration file is present. */\n    // \"noResolve\": true,                                /* Disallow 'import's, 'require's or '&lt;reference&gt;'s from expanding the number of files TypeScript should add to a project. */\n\n    /* JavaScript Support */\n    // \"allowJs\": true,                                  /* Allow JavaScript files to be a part of your program. Use the 'checkJS' option to get errors from these files. */\n    // \"checkJs\": true,                                  /* Enable error reporting in type-checked JavaScript files. */\n    // \"maxNodeModuleJsDepth\": 1,                        /* Specify the maximum folder depth used for checking JavaScript files from 'node_modules'. Only applicable with 'allowJs'. */\n\n    /* Emit */\n    // \"declaration\": true,                              /* Generate .d.ts files from TypeScript and JavaScript files in your project. */\n    // \"declarationMap\": true,                           /* Create sourcemaps for d.ts files. */\n    // \"emitDeclarationOnly\": true,                      /* Only output d.ts files and not JavaScript files. */\n    // \"sourceMap\": true,                                /* Create source map files for emitted JavaScript files. */\n    // \"inlineSourceMap\": true,                          /* Include sourcemap files inside the emitted JavaScript. */\n    // \"noEmit\": true,                                   /* Disable emitting files from a compilation. */\n    // \"outFile\": \"./\",                                  /* Specify a file that bundles all outputs into one JavaScript file. If 'declaration' is true, also designates a file that bundles all .d.ts output. */\n    // \"outDir\": \"./\",                                   /* Specify an output folder for all emitted files. */\n    // \"removeComments\": true,                           /* Disable emitting comments. */\n    // \"importHelpers\": true,                            /* Allow importing helper functions from tslib once per project, instead of including them per-file. */\n    // \"downlevelIteration\": true,                       /* Emit more compliant, but verbose and less performant JavaScript for iteration. */\n    // \"sourceRoot\": \"\",                                 /* Specify the root path for debuggers to find the reference source code. */\n    // \"mapRoot\": \"\",                                    /* Specify the location where debugger should locate map files instead of generated locations. */\n    // \"inlineSources\": true,                            /* Include source code in the sourcemaps inside the emitted JavaScript. */\n    // \"emitBOM\": true,                                  /* Emit a UTF-8 Byte Order Mark (BOM) in the beginning of output files. */\n    // \"newLine\": \"crlf\",                                /* Set the newline character for emitting files. */\n    // \"stripInternal\": true,                            /* Disable emitting declarations that have '@internal' in their JSDoc comments. */\n    // \"noEmitHelpers\": true,                            /* Disable generating custom helper functions like '__extends' in compiled output. */\n    // \"noEmitOnError\": true,                            /* Disable emitting files if any type checking errors are reported. */\n    // \"preserveConstEnums\": true,                       /* Disable erasing 'const enum' declarations in generated code. */\n    // \"declarationDir\": \"./\",                           /* Specify the output directory for generated declaration files. */\n\n    /* Interop Constraints */\n    // \"isolatedModules\": true,                          /* Ensure that each file can be safely transpiled without relying on other imports. */\n    // \"verbatimModuleSyntax\": true,                     /* Do not transform or elide any imports or exports not marked as type-only, ensuring they are written in the output file's format based on the 'module' setting. */\n    // \"isolatedDeclarations\": true,                     /* Require sufficient annotation on exports so other tools can trivially generate declaration files. */\n    // \"erasableSyntaxOnly\": true,                       /* Do not allow runtime constructs that are not part of ECMAScript. */\n    // \"allowSyntheticDefaultImports\": true,             /* Allow 'import x from y' when a module doesn't have a default export. */\n    \"esModuleInterop\": true,                             /* Emit additional JavaScript to ease support for importing CommonJS modules. This enables 'allowSyntheticDefaultImports' for type compatibility. */\n    // \"preserveSymlinks\": true,                         /* Disable resolving symlinks to their realpath. This correlates to the same flag in node. */\n    \"forceConsistentCasingInFileNames\": true,            /* Ensure that casing is correct in imports. */\n\n    /* Type Checking */\n    \"strict\": true,                                      /* Enable all strict type-checking options. */\n    // \"noImplicitAny\": true,                            /* Enable error reporting for expressions and declarations with an implied 'any' type. */\n    // \"strictNullChecks\": true,                         /* When type checking, take into account 'null' and 'undefined'. */\n    // \"strictFunctionTypes\": true,                      /* When assigning functions, check to ensure parameters and the return values are subtype-compatible. */\n    // \"strictBindCallApply\": true,                      /* Check that the arguments for 'bind', 'call', and 'apply' methods match the original function. */\n    // \"strictPropertyInitialization\": true,             /* Check for class properties that are declared but not set in the constructor. */\n    // \"strictBuiltinIteratorReturn\": true,              /* Built-in iterators are instantiated with a 'TReturn' type of 'undefined' instead of 'any'. */\n    // \"noImplicitThis\": true,                           /* Enable error reporting when 'this' is given the type 'any'. */\n    // \"useUnknownInCatchVariables\": true,               /* Default catch clause variables as 'unknown' instead of 'any'. */\n    // \"alwaysStrict\": true,                             /* Ensure 'use strict' is always emitted. */\n    // \"noUnusedLocals\": true,                           /* Enable error reporting when local variables aren't read. */\n    // \"noUnusedParameters\": true,                       /* Raise an error when a function parameter isn't read. */\n    // \"exactOptionalPropertyTypes\": true,               /* Interpret optional property types as written, rather than adding 'undefined'. */\n    // \"noImplicitReturns\": true,                        /* Enable error reporting for codepaths that do not explicitly return in a function. */\n    // \"noFallthroughCasesInSwitch\": true,               /* Enable error reporting for fallthrough cases in switch statements. */\n    // \"noUncheckedIndexedAccess\": true,                 /* Add 'undefined' to a type when accessed using an index. */\n    // \"noImplicitOverride\": true,                       /* Ensure overriding members in derived classes are marked with an override modifier. */\n    // \"noPropertyAccessFromIndexSignature\": true,       /* Enforces using indexed accessors for keys declared using an indexed type. */\n    // \"allowUnusedLabels\": true,                        /* Disable error reporting for unused labels. */\n    // \"allowUnreachableCode\": true,                     /* Disable error reporting for unreachable code. */\n\n    /* Completeness */\n    // \"skipDefaultLibCheck\": true,                      /* Skip type checking .d.ts files that are included with TypeScript. */\n    \"skipLibCheck\": true                                 /* Skip type checking all .d.ts files. */\n  }\n}\n</code></pre> <p>After</p> <pre><code>{\n  // Visit https://aka.ms/tsconfig to read more about this file\n  \"compilerOptions\": {\n    // File Layout\n    // \"rootDir\": \"./src\",\n    // \"outDir\": \"./dist\",\n    // Environment Settings\n    // See also https://aka.ms/tsconfig/module\n    \"module\": \"nodenext\",\n    \"target\": \"esnext\",\n    \"types\": [],\n    // For nodejs:\n    // \"lib\": [\"esnext\"],\n    // \"types\": [\"node\"],\n    // and npm install -D @types/node\n    // Other Outputs\n    \"sourceMap\": true,\n    \"declaration\": true,\n    \"declarationMap\": true,\n    // Stricter Typechecking Options\n    \"noUncheckedIndexedAccess\": true,\n    \"exactOptionalPropertyTypes\": true,\n    // Style Options\n    // \"noImplicitReturns\": true,\n    // \"noImplicitOverride\": true,\n    // \"noUnusedLocals\": true,\n    // \"noUnusedParameters\": true,\n    // \"noFallthroughCasesInSwitch\": true,\n    // \"noPropertyAccessFromIndexSignature\": true,\n    // Recommended Options\n    \"strict\": true,\n    \"jsx\": \"react-jsx\",\n    // \"verbatimModuleSyntax\": true,\n    \"isolatedModules\": true,\n    \"noUncheckedSideEffectImports\": true,\n    \"moduleDetection\": \"force\",\n    \"skipLibCheck\": true,\n  }\n}\n</code></pre> <p>I'm still working through my thoughts on these changes. On one hand, I like the fact that the tsconfig.json is much smaller now and that you can use VS Code to get intellisense on the other options. On the other, if you don't know what you're looking for, it was nice to just see what the options were (with their default values).</p>"},{"location":"articles/2025/08/05/running-single-file-typescript-in-59-with-ts-node-and-tsx/#exports-not-working","title":"Exports Not Working","text":"<p>After setting up a new test application, I tried running my app with <code>ts-node</code>, but got an error with one of my exports.</p> <p></p> <p>Huh, I'm not familiar with that setting, let's look into it.</p> <p>At a high level, this setting makes sure that you're not accidentally mixing CommonJS syntax (i.e. using <code>require</code> to access dependencies) with ESModule syntax (i.e. using <code>import</code> to access dependencies).</p>"},{"location":"articles/2025/08/05/running-single-file-typescript-in-59-with-ts-node-and-tsx/#finding-the-root-cause","title":"Finding the Root Cause","text":"<p>Weird, I'm not aware that I'm using CommonJS syntax anywhere, so I wonder where that's coming from. Doing some digging, I find out that there's a <code>type</code> setting you can specify for <code>package.json</code> with the following docs:</p> <p></p> <p>Ah, yeah, that will do it. So the issue is that by default, when you generate a package.json file, it doesn't create a <code>type</code> setting, so by default, it will be CommonJS.</p> <p>However, with TypeScript 5.9, the default configuration is assuming ESModule and with the <code>verbatimModuleSyntax</code> being set to true, this breaks our initial set-up scripts.</p>"},{"location":"articles/2025/08/05/running-single-file-typescript-in-59-with-ts-node-and-tsx/#migrating-to-esmodule","title":"Migrating to ESModule","text":"<p>Given the docs, the first thing I tried was to set the <code>type</code> setting in the package.json to <code>module</code>.</p> <p>This fixed my import and now my index file looks like this</p> <pre><code>// I wasn't expecting to see an import from `other.js`, but \n// it seems like this is expected behavior (see https://nodejs.org/api/esm.html#mandatory-file-extensions)\n\nimport { add } from \"./other.js\";\n\nconst sum = add(2, 3);\n\nconsole.log(sum);\n</code></pre> <p>At this point, I figured I was in a good spot, so I tried <code>npx ts-node src/index.ts</code> again.</p> <p>However, I was greeted with the following:</p> <p></p>"},{"location":"articles/2025/08/05/running-single-file-typescript-in-59-with-ts-node-and-tsx/#fighting-with-ts-node","title":"Fighting with ts-node","text":"<p>What in the world? I've never seen that error before, but doing some more digging into ts-node, it turns out that the issue is that ts-node doesn't know how to load ESModules by default, so you need to get an ESM specific loader to work. The docs mention a few different ways to work around that, so I started trying them out.</p> <p>No dice, even with trying all of that (and even some help with AI), I still wasn't able to get the TypeScript file to load. </p> <p>At the end of the day, if I wanted to stick with <code>ts-node</code>, I had to make the following changes.</p> <ul> <li>Update the <code>package.json</code> to have a type of <code>commonjs</code></li> <li>Update the <code>tsconfig.json</code> to set <code>verbatimModuleSyntax</code> to be false</li> <li>Update the import in <code>index</code> to be <code>import {add} from \"./other\"</code></li> </ul> <p>This works, however, CommonJS is an old way of setting up files and I don't like that I had to turn off the <code>verbatimModuleSyntax</code> setting.</p>"},{"location":"articles/2025/08/05/running-single-file-typescript-in-59-with-ts-node-and-tsx/#an-alternative-solution-with-tsx","title":"An Alternative Solution with tsx","text":"<p>Doing more digging, I found an alternative tool to <code>ts-node</code>, <code>[tsx](https://tsx.is/)</code> which purports having a simpler configuration and better support for ESM. This package has been around for a while now, has good support, and is actively maintained, so let's give that a whirl.</p> <pre><code>npm install --save-dev tsx\n</code></pre> <p>After installing <code>tsx</code>, I changed my <code>package.json</code> back to having a type of <code>module</code> and set <code>verbatimModuleSyntax</code> back to its default setting of true.</p> <p>Let's try to run the file</p> <p><pre><code>npx tsx src/index.ts\n\n5\n</code></pre> Hooray! I've never been happier to see basic addition working.</p>"},{"location":"articles/2025/08/05/running-single-file-typescript-in-59-with-ts-node-and-tsx/#wrapping-up","title":"Wrapping Up","text":"<p>I've been a big proponent of ts-node for a long-time now due to its easy-to-setup nature and how well it played with TypeScript out of the box. However, with the latest changes to TypeScript, I highly recommend using <code>tsx</code> over <code>ts-node</code> as it just seems to work without having to mess with a bunch of other settings. With the friction I ran into and the direction TypeScript is going, I'll be curious to see how ts-node evolves over time.</p>"},{"location":"articles/2023/04/30/better-domain-modeling-with-discriminated-unions/","title":"Better Domain Modeling with Discriminated Unions","text":"<p>When I think about software, I like designing software so that doing the right things are easy and doing the wrong things are impossible (or at least very hard). This approach is typically called falling into the pit of success.</p> <p>Having a well-defined domain model can prevent many mistakes from happening just because the code literally won't let it happen (either through a compilation error or other mechanisms).</p> <p>I'm a proponent of functional programming as it allows us to model software in a better way that can reduce the number of errors we make.</p> <p>Let's at one of my favorite techniques discriminated unions.</p>"},{"location":"articles/2023/04/30/better-domain-modeling-with-discriminated-unions/#motivation","title":"Motivation","text":"<p>In the GitHub API, there's an endpoint that allows you to get the events that have occurred for a pull request.</p> <p>Let's take a look at the example response in the docs.</p> <pre><code>[\n  {\n    \"id\": 6430295168,\n    \"url\": \"https://api.github.com/repos/github/roadmap/issues/events/6430295168\",\n    \"event\": \"locked\",\n    \"commit_id\": null,\n    \"commit_url\": null,\n    \"created_at\": \"2022-04-13T20:49:13Z\",\n    \"lock_reason\": null\n  },\n  {\n    \"id\": 6430296748,\n    \"url\": \"https://api.github.com/repos/github/roadmap/issues/events/6430296748\",\n    \"event\": \"labeled\",\n    \"commit_id\": null,\n    \"commit_url\": null,\n    \"created_at\": \"2022-04-13T20:49:34Z\",\n    \"label\": {\n      \"name\": \"beta\",\n      \"color\": \"99dd88\"\n    }\n  },\n  {\n    \"id\": 6635165802,\n    \"url\": \"https://api.github.com/repos/github/roadmap/issues/events/6635165802\",\n    \"event\": \"renamed\",\n    \"commit_id\": null,\n    \"commit_url\": null,\n    \"created_at\": \"2022-05-18T19:29:01Z\",\n    \"rename\": {\n      \"from\": \"Secret scanning: dry-runs for enterprise-level custom patterns (cloud)\",\n      \"to\": \"Secret scanning: dry-runs for enterprise-level custom patterns\"\n    }\n  }\n]\n</code></pre> <p>Based on the name of the docs, it seems like we'd expect to get back an array of events, let's call this <code>TimelineEvent[]</code>.</p> <p>Let's go ahead and define the <code>TimelineEvent</code> type. One approach is to start copying the fields from the events in the array. By doing this, we would get the following.</p> <pre><code>type TimelineEvent = {\n  id: number;\n  url: string;\n  event: string;\n  commit_id?: string;\n  commit_url?: string;\n  created_at: string;\n  lock_reason?: string;\n  label?: {\n    name: string;\n    color: string;\n  };\n  rename?: {\n    from: string;\n    to: string;\n  };\n};\n</code></pre>"},{"location":"articles/2023/04/30/better-domain-modeling-with-discriminated-unions/#the-problem","title":"The Problem","text":"<p>This definition will work, as it will cover all the data. However, the problem with this approach is that <code>lock_reason</code>, <code>label</code>, and <code>rename</code> had to be defined as nullable as they can sometimes be specified, but not always (for example, the <code>lock_reason</code> isn't specified for a label event).</p> <p>Let's say that we wanted to write a function that printed data about <code>TimelineEvent</code>, we would have to write something like the following:</p> <pre><code>function printData(event: TimelineEvent) {\n  if (event.event === \"labeled\") {\n    console.log(event.label!.name); // note the ! here, to tell TypeScript that I know it'll have a value\n  } else if (event.event == \"locked\") {\n    console.log(event.lock_reason);\n  } else {\n    console.log(event.rename!.from); // note the ! here, to tell Typescript that I know it'll have a value\n  }\n}\n</code></pre> <p>The main problem is that the we have to remember that the <code>labeled</code> event has a <code>label</code> property, but not the <code>locked</code> property. It might not be a big deal right now, but given that the GitHub API has over 40 event types, the odds of forgetting which properties belong where can be challenging.</p> <p>The pattern here is that we have a type <code>TimelineEvent</code> that can have different, separate shapes, and we need a type that can represent all the shapes.</p>"},{"location":"articles/2023/04/30/better-domain-modeling-with-discriminated-unions/#the-solution","title":"The Solution","text":"<p>One of the cool things about Typescript is that there is a union operator (|), that allows you to define a type as one of the other types.</p> <p>Let's refactor our <code>TimelineEvent</code> model to use the union operator.</p> <p>First, we need to define the different events as their own types</p> <pre><code>type LockedEvent = {\n  id: number;\n  url: string;\n  event: \"locked\"; // note the hardcoded value for event\n  commit_id?: string;\n  commit_url?: string;\n  created_at: string;\n  lock_reason?: string;\n};\n\ntype LabeledEvent = {\n  id: number;\n  url: string;\n  event: \"labeled\"; // note the hardcoded value for event\n  commit_id?: string;\n  commit_url: string;\n  created_at: string;\n  label: {\n    name: string;\n    color: string;\n  };\n};\n\ntype RenamedEvent = {\n  id: number;\n  url: string;\n  event: \"renamed\"; // note the hardcoded value for event\n  commit_id?: string;\n  commit_url?: string;\n  created_at: string;\n  rename: {\n    from: string;\n    to: string;\n  };\n};\n</code></pre> <p>At this point, we have three types, one for each specific event. A <code>LockedEvent</code> has no knowledge of a <code>label</code> property and a <code>RenamedEvent</code> has no knowledge of a <code>lock_reason</code> property.</p> <p>Next, we can update our definition of <code>TimelineEvent</code> to use the union operator as so.</p> <pre><code>type TimelineEvent = LockedEvent | LabeledEvent | RenamedEvent;\n</code></pre> <p>This would be read as A <code>TimelineEvent</code> can either be a <code>LockedEvent</code> or a <code>LabeledEvent</code> or a <code>RenamedEvent</code>.</p> <p>With this new definition, let's rewrite the <code>printData</code> function.</p> <pre><code>function printData(event: TimelineEvent) {\n  if (event.event == \"labeled\") {\n    console.log(event.label.name); // note that we no longer need !\n  } else if (event.event == \"locked\") {\n    console.log(event.lock_reason);\n  } else {\n    console.log(event.rename.to); // note that we no longer need !\n  }\n}\n</code></pre> <p>Not only do we not have to use the <code>!</code> operator to ignore type safety, but we also have better autocomplete (note that <code>locked_reason</code> and <code>rename</code> don't appear when working with a labeled event). </p>"},{"location":"articles/2023/04/30/better-domain-modeling-with-discriminated-unions/#deeper-dive","title":"Deeper Dive","text":"<p>At a general level, what we've modeled is a sum type and it's great for when you have a type that can take on a finite number of differing shapes.</p> <p>Sum types are implemented as either tagged unions or untagged unions. Typescript has untagged unions, however, other languages like Haskell and F#, use tagged unions. Let's see what the same implementation in F# would have looked like.</p> <pre><code>// specific type definitions omitted since they're\n// similar to typescript definition\n// ....\ntype TimelineEvent = Locked of LockedEvent | Labeled of LabeledEvent | Renamed of RenamedEvent\n\nlet printData e =\n    match e with\n    | Locked l -&gt; printf \"%s\" l.lock_reason\n    | Labeled l -&gt; printf \"%s\" l.label.name\n    | Renamed r -&gt; printf \"%s\" r.rename.``to`` // the `` is needed here as to is a reserved word in F#\n</code></pre> <p>A tagged union is when each shape has a specific constructor. So in the F# version, the <code>Locked</code> is the tag for the <code>LockedEvent</code>, <code>Labeled</code> is the tag for the <code>LabeledEvent</code>, so on and so forth. In the Typescript example, we worked around it because the <code>event</code> property is on every <code>TimelineEvent</code> and is a different value.</p> <p>If that wasn't true, then we would had to have added a field to <code>TimelineEvent</code> (typically called <code>kind</code> or <code>tag</code>) that would help us differentiate between the various shapes.</p>"},{"location":"articles/2023/04/30/better-domain-modeling-with-discriminated-unions/#wrapping-up","title":"Wrapping Up","text":"<p>When defining domain models where the model can have different shapes, you can use a sum type to define the model.</p>"},{"location":"articles/2025/06/20/today-i-learned---leveraging-records-to-eliminate-switch-statements/","title":"Today I Learned - Leveraging Records to Eliminate Switch Statements","text":""},{"location":"articles/2025/06/20/today-i-learned---leveraging-records-to-eliminate-switch-statements/#the-problem","title":"The Problem","text":"<p>A common approach is to have a function for each command (<code>moveForward</code>, <code>moveBackward</code>, <code>turnLeft</code>, and <code>turnRight</code>).</p> <p>When it comes to implementation though, they all have a similar pattern:</p> <pre><code>function moveForward(r:Rover): Rover {\n  switch(r.direction){\n    case 'North': return {...r, y:r.y+1};\n    case 'South': return {...r, y:r.y-1};\n    case 'East': return {...r, x:r.x+1};\n    case 'West': return {...r, x:r.x-1};\n  }\n}\n\nfunction turnLeft(r:Rover): Rover {\n  switch(r.direction){\n    case 'North': return {...r, direction:'West'};\n    case 'West': return {...r, direction:'South'};\n    case 'South': return {...r, direction:'East'};\n    case 'East': return {...r, direction:'North'};\n  }\n}\n</code></pre> <p>This works, however, the duplicated switch logic can be annoying to deal with. </p> <p>During a code review, one of our interns proposed an interesting solution using a dictionary for lookups.</p> <p>Let's take a closer look.</p>"},{"location":"articles/2025/06/20/today-i-learned---leveraging-records-to-eliminate-switch-statements/#possible-solution","title":"Possible solution","text":"<p>Instead of leveraging a <code>switch</code> statement, they thought about creating a dictionary where the key was the direction the rover was facing and the value being how to update the rover.</p> <p>At a high level, it looked something like this:</p> <pre><code>type Direction = \"North\" | \"South\" | \"East\" | \"West\";\ntype Action = (r:Rover) =&gt; Rover;\ntype ActionLookup = Record&lt;Direction, Action&gt;;\n\nfunction moveForward(r:Rover): Rover {\n  const lookup: ActionLookup = {\n    \"North\": (r)=&gt;({...r, y:r.y+1}),\n    \"South\": (r)=&gt;({...r, y:r.y-1}),\n    \"East\": (r)=&gt;({...r, x:r.x+1}),\n    \"West\": (r)=&gt;({...r, x:r.x-1})\n  };\n  const action = lookup[r.direction];\n  return action(r);\n}\n</code></pre> <p>Even though this looks like a dictionary, I like this approach better for two reasons:</p> <ol> <li>Explicit key coverage - By defining Record to have a key of <code>Direction</code>, we're forcing the developer to define options for every direction, not just some of them.</li> <li>Breaks when type changes - If a new option is added to <code>Direction</code>, this code won't compile anymore as it doesn't cover every option, which allows us to find bugs faster. </li> </ol>"},{"location":"articles/2025/06/20/today-i-learned---leveraging-records-to-eliminate-switch-statements/#closing-thoughts","title":"Closing Thoughts","text":"<p>When working with data that requires different access levels, think about leveraging private fields and then providing access through public properties.</p>"},{"location":"articles/2025/03/27/tips-and-tricks-with-typescript/","title":"Tips and Tricks with TypeScript","text":"<p>One of my most recent projects has been tackling how to model the card game, Love Letter. For those who've seen me present my How Functional Programming Made Me a Better Developer talk, you might recall that this was my second project to tackle in F# and that even though I was able to get some of it to work, there were some inconsistency in the rules that I wasn't able to reason about.</p> <p>While implementing in TypeScript, I came across some cool tricks and thought I'd share some of them here, enjoy!</p>"},{"location":"articles/2025/03/27/tips-and-tricks-with-typescript/#swapping-two-variables","title":"Swapping two variables","text":"<p>A common enough task in programming, we need to swap two values around. When I was learning C++, I had a memory trick to remember the ordering (think like a zigzag pattern)</p> <pre><code>int a = 100;\nint b = 200;\n\n// Note how a starts on the right, then goes left\nint temp = a;\na = b;\nb = temp;\n</code></pre> <p>You can do the same thing in TypeScript, however, you can remove the need for the <code>temp</code> variable by using array destructuring. The idea is that we create an array which contains the two variables to swap. We then assign this array to destructured variables (see below)</p> <pre><code>let a:number = 100;\nlet b:number = 200;\n\n// using array destructuring\n\n[a,b] = [b,a]\n</code></pre>"},{"location":"articles/2025/03/27/tips-and-tricks-with-typescript/#drawing-a-card","title":"Drawing a Card","text":"<p>One of the common interactions that happens in the game is that we need to model drawing a card from a deck. As such, we will have a function that looks like the following</p> <pre><code>type Card = \"Guard\" | \"Priest\" | \"Baron\" | .... // other options omitted for brevity\ntype Deck = Card[]\n\nfunction draw(d:Deck): {card:Card, restOfDeck:Deck} {\n  const card = d[0];\n  const restOfDeck = d.slice(1);\n  return {card, restOfDeck};\n}\n</code></pre> <p>This absolutely works, however, we can use the rest operator (...) with array destructuring to simplify this code. </p> <pre><code>type Card = \"Guard\" | \"Priest\" | \"Baron\" | .... // other options omitted for brevity\ntype Deck = Card[]\n\nfunction draw(d:Deck): {card:Card, restOfDeck:Deck} {\n  const [card, ...restOfDeck] = d; // note the rest operator before restOfDeck\n  return {card, restOfDeck};\n}\n</code></pre> <p>This code should be read as assign the first element of the array to a const called <code>card</code> and the rest of the array to a const called <code>restOfDeck</code>.</p>"},{"location":"articles/2025/03/27/tips-and-tricks-with-typescript/#drawing-multiple-cards-with-object-destructuring","title":"Drawing Multiple Cards with Object Destructuring","text":"<pre><code>function draw(d:Deck): {card:Card, restOfDeck:Deck} {\n  const [card, ...restOfDeck] = d;\n  return {card, restOfDeck};\n}\n\nfunction drawMultipleCads(d:Deck, count:number): {cards:Card[], restOfDeck:Deck} {\n  let currentDeck = d;\n  const cards:Card[] = [];\n  for (let i = 0; i &lt; count; i++) {\n    const result = draw(currentDeck)\n    cards.push(result.card)\n    currentDeck = result.restOfDeck\n  }\n  return {cards, restOfDeck:currentDeck};\n}\n</code></pre> <p>This works, however, we don't actually care about <code>result</code>, but the specific values <code>card</code> and <code>restOfDeck</code>. Instead of referencing them via property drilling, we can use object destructuring to get their values.</p> <pre><code>function drawMultipleCads(d:Deck, count:number): {cards:Card[], restOfDeck:Deck} {\n  let currentDeck = d;\n  const cards:Card[] = [];\n  for (let i = 0; i &lt; count; i++) {\n    const {card, restOfDeck} = draw(currentDeck) // note using curly braces\n    cards.push(card)\n    currentDeck = restOfDeck\n  }\n  return {cards, restOfDeck:currentDeck};\n}\n</code></pre>"},{"location":"articles/2024/03/26/how-using-vertical-slicing-can-minimize-dependencies-and-deliver-value-faster/","title":"How Using Vertical Slicing Can Minimize Dependencies and Deliver Value Faster","text":"<p>How do we break down this work?</p> <p>It's a good question and it can help set the tone for the project. Assuming the work is more than a bug fix, it's natural to look at a big project and break it down to smaller, more approachable pieces.</p> <p>Depending on how you break down the work, you can dramatically change the timeline from when you can get feedback from your users and find issues much sooner.</p> <p>In this post, let's look at a team breaking down a new feature for their popular application, TakeItEasy.</p>"},{"location":"articles/2024/03/26/how-using-vertical-slicing-can-minimize-dependencies-and-deliver-value-faster/#a-new-day-a-new-feature","title":"A New Day - A New Feature","text":"<p>It's a new sprint and your team is tackling a highly requested feature for TakeItEasy, the ability to setup a User Profile. Everyone is clear on the business requirements as we need the ability to save and retrieve the following information so that we can personalize the application experience for the logged in user:</p> <ul> <li>Display Name</li> <li>Name</li> <li>Email Address</li> <li>Profile Picture</li> </ul> <p>Going over the high level design with the engineers, it's discovered that there's not a way to save this data right now. In addition, we don't have a way to display this data for the user to enter or modify.</p>"},{"location":"articles/2024/03/26/how-using-vertical-slicing-can-minimize-dependencies-and-deliver-value-faster/#breaking-work-down-as-horizontal-layers","title":"Breaking Work Down as Horizontal Layers","text":"<p>Working with the team, the feature gets broken down as the following stories:</p> <ul> <li>Create the data storage</li> <li>Create the data access layer</li> <li>Create the User Profile screen</li> </ul> <p>Once these stories are done, this feature is done and that seems easy enough. As you talk with the team though, a few things stand-out to you.</p> <ol> <li> <p>None of these stories are fully independent of each other. You can build out the User Profile screen, but without the Data Access Layer, it's incomplete. Same thing with the data access layer, it can't be fully complete until the data storage story is done.</p> </li> <li> <p>It's difficult to demo the majority of the stories. Stakeholders don't care about data storage or the data access layer, but they do care about the user setting up their profile. With the current approach, it's not possible to demo any work until all three stories are done.</p> </li> </ol> <p>As you approach each story, they seem to be quite large:</p> <ol> <li>For the Data Storage work, it's an upgrade script to modify the Users table with nullable columns.</li> <li>For the data access story, it's updating logic to retrieve each of the additional fields and making sure to handle missing values from the database.</li> <li>For the User Profile screen, it's creating a new page, update the routing, and adding multiple controls with validation logic for each of the new fields.</li> </ol> <p>Is there a different way we can approach this work such that we can deliver something useful sooner?</p>"},{"location":"articles/2024/03/26/how-using-vertical-slicing-can-minimize-dependencies-and-deliver-value-faster/#breaking-down-the-work-as-vertical-slices","title":"Breaking Down the Work as Vertical Slices","text":"<p>The main issue with the above approach is that there's a story for each layer of the application (data, business rules, and user interface) and each of these layers are dependent upon each other. However, no one cares about any single layer, they care about all of it working together.</p>  Seriously, could you imagine enjoying a plate of nachos by first eating all the chips, then the beans, then the salsa?   Photo by Herson Rodriguez on Unsplash <p>One way to solve this problem would be to have a single story Implement User Profile that has all this work, but that sounds like more than a sprints worth of work. We know that that the more work in a story, the harder it is to give a fair estimate for what's needed.</p> <p>Another approach to solve this problem is by changing the way we slice the work by taking a bit of each layer into a story. This means that we'll have a little bit of database, little bit of data access, and little bit of the user interface.</p> <p>If we take this approach, we would have the following stories for our User Profile feature.</p> <p>Feature: Implement User Profile</p> <ul> <li>Story: Implement Display Name</li> <li>Story: Implement Name</li> <li>Story: Implement Email</li> <li>Story: Implement Profile Picture</li> </ul> <p>Each story would have the following tasks:</p> <ul> <li>Add storage for field</li> <li>Update data access to store/retrieve field</li> <li>Update interface with control and validation logic</li> </ul> <p>There are quite a few advantages with this approach.</p> <p>First, instead of waiting for all the stories to get done before you can demo any functionality, you can demo after getting one story completed. This is huge advantage because if things are looking well, you could could potentially go live with one story instead of waiting for all three stories from before.</p> <p>Second, these stories are independent of each other as the work to Implement Display Name doesn't depend on anything from Implement Email. This increases the autonomy of the team and allows us to shift priorities easier as at the end of any one story, we can pick any of the remaining stories.</p> <p>For example, let's say that after talking more with customers, we need a way for them to add their favorite dessert. Instead of the business bringing in the new requirement and pushing back the timeline, engineering can work on that functionality next and get that shipped sooner.</p> <p>Third, it's much easier to explain to engineers and stakeholders for when a certain piece of functionality will be available. Going back to horizontal layering, it's not clear when a user would be able to set-up their email address. Now, it's clear when that work is coming up.</p>"},{"location":"articles/2024/03/26/how-using-vertical-slicing-can-minimize-dependencies-and-deliver-value-faster/#why-the-horizontally-slicing","title":"Why The Horizontally Slicing?","text":"<p>I'm going to let you on a little secret. Most engineers are technically strong, but can be ignorant of the business domain that they're solving in. Unless you're taking time to coach them on the business (or if they've been working in the domain for a long period of time), engineers just don't know the business.</p> <p>As such, it's difficult for engineers to speak in the ubiquitous language of the business, it's much easier to speak in the technical details. This, in turn, leads to user stories that are more technical details in nature (modify table, build service, update pipeline) instead of user focused (can set display name, can set email address).</p> <p>If you're an Engineer, you need to learn the business domain that you're working in. This will help you prevent problems from happening in your software because it literally can't do that. In addition, this will help you see the bigger picture and build empathy with your users as you understand them better.</p> <p>If you're in Product or Business, you need to work with your team to level up their business domain. This can be done by having them use the product like a user, giving them example tasks, and spending time to talk about the domain. If you can get the engineers to be hands-on, every hour you invest here is going to pay huge dividends when it comes time to pick up the next feature.</p>"},{"location":"articles/2024/03/26/how-using-vertical-slicing-can-minimize-dependencies-and-deliver-value-faster/#wrapping-up","title":"Wrapping Up","text":"<p>The next time you and the team have a feature, try experimenting with vertically slicing your stories and see how that changes the dynamics of the team.</p> <p>To get started, remember, focus on the user outcomes and make sure that each story can stand independently of one another.</p> <p>If this post resonated with you, I'd like to hear from you! Feel free to drop me a line at CoachingCorner@TheSoftwareMentor.com!</p>"},{"location":"articles/archive/2025/","title":"2025","text":""},{"location":"articles/archive/2024/","title":"2024","text":""},{"location":"articles/archive/2023/","title":"2023","text":""},{"location":"articles/archive/2020/","title":"2020","text":""},{"location":"articles/archive/2018/","title":"2018","text":""},{"location":"articles/archive/2015/","title":"2015","text":""},{"location":"articles/archive/2014/","title":"2014","text":""},{"location":"articles/archive/2013/","title":"2013","text":""},{"location":"articles/category/net/","title":".NET","text":""},{"location":"articles/category/functional-programming/","title":"Functional Programming","text":""},{"location":"articles/category/typescript/","title":"TypeScript","text":""},{"location":"articles/category/debugging/","title":"Debugging","text":""},{"location":"articles/category/career/","title":"Career","text":""},{"location":"articles/category/today-i-learned/","title":"Today I Learned","text":""},{"location":"articles/category/process/","title":"Process","text":""},{"location":"articles/category/tooling/","title":"Tooling","text":""},{"location":"articles/category/leadership/","title":"Leadership","text":""},{"location":"articles/category/coaching-corner/","title":"Coaching Corner","text":""},{"location":"articles/category/c/","title":"C<code>#</code>","text":""},{"location":"articles/category/aws/","title":"AWS","text":""},{"location":"articles/category/interviewing/","title":"Interviewing","text":""},{"location":"articles/category/learning-through-example/","title":"Learning Through Example","text":""},{"location":"articles/category/basics/","title":"Basics","text":""},{"location":"articles/category/solid/","title":"SOLID","text":""},{"location":"articles/page/2/","title":"Index","text":""},{"location":"articles/page/3/","title":"Index","text":""},{"location":"articles/page/4/","title":"Index","text":""},{"location":"articles/page/5/","title":"Index","text":""},{"location":"articles/page/6/","title":"Index","text":""},{"location":"articles/page/7/","title":"Index","text":""},{"location":"articles/page/8/","title":"Index","text":""},{"location":"articles/page/9/","title":"Index","text":""},{"location":"articles/archive/2025/page/2/","title":"2025","text":""},{"location":"articles/archive/2024/page/2/","title":"2024","text":""},{"location":"articles/archive/2024/page/3/","title":"2024","text":""},{"location":"articles/archive/2023/page/2/","title":"2023","text":""},{"location":"articles/archive/2020/page/2/","title":"2020","text":""},{"location":"articles/category/learning-through-example/page/2/","title":"Learning Through Example","text":""},{"location":"articles/category/process/page/2/","title":"Process","text":""},{"location":"articles/category/today-i-learned/page/2/","title":"Today I Learned","text":""},{"location":"articles/category/today-i-learned/page/3/","title":"Today I Learned","text":""},{"location":"articles/category/typescript/page/2/","title":"TypeScript","text":""},{"location":"articles/category/typescript/page/3/","title":"TypeScript","text":""}]}